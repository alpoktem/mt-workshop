{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Machine translation workshop",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Machine Translation for Translators Workshop\n",
        "\n",
        "In this notebook, we'll learn:\n",
        "\n",
        "- Running a pre-trained English-Turkish MT model \n",
        "- Byte-pair encoding (BPE)\n",
        "- Translating a document\n",
        "- Creating training data from translation memory (TMX)\n",
        "- Creating a test set and calculating BLEU\n",
        "- Domain adaptation on our pre-trained model\n",
        "- Training a model from scratch\n",
        "\n",
        "NOTE: This coding template is partially based on Masakhane's starter notebook (https://github.com/masakhane-io/masakhane-mt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Machine translation with JoeyNMT and AfroTranslate\n",
        "\n",
        "For this workshop we will use ðŸ¨ JoeyNMT. \n",
        "\n",
        "JoeyNMT is an open-source, minimalist neural machine translation toolkit for educational purposes. [code](https://github.com/joeynmt/joeynmt), [documentation](https://joeynmt.readthedocs.io/en/latest/).\n",
        "\n",
        "We will also use a Python package called [AfroTranslate](https://github.com/hgilles06/AfroTranslate) to easily interact with JoeyNMT.\n",
        "\n",
        "Let's start by installing AfroTranslate. Since it's dependent on JoeyNMT, it'll automatically install it for us.\n",
        "\n"
      ],
      "metadata": {
        "id": "k-xY0ng0Qx-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install AfroTranslate"
      ],
      "metadata": {
        "id": "tqxIcN9FQj4q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d15fc0c-3f0b-4eba-cc57-ff41f130b4e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting AfroTranslate\n",
            "  Downloading AfroTranslate-0.0.6-py3-none-any.whl (12 kB)\n",
            "Collecting spacy==3.2.1\n",
            "  Downloading spacy-3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.0 MB 11.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader==0.4 in /usr/local/lib/python3.7/dist-packages (from AfroTranslate) (0.4)\n",
            "Collecting joeynmt==1.3\n",
            "  Downloading joeynmt-1.3-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (57.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (7.1.2)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (2.7.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596 kB 36.0 MB/s \n",
            "\u001b[?25hCollecting pylint\n",
            "  Downloading pylint-2.12.2-py3-none-any.whl (414 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414 kB 15.9 MB/s \n",
            "\u001b[?25hCollecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.1 MB 20.0 MB/s \n",
            "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90 kB 10.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (3.2.2)\n",
            "Collecting wrapt==1.11.1\n",
            "  Downloading wrapt-1.11.1.tar.gz (27 kB)\n",
            "Collecting torch==1.8.0\n",
            "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 735.5 MB 14 kB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (0.11.2)\n",
            "Collecting six==1.12\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (0.16.0)\n",
            "Collecting numpy==1.20.1\n",
            "  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15.3 MB 44.9 MB/s \n",
            "\u001b[?25hCollecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.1 MB 42.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (21.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (2.0.6)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (4.62.3)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (3.10.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (1.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (3.0.6)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181 kB 58.4 MB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 451 kB 40.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (2.11.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 628 kB 49.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (0.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy==3.2.1->AfroTranslate) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.2.1->AfroTranslate) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy==3.2.1->AfroTranslate) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1->AfroTranslate) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1->AfroTranslate) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1->AfroTranslate) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1->AfroTranslate) (2.10)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt==1.3->AfroTranslate) (0.8.9)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt==1.3->AfroTranslate) (2019.12.20)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.43.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (4.10.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (3.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy==3.2.1->AfroTranslate) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.2.1->AfroTranslate) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3->AfroTranslate) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3->AfroTranslate) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3->AfroTranslate) (2.8.2)\n",
            "Collecting isort<6,>=4.2.5\n",
            "  Downloading isort-5.10.1-py3-none-any.whl (103 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103 kB 52.4 MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Collecting platformdirs>=2.2.0\n",
            "  Downloading platformdirs-2.4.1-py3-none-any.whl (14 kB)\n",
            "Collecting astroid<2.10,>=2.9.0\n",
            "  Downloading astroid-2.9.3-py3-none-any.whl (254 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 254 kB 53.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3->AfroTranslate) (0.10.2)\n",
            "Collecting lazy-object-proxy>=1.4.0\n",
            "  Downloading lazy_object_proxy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (57 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting typed-ast<2.0,>=1.4.0\n",
            "  Downloading typed_ast-1.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 843 kB 39.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3->AfroTranslate) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3->AfroTranslate) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3->AfroTranslate) (2018.9)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68441 sha256=d4fdd82f22bfd33ca660ae058424ce90235c62441d3f562dbbd2f99906a5fb25\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/58/9d/da8bad4545585ca52311498ff677647c95c7b690b3040171f8\n",
            "Successfully built wrapt\n",
            "Installing collected packages: six, wrapt, typed-ast, numpy, lazy-object-proxy, catalogue, typer, torch, srsly, pydantic, portalocker, platformdirs, mock, mccabe, isort, colorama, astroid, torchtext, thinc, subword-nmt, spacy-loggers, spacy-legacy, sacrebleu, pyyaml, pylint, pathy, langcodes, spacy, joeynmt, AfroTranslate\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.13.3\n",
            "    Uninstalling wrapt-1.13.3:\n",
            "      Successfully uninstalled wrapt-1.13.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.20.1 which is incompatible.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.8.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-python-client 1.12.10 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed AfroTranslate-0.0.6 astroid-2.9.3 catalogue-2.0.6 colorama-0.4.4 isort-5.10.1 joeynmt-1.3 langcodes-3.3.0 lazy-object-proxy-1.7.1 mccabe-0.6.1 mock-4.0.3 numpy-1.20.1 pathy-0.6.1 platformdirs-2.4.1 portalocker-2.3.2 pydantic-1.8.2 pylint-2.12.2 pyyaml-6.0 sacrebleu-2.0.0 six-1.12.0 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 subword-nmt-0.3.8 thinc-8.0.13 torch-1.8.0 torchtext-0.9.0 typed-ast-1.5.2 typer-0.4.0 wrapt-1.11.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AfroTranslate comes with direct links to many African languages ([list](https://github.com/masakhane-io/masakhane-mt/tree/master/benchmarks)). \n",
        "\n",
        "Let's use it to load its Tigrinya model and use it."
      ],
      "metadata": {
        "id": "_L0NWncARvl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import translator\n",
        "from afrotranslate import MasakhaneTranslate"
      ],
      "metadata": {
        "id": "r83MXvgZ4prs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create translator object\n",
        "translator = MasakhaneTranslate(model_name=\"en-ti\")\n",
        "\n",
        "#translate \n",
        "translator.translate(\"I love you so much!\")"
      ],
      "metadata": {
        "id": "TE2EArLXSWhp",
        "outputId": "428d4c1b-dda3-4729-b3ba-e33e6d71b64f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading en-ti ...\n",
            "/content\n",
            "Downloading 14d7U3H-CrIjZSK48KxGTXkkW42ttRjdo into /usr/local/lib/python3.7/dist-packages/afrotranslate/models/en-ti/en-ti.zip... Done.\n",
            "Unzipping...Done.\n",
            "en-ti downloaded!\n",
            "As you don't provide any version we use this one by default: tigmix-baseline\n",
            "Here is the complete list of versions: ['tigmix-baseline', 'jw300-tigrigna-baseline']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'á‹­áˆá‰µá‹‰áŠ» áŠ¢á‹®áˆ'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator.translate(\"Hello localization school!\")"
      ],
      "metadata": {
        "id": "YRZBdmNTCuXt",
        "outputId": "f91a233d-ee7d-4a3f-c1ea-7709b5e5e2da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'áŒˆáŠ•á‹˜á‰¥'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also load a model of our own.\n",
        "\n",
        "Let's first download the model stored in the cloud.\n",
        "\n",
        "[Direct link](https://drive.google.com/file/d/1E_bdkdnYW4wTSdujsDiCqBlBYngvjX0m/view?usp=sharing)"
      ],
      "metadata": {
        "id": "G4Typ_EGYGaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute this to download directly to Colab\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id=\"1E_bdkdnYW4wTSdujsDiCqBlBYngvjX0m\",\n",
        "    \t\t                            dest_path=\"models/entr/entr.zip\",\n",
        "    \t\t                            unzip=True)"
      ],
      "metadata": {
        "id": "y_XH3nSmYM1m",
        "outputId": "6d804343-2225-496c-ee58-1818455cf4cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 1E_bdkdnYW4wTSdujsDiCqBlBYngvjX0m into models/entr/entr.zip... Done.\n",
            "Unzipping...Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's load our English-Turkish translator model using AfroTranslate"
      ],
      "metadata": {
        "id": "DfTj0kRsVWvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = MasakhaneTranslate(model_path=\"/content/models/entr\")"
      ],
      "metadata": {
        "id": "cTzqhVbKVgRe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator.translate(\"Hello world!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2cd8TXgSVwS3",
        "outputId": "bc7d07fb-3975-4d29-cf12-5798e7daa563"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'â– ut an Ã§ â–.'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator.translate(\"What's going on?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AThMQfxCaRg2",
        "outputId": "2d3e74e3-610c-4768-b913-3a7be0087c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'â– ut an Ã§ â–.'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results seem to be a bit strange. It's because the models were trained with BPE encoded words. "
      ],
      "metadata": {
        "id": "bjn4LYzZcXtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Byte-pair encoding (BPE)\n",
        "\n",
        "- One of the most powerful improvements for neural machine translation is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- BPE tokenization limits the number of vocabulary into a certain size by smartly dividing words into subwords\n",
        "\n",
        "- This is especially useful for agglutinative languages (like Turkish) where vocabulary is effectively endless. \n",
        "\n",
        "- Below you have the scripts for doing BPE tokenization of our data. We use bpemb library that has pre-trained BPE models to convert our text into subwords.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vMdOfnCOaUgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install bpemb"
      ],
      "metadata": {
        "id": "3GFu_jc5b34b",
        "outputId": "ad6ba4af-57d6-40f9-e97c-d7e6168a5a47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bpemb\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bpemb) (4.62.3)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2 MB 13.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb) (1.20.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bpemb) (2.23.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from bpemb) (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2.10)\n",
            "Installing collected packages: sentencepiece, bpemb\n",
            "Successfully installed bpemb-0.3.3 sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bpemb import BPEmb\n",
        "\n",
        "BPE_VOCAB_SIZE = 5000\n",
        "bpemb_en = BPEmb(lang=\"en\", vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=True)\n",
        "bpemb_tr = BPEmb(lang=\"tr\", vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=True)"
      ],
      "metadata": {
        "id": "r0ZErPmDbuVH",
        "outputId": "e3c0d9fb-8463-46e1-f7a3-b4839d8fb338",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs5000.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 315918/315918 [00:00<00:00, 10850887.53B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/tr/tr.wiki.bpe.vs5000.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 315775/315775 [00:00<00:00, 9713366.28B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use it now and see how it looks like"
      ],
      "metadata": {
        "id": "Ne7uYQ0_cIuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = \"Hello world so nice to be here!\""
      ],
      "metadata": {
        "id": "PhBPgoEwcIgV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpemb_tokens = bpemb_en.encode(string)\n",
        "bpemb_encoded_string = ' '.join(bpemb_tokens)"
      ],
      "metadata": {
        "id": "slwFSJ85cOTe"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bpemb_encoded_string)"
      ],
      "metadata": {
        "id": "F_yAyw-IE-2C",
        "outputId": "0408ae06-b0f3-4687-8e45-02ad2af04e74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â–mer hab a â–d Ã¼ n ya â–nas Ä± ls Ä± n ? â–g Ã¼ n ay d Ä± n !! !! â–# s ab ah\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string= \"merhaba dÃ¼nya nasÄ±lsÄ±n? GÃ¼naydÄ±n!!!! #sabah\"\n"
      ],
      "metadata": {
        "id": "VAYDEW3SFe-Z"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's go back using our model with properly encoded strings.\n",
        "\n",
        "Don't forget that `bpemb.encode` outputs a list but we need to input a string to our translator.\n",
        "\n",
        "You can use `' '.join(list)` to convert a list to string."
      ],
      "metadata": {
        "id": "Gpvp4BmhcVTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(string, bpemb):\n",
        "  bpemb_tokens = bpemb.encode(string)\n",
        "  bpemb_encoded_string = ' '.join(bpemb_tokens)\n",
        "  return bpemb_encoded_string"
      ],
      "metadata": {
        "id": "QC59yVXFc4nK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"merhaba\", bpemb_tr)"
      ],
      "metadata": {
        "id": "sfbPO5p8HR_M",
        "outputId": "ea3ebd5c-8cce-4d29-99b4-485c39c1b06f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'â–mer h aba'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w9d05981_2SH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hello world!\")\n",
        "input_encoded = encode(\"Hello world!\", bpemb_en)\n",
        "print(input_encoded)\n",
        "translated = translator.translate(input_encoded)\n",
        "print(translated)\n",
        "\n",
        "translated_tokens = translated.split()\n",
        "print(translated_tokens)\n",
        "translation = bpemb_tr.decode(translated_tokens)\n",
        "print(translation)"
      ],
      "metadata": {
        "id": "zkClamJ9k9Co",
        "outputId": "bdc0feb0-b160-48be-c3e5-70a22541e591",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world!\n",
            "â–hel lo â–world !\n",
            "â–mer h aba â–dÃ¼nya â– !\n",
            "['â–mer', 'h', 'aba', 'â–dÃ¼nya', 'â–', '!']\n",
            "merhaba dÃ¼nya !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a translator function that'll reduce our work..."
      ],
      "metadata": {
        "id": "rwMtdgVCm7Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_entr(en_in):\n",
        "  if not en_in:\n",
        "    return en_in\n",
        "\n",
        "  #print(en_in)\n",
        "  input_encoded = encode(en_in, bpemb_en)\n",
        "  #print(input_encoded)\n",
        "  translated = translator.translate(input_encoded)\n",
        "  #print(translated)\n",
        "\n",
        "  if type(translated) == dict:\n",
        "    translated = \" \".join(translated.values())\n",
        "  #print(translated)\n",
        "\n",
        "  translated_tokens = translated.split()\n",
        "  #print(translated_tokens)\n",
        "  translation = bpemb_tr.decode(translated_tokens)\n",
        "  #print(translation)\n",
        "\n",
        "  return translation"
      ],
      "metadata": {
        "id": "msoiIxUYkagg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_entr(\"Hello! What's up? This is a longer phrase. World is getting hotter and hotter. We need to act!\")"
      ],
      "metadata": {
        "id": "l3x1V1gdlyR5",
        "outputId": "aa01b76b-0497-4a90-8dfa-b04df18aa36f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Several sentences are detected. We split and translate them sequentially :).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'merhaba ! ne oldu ? bu daha uzun bir ifade . dÃ¼nya daha sÄ±cak ve daha sÄ±caklaÅŸÄ±yor . hareket etmeliyiz !'"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate_entr(\"\")"
      ],
      "metadata": {
        "id": "DX9AmUH3NMza",
        "outputId": "f498e6b9-1cdf-4f8a-89d8-c5caae954651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 (Challenge) - Translating a document\n",
        "\n",
        "For this exercise, we're going to create a script that translates a word document automatically. \n",
        "\n",
        "We're going to reuse some of the code from last session."
      ],
      "metadata": {
        "id": "utP_op_9ekwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "fpWFnsoZf0S1",
        "outputId": "dc48c8a5-c4be-4380-93c1-713d5a564085",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.6 MB 10.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx) (4.2.6)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=1458b45f80e0c8b509d13604a1831ea2c942da89e5987607e178db1c9de5c0bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download our document to translate.\n",
        "\n",
        "[Direct link](https://docs.google.com/document/d/1jwNdh4n_m4M-0Z4j23lH4xyeWfTyHPY-/edit?usp=sharing&ouid=114670265863192986077&rtpof=true&sd=true)"
      ],
      "metadata": {
        "id": "Ho5hJMT5gjHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute this to download directly to Colab\n",
        "gdd.download_file_from_google_drive(file_id=\"1jwNdh4n_m4M-0Z4j23lH4xyeWfTyHPY-\", dest_path=\"/content/MT.docx\")"
      ],
      "metadata": {
        "id": "I6_TtPMWgFlP",
        "outputId": "bd9dc932-366d-4e00-81c5-aeaa8e882d4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 1jwNdh4n_m4M-0Z4j23lH4xyeWfTyHPY- into /content/MT.docx... Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import docx and use it to read our document"
      ],
      "metadata": {
        "id": "h50CETebfHGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document"
      ],
      "metadata": {
        "id": "3Ui1asSqgFc6"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_path = \"/content/MT.docx\"\n",
        "document = Document(doc_path)"
      ],
      "metadata": {
        "id": "bj_F7IIMhLov"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what's inside"
      ],
      "metadata": {
        "id": "MtbXSjQifLoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for p in document.paragraphs:\n",
        "  print(p.text)"
      ],
      "metadata": {
        "id": "1PaC1Qj9hNU5",
        "outputId": "3ca79703-4526-446b-9d6e-0061667e105f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine translation\n",
            "Machine translation, sometimes referred to by the abbreviation MT (not to be confused with, machine-aided human translation or), is a sub-field of that investigates the use of software to text or speech from one to another.\n",
            "On a basic level, MT performs mechanical substitution of words in one language for words in another, but that alone rarely produces a good translation because recognition of whole phrases and their closest counterparts in the target language is needed. Not all words in one language have equivalent words in another language, and many words have more than one meaning.\n",
            "Solving this problem with statistical and techniques is a rapidly-growing field that is leading to better translations, handling differences in, translation of, and the isolation of anomalies.\n",
            "Current machine translation software often allows for customization by domain or (such as), improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used. It follows that machine translation of government and legal documents more readily produces usable output than conversation or less standardised text.\n",
            "Improved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has which words in the text are proper names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports).\n",
            "The progress and potential of machine translation have been much debated through its history. Since the 1950s, a number of scholars, first and most notably, have questioned the possibility of achieving fully automatic machine translation of high quality.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's translate paragraphs one by one and put the translations into another list\n",
        "\n"
      ],
      "metadata": {
        "id": "13ZqOe07iw22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = []\n",
        "\n",
        "for p in document.paragraphs:\n",
        "  paragraphs.append(p.text)\n",
        "\n",
        "print(paragraphs)"
      ],
      "metadata": {
        "id": "zipTAjg9ivyC",
        "outputId": "8f9c0dcd-c641-4ef8-a42f-a8e2b940f8a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Machine translation', 'Machine translation, sometimes referred to by the abbreviation MT (not to be confused with, machine-aided human translation or), is a sub-field of that investigates the use of software to text or speech from one to another.', 'On a basic level, MT performs mechanical substitution of words in one language for words in another, but that alone rarely produces a good translation because recognition of whole phrases and their closest counterparts in the target language is needed. Not all words in one language have equivalent words in another language, and many words have more than one meaning.', 'Solving this problem with statistical and techniques is a rapidly-growing field that is leading to better translations, handling differences in, translation of, and the isolation of anomalies.', 'Current machine translation software often allows for customization by domain or (such as), improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used. It follows that machine translation of government and legal documents more readily produces usable output than conversation or less standardised text.', 'Improved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has which words in the text are proper names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports).', 'The progress and potential of machine translation have been much debated through its history. Since the 1950s, a number of scholars, first and most notably, have questioned the possibility of achieving fully automatic machine translation of high quality.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translated_paragraphs = []\n",
        "for para in paragraphs:\n",
        "  translation = translate_entr(para)\n",
        "  print(translation)\n",
        "  translated_paragraphs.append(translation)"
      ],
      "metadata": {
        "id": "E2YWXgytjfsy",
        "outputId": "0465f029-8f74-4ef1-8d02-14cd4589125c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "makine tercÃ¼me .\n",
            "makine tercÃ¼mesi bazen salgÄ±n mtyasyon mtt tarafÄ±ndan adlandÄ±rÄ±lÄ±r . makine tarafÄ±ndan yÃ¶netilen insan tercÃ¼mesi ile karÄ±ÅŸtÄ±rÄ±lÄ±r ve bu yazÄ±lÄ±mÄ±n kullanÄ±mÄ± , birinden diÄŸerine konuÅŸma veya konuÅŸma kullanÄ±mÄ± araÅŸtÄ±rÄ±r .\n",
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "temel seviyede mt kelimelerin mekanik bir maddeyi diÄŸer bir dilde bir dilde yapÄ±yor , ama bu sadece nadiren iyi bir tercÃ¼me oluÅŸturuyor Ã§Ã¼nkÃ¼ bÃ¼tÃ¼n veriler ve hedef dilindeki en yakÄ±n iÃ§ kÄ±sÄ±mlarÄ±n ihtiyaÃ§ duyulmasÄ± . bir dildeki tÃ¼m kelimeler baÅŸka bir dilde ve birÃ§ok kelimenin bir anlamdan daha fazlasÄ± var .\n",
            "bu sorunu istatistiksel ve tekniklerle Ã§Ã¶zmek , daha iyi Ã§evresel Ã§evirilere yol aÃ§an hÄ±zlÄ± bÃ¼yÃ¼yen bir alandÄ±r. ve anomalilerin izole edilmesi .\n",
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "mevcut makine tercÃ¼mesi yazÄ±lÄ±mÄ± genellikle kiÅŸiselleÅŸtirilmeyi saÄŸlÄ±yor ya da alÃ§akgÃ¶nÃ¼llÃ¼ maddelerin Ã¼mitlerini sÄ±nÄ±rlayarak Ã§Ä±kararak . bu teknik Ã¶zellikle resmi veya formÃ¼lik dilin kullanÄ±ldÄ±ÄŸÄ± yerlerde etkilidir . hÃ¼kÃ¼metin ve yasal belgelerin tercÃ¼mesini takip ediyor , sohbetten ya da daha az standart yazÄ±lmÄ±ÅŸ metinlerden faydalanabilir sonuÃ§lar Ã¼retiyor .\n",
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "geliÅŸmiÅŸ sonuÃ§ kalitesi , Ã¶rneÄŸin , bazÄ± sistemler iÃ§in insan mÃ¼dahalesi iÃ§in insan mÃ¼dahalesi iÃ§in de ulaÅŸÄ±labilir . eÄŸer kullanÄ±cÄ±da kelimelerin doÄŸru isimleri varsa daha doÄŸru tercÃ¼me edebilir . bu tekniklerin yardÄ±mÄ±yla mt insan tercÃ¼manlarÄ±na yardÄ±m etmek iÃ§in bir araÃ§ olarak faydalÄ± olduÄŸunu kanÄ±tladÄ± ve Ã§ok sÄ±nÄ±rlÄ± sayÄ±da durumda , hatta kullanÄ±labilecek sonuÃ§lar Ã¼retebilir . affet . hava raporlarÄ± .\n",
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "makine tercÃ¼mesinin ilerleme ve potansiyelinin tarihi boyunca Ã§ok tartÄ±ÅŸÄ±ldÄ± . iÌ‡lk ve en Ã¶nemlisi , yÃ¼ksek kalitenin otomatik makine tercÃ¼mesini gerÃ§ekleÅŸtirme olasÄ±lÄ±ÄŸÄ±nÄ± sorguladÄ± .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, write the translated paragraphs into a newly created document."
      ],
      "metadata": {
        "id": "AREK4gqmfXzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translated_doc_name = \"MT_translated.docx\"\n",
        "\n",
        "document = Document()\n",
        "\n",
        "for p in translated_paragraphs:\n",
        "  document.add_paragraph(p)\n",
        "\n",
        "document.save(translated_doc_name)"
      ],
      "metadata": {
        "id": "wCfQVVv2niZS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4 - Creating parallel data from TMX\n",
        "\n",
        "Sometimes, we would like to adapt a model into our style or a domain that we translate in. If we have done plenty of translations already, we could use our translation memory to later enhance a model. For that we need to convert our translation memory to a classic parallel data format. \n",
        "\n",
        "The classic parallel data format is two files, each of them containing the sentences at each line in different languages. \n",
        "\n",
        "A translation memory contains already parallel data in this sense. Although, it is not in the format that we want. \n",
        "\n",
        "We can automate this conversion using Python scripting. \n",
        "\n",
        "Since TMX parsing is a bit complicated for our level, we're going to use this code by [Yasmin Moslem](https://github.com/ymoslem/file-converters/blob/main/TMX2MT/TMX2MT-ElementTree.py)."
      ],
      "metadata": {
        "id": "eFh05S37o_Cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml\n",
        "import xml.etree.ElementTree as ET\n",
        "import sys\n",
        "import re\n",
        "import os\n",
        "\n",
        "def xml_to_parallel(file, source, target):\n",
        "  source_file = os.path.splitext(file)[0] + \".\" + source\n",
        "  target_file = os.path.splitext(file)[0] + \".\" + target\n",
        "\n",
        "  tree = ET.parse(file)  \n",
        "  root = tree.getroot()\n",
        "\n",
        "  langs = []\n",
        "\n",
        "  for tu in root.iter('tu'):\n",
        "      for tuv in tu.iter('tuv'):\n",
        "          lang = list(tuv.attrib.values())\n",
        "          langs.append(lang[0].lower())\n",
        "\n",
        "  langs = set(langs)\n",
        "\n",
        "  if source in langs and target in langs:\n",
        "      with open(source_file, \"w+\", encoding='utf-8') as source_file, open(target_file, \"w+\", encoding='utf-8') as target_file:\n",
        "          for tu in root.iter('tu'):\n",
        "              for tuv in tu.iter('tuv'):\n",
        "                  lang = list(tuv.attrib.values())\n",
        "                  #print(lang[0])\n",
        "                  if lang[0].lower() == source.lower():\n",
        "                      for seg in tuv.iter('seg'):\n",
        "                          source_text = ET.tostring(seg, 'utf-8', method=\"xml\")\n",
        "                          source_text = source_text.decode(\"utf-8\")\n",
        "                          source_text = re.sub('<.*?>|&lt;.*?&gt;|&?(amp|nbsp|quot);|{}', ' ', source_text)\n",
        "                          source_text = re.sub(r'[ ]{2,}', ' ', source_text).strip()\n",
        "                          source_file.write(str(source_text) + \"\\n\")\n",
        "                          #print(source_text)\n",
        "                  elif lang[0].lower() == target.lower():\n",
        "                      for seg in tuv.iter('seg'):\n",
        "                          target_text = ET.tostring(seg, 'utf-8', method=\"xml\")\n",
        "                          target_text = target_text.decode(\"utf-8\")\n",
        "                          target_text = re.sub('<.*?>|&lt;.*?&gt;|&quot;|&apos;|{}', ' ', target_text)\n",
        "                          target_text = re.sub(r'[ ]{2,}', ' ', target_text).strip()\n",
        "                          target_file.write(str(target_text) + \"\\n\")\n",
        "                          #print(target_text)"
      ],
      "metadata": {
        "id": "KFjWrzT6pSfI"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have a translation memory that contains News translations."
      ],
      "metadata": {
        "id": "hFsaWt3PqOFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdd.download_file_from_google_drive(file_id=\"1xh6k91VFfiWpUnjrs-9mU_tkRkdS-INm\", dest_path=\"/content/news.en-tr.tmx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hywRVrX6qf7C",
        "outputId": "197cbb58-142b-4b8d-f9a2-db5db08a642b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 1xh6k91VFfiWpUnjrs-9mU_tkRkdS-INm into /content/news.en-tr.tmx... Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We just need to call our function now.\n",
        "\n"
      ],
      "metadata": {
        "id": "9GAitwTSr0B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xml_to_parallel(\"news.en-tr.tmx\", \"en\", \"tr\")"
      ],
      "metadata": {
        "id": "Z1JhQUr_qphF"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can download and check the parallel data that we created.\n",
        "\n",
        "(You can see them in the files panel once you hit refresh)"
      ],
      "metadata": {
        "id": "d3FXTYl5rcRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5 - Creating a test set and calculating BLEU\n",
        "\n",
        "In this part, we'll see how our model performs on a test set we create from our in-domain data. \n",
        "\n",
        "Usually, we don't want use all our in-domain data for training. We allocate a portion of it for testing purposes and we make sure that we don't mix this in the training data. Because if we do, it will be sort-of cheating and the results we get won't reflect the generalized quality of the model.\n",
        "\n",
        "Let's see how big is our data first:"
      ],
      "metadata": {
        "id": "o0DE_xMbrhB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute this to see the size of news dataset\n",
        "#NOTE: This is not Python! \n",
        "!wc news.en-tr.en news.en-tr.tr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yELXCDMugfmO",
        "outputId": "63289774-bc7d-4e45-f257-7567ec0488ce"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  10007  197808 1180320 news.en-tr.en\n",
            "  10007  149199 1255282 news.en-tr.tr\n",
            "  20014  347007 2435602 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's take a portion of it, say the final 200 samples as testing data, and the rest as training data."
      ],
      "metadata": {
        "id": "pAUVcrWXhEdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n -200 news.en-tr.en > news.en-tr.train.en \n",
        "!tail -n 200 news.en-tr.en > news.en-tr.test.en \n",
        "\n",
        "!head -n -200 news.en-tr.en > news.en-tr.train.tr \n",
        "!tail -n 200 news.en-tr.en > news.en-tr.test.tr "
      ],
      "metadata": {
        "id": "41N-3DswhdfB"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc news.en-tr.test.en news.en-tr.test.tr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkr7k2JUiBAm",
        "outputId": "dcac5f6d-c6c8-4d1e-ebcc-97220ab474be"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  200  4291 26329 news.en-tr.test.en\n",
            "  200  4291 26329 news.en-tr.test.tr\n",
            "  400  8582 52658 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, now we have a separate training and testing set. Let's see how our generic English-Turkish model performs on it.\n",
        "\n",
        "We'll first translate the English portion of our test set using our model."
      ],
      "metadata": {
        "id": "JY3QPWqMi7pS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#You can use this function to read a file line by line into a list\n",
        "def read_file_lines_to_list(filename):\n",
        "  return [l[:-1] for l in open(filename, 'r').readlines()]\n",
        "\n",
        "#You can use this function to write a list of strings into a file\n",
        "def write_list_to_file(strlist, filename):\n",
        "  with open(filename, 'w') as f:\n",
        "    for s in strlist:\n",
        "      f.write(s+\"\\n\")\n",
        "\n",
        "#You can use this function to see first n elements in a list\n",
        "def get_first_n(l, n):\n",
        "  for i, elem in enumerate(l):\n",
        "    print(elem)\n",
        "    if i == n:\n",
        "      break"
      ],
      "metadata": {
        "id": "sSLuFu5Yi7Vc"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_test_en = read_file_lines_to_list(\"news.en-tr.test.en\")\n",
        "\n",
        "get_first_n(news_test_en, 10)"
      ],
      "metadata": {
        "id": "7P3uPK_Aj4DO",
        "outputId": "c1fbabe6-cd9c-4259-bab0-f080e3969c8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to SWEDEN Social Welfare Institutionâ€™s data, while in 2005 one in 100,000 people were diagnosed with gender dysphoria, in 2015, this figure rose to eight in 100,000.\n",
            "Last year, 197 children and young people were referred to the Astrid Lindgen Childrenâ€™s Hospital due to discomfort with the gender with which they had been born.\n",
            "Working in the same hospital, child psychiatrist Lousise Frise, while stressing that such an increase could stem from becoming a more open society, said \"The openness of society to psycho-sexuality, including to questions related to sexual identity, could be perceived as a general trend\".\n",
            "Gender dysphoria is known to have no connection to a personâ€™s sexual orientation.\n",
            "Drug operation in Ä°zmir\n",
            "Following a tip, teams from the Ã–demiÅŸ District Police Directorateâ€™s Fight Against Smuggling and Organized Crimes Bureau raided to the house of F.T. (36), in Ãœlker Sokak in 3 EylÃ¼l Mahallesi.\n",
            "In the search of the house, thirty-three ecstasy pills and a set of precision scales were taken.\n",
            "Trying to jump off the balcony and run away, F.T. was caught and detained.\n",
            "It was reported that F.T. would be referred to court after procedures had been completed.\n",
            "Transfer in Evkur Yeni Malatyaspor\n",
            "One of the new teams in the Super Lig, Evkur Yeni Malatyaspor have come to an agreement with Turgut Dogan Sahin, a winger who wore the Kasimpasa uniform last season.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the same code we used before to translate the sentences in our list"
      ],
      "metadata": {
        "id": "FWtFz5T2kfEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_test_en_translated = []\n",
        "\n",
        "for sent in news_test_en:\n",
        "  news_test_en_translated.append(translate_entr(sent))"
      ],
      "metadata": {
        "id": "cz7Tbj_8kksT",
        "outputId": "0464e68a-25d2-4e86-97b2-8360087aefcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "Several sentences are detected. We split and translate them sequentially :).\n",
            "Several sentences are detected. We split and translate them sequentially :).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the translations look like"
      ],
      "metadata": {
        "id": "fRDTHCTzl9Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_first_n(news_test_en_translated, 10)"
      ],
      "metadata": {
        "id": "_yQXULL1l5cy",
        "outputId": "d1e895e6-14c2-455c-ef53-5f0d0ad4a5f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iÌ‡sveÃ§ sosyal refah kurumunun verisine gÃ¶re , 7 'de , 1 'de insanlar cinsiyet arzorisi teÅŸhisi konuldu , bu figÃ¼r 8 'e yÃ¼kseldi .\n",
            "geÃ§en yÄ±l 2000 Ã§ocuk ve genÃ§ler , doÄŸmuÅŸ olduklarÄ± cinsiyetle rahatsÄ±z olmasÄ±ndan dolayÄ± bir linden Ã§ocuk hastanesine Ã§aÄŸÄ±rÄ±ldÄ± .\n",
            "aynÄ± hastanede , aynÄ± hastanede Ã§ocuk psikiyatrist meyve frekansÄ±nda Ã§alÄ±ÅŸÄ±rken , bÃ¶yle bir artÄ±ÅŸÄ±n daha aÃ§Ä±k bir toplum haline gelebileceÄŸini stres ederken , \" toplumun aÃ§Ä±klÄ±ÄŸÄ± , cinsel kimlikle ilgili sorular da dahil olmak Ã¼zere psiko- cinsel kimlik ile iliÅŸkili sorular da dahil olmak Ã¼zere toplumun aÃ§Ä±k bir trenderi olarak algÄ±layabilir .\n",
            "cinsiyet arzorisi bir kiÅŸinin cinsel yÃ¶nelimiyle baÄŸlantÄ±sÄ± olmadÄ±ÄŸÄ±nÄ± biliyor .\n",
            "izmir 'de ilaÃ§ operasyonu .\n",
            "iÌ‡ndemi bÃ¶lge polis mÃ¼dÃ¼rÃ¼nÃ¼n savaÅŸtÄ±ÄŸÄ±nÄ± ve organize suÃ§ bÃ¼rosu f 'nun evine yÃ¶nlendirdi . bt . uÄŸlker sokak , 0 eylu mahallesi 'de .\n",
            "evin 3.3 . ekstezi haplarÄ±nÄ±n arayÄ±ÅŸÄ±nda ve bir dizi hassas Ã¶lÃ§ek Ã§ekildi .\n",
            "balkondan atlamaya Ã§alÄ±ÅŸmak ve b. f 'yi kaÃ§mak . bt . yakalandÄ± ve onaylandÄ± .\n",
            "f 'yi bildirildi . bt . prosedÃ¼rlerin tamamlanmasÄ±ndan sonra mahkemeye Ã§aÄŸÄ±rÄ±lÄ±rdÄ± .\n",
            "evkuri malatyaspor 'da transfer edin .\n",
            "sÃ¼per lig. evkuri malatyasporporpor 'daki yeni takÄ±mlardan biri turgut kÃ¶pek sahin , geÃ§en mevsimde kasimpasha Ã¼niformasÄ± ile bir anlaÅŸmaya geldi .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's write the translations to a textfile."
      ],
      "metadata": {
        "id": "EffswOIdn62d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "write_list_to_file(news_test_en_translated, \"news.en-tr.test.en.MT.tr\")"
      ],
      "metadata": {
        "id": "1UKNZXSDn_h5"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For doing BLEU evaluations, we can use the SacreBLEU package"
      ],
      "metadata": {
        "id": "yB2yMe2InBa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-88G48FDnOb0",
        "outputId": "7eb948ce-8ef7-4356-a4cb-bc423a401062"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.20.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2.3.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2019.12.20)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.4.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sacrebleu news.en-tr.test.tr -i news.en-tr.test.en.MT.tr"
      ],
      "metadata": {
        "id": "XV1B0T_rnvc2",
        "outputId": "701b3aef-da9a-4f0f-fc20-3af6068fafde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 0.0,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0\",\n",
            " \"verbose_score\": \"10.5/0.0/0.0/0.0 (BP = 0.692 ratio = 0.731 hyp_len = 3520 ref_len = 4818)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.0.0\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 7 - Fine-tuning our model\n"
      ],
      "metadata": {
        "id": "w95KvrhTqlW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we'll fine-tune our pre-trained model into our translation memory dataset.\n",
        "\n",
        "IMPORTANT: There's a discrepancy between AfroTranslate and JoeyNMT. So, we need to restart our runtime at this step and install JoeyNMT and bpemb again. \n",
        "\n",
        "Don't worry, the files we have prepared will stay in their places. "
      ],
      "metadata": {
        "id": "yiwcjrJV5_V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "metadata": {
        "id": "3HsYgpZf5-kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bpemb import BPEmb\n",
        "\n",
        "BPE_VOCAB_SIZE = 5000\n",
        "bpemb_en = BPEmb(lang=\"en\", vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=True)\n",
        "bpemb_tr = BPEmb(lang=\"tr\", vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=True)"
      ],
      "metadata": {
        "id": "MyEFTPox6r99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BPE encode the in-domain training data\n",
        "news_en_bpe = [' '.join(bpemb_en.encode(l[:-1])) for l in open('news.en-tr.en', 'r').readlines()]\n",
        "news_tr_bpe = [' '.join(bpemb_tr.encode(l[:-1])) for l in open('news.en-tr.tr', 'r').readlines()]\n",
        "\n",
        "#Allocate first 1000 samples as development set\n",
        "news_dev_en_bpe = news_en_bpe[0:1000]\n",
        "news_dev_tr_bpe = news_tr_bpe[0:1000]\n",
        "\n",
        "#Allocate last 200 samples as test set\n",
        "news_test_en_bpe = news_en_bpe[-200:]\n",
        "news_test_tr_bpe = news_tr_bpe[-200:]\n",
        "\n",
        "#Allocate rest as training data\n",
        "news_train_en_bpe = news_en_bpe[1000:-200]\n",
        "news_train_tr_bpe = news_tr_bpe[1000:-200]"
      ],
      "metadata": {
        "id": "LXxiFiZLq1zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_list_to_file(news_train_en_bpe, \"news.en-tr.train.BPE.en\")\n",
        "write_list_to_file(news_train_tr_bpe, \"news.en-tr.train.BPE.tr\")\n",
        "write_list_to_file(news_dev_en_bpe, \"news.en-tr.dev.BPE.en\")\n",
        "write_list_to_file(news_dev_tr_bpe, \"news.en-tr.dev.BPE.tr\")\n",
        "write_list_to_file(news_test_en_bpe, \"news.en-tr.test.BPE.en\")\n",
        "write_list_to_file(news_test_tr_bpe, \"news.en-tr.test.BPE.tr\")"
      ],
      "metadata": {
        "id": "i50AAey2sb-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a JoeyNMT config file for finetuning training\n",
        "# Changes from previous config are dataset names, model name, batch size and learning rate\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"entr_finetune\"\n",
        "\n",
        "data:\n",
        "    src: \"en\"\n",
        "    trg: \"tr\"\n",
        "    train: \"/content/news.en-tr.train.BPE\"\n",
        "    dev:   \"/content/news.en-tr.dev.BPE\"\n",
        "    test:  \"/content/news.en-tr.test.BPE\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 150\n",
        "    src_vocab: \"/content/models/entr/src_vocab.txt\"\n",
        "    trg_vocab: \"/content/models/entr/trg_vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    load_model: \"/content/models/entr/best.ckpt\" # Load base model from its best scoring checkpoint (from local)\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0001\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 128\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 64\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 100          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"/content/models/entr_finetune\"\n",
        "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_best_ckpts: 3\n",
        "    save_latest_ckpt: True\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\"\n",
        "with open(\"/content/entr_finetune.yaml\",'w') as f:\n",
        "    f.write(config)"
      ],
      "metadata": {
        "id": "s_0Ukw5sqscX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cd joeynmt; python3 -m joeynmt train \"/content/entr_finetune.yaml\""
      ],
      "metadata": {
        "id": "ElUNLotpwEFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c3794e9-65d6-418a-be62-df478b3b74b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-28 08:46:42,471 - INFO - root - Hello! This is Joey-NMT (version 1.5.1).\n",
            "2022-01-28 08:46:42,497 - INFO - joeynmt.data - Loading training data...\n",
            "2022-01-28 08:46:42,681 - INFO - joeynmt.data - Building vocabulary...\n",
            "2022-01-28 08:46:44,037 - INFO - joeynmt.data - Loading dev data...\n",
            "2022-01-28 08:46:44,055 - INFO - joeynmt.data - Loading test data...\n",
            "2022-01-28 08:46:44,058 - INFO - joeynmt.data - Data loaded.\n",
            "2022-01-28 08:46:44,058 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2022-01-28 08:46:44,344 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2022-01-28 08:46:46,757 - INFO - joeynmt.training - Total params: 13372928\n",
            "2022-01-28 08:46:49,639 - INFO - joeynmt.training - Loading model from /content/models/entr/best.ckpt\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                           cfg.name : entr_transformer\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                       cfg.data.src : en\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                       cfg.data.trg : tr\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                     cfg.data.train : /content/news.en-tr.train.BPE\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                       cfg.data.dev : /content/news.en-tr.dev.BPE\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                      cfg.data.test : /content/news.en-tr.test.BPE\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                     cfg.data.level : bpe\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -                 cfg.data.lowercase : False\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -           cfg.data.max_sent_length : 100\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -                 cfg.data.src_vocab : /content/models/entr/src_vocab.txt\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -                 cfg.data.trg_vocab : /content/models/entr/trg_vocab.txt\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -            cfg.training.load_model : /content/models/entr/best.ckpt\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -              cfg.training.patience : 5\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -  cfg.training.learning_rate_factor : 0.5\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 1000\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -            cfg.training.batch_size : 1028\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -            cfg.training.batch_type : token\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 3600\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 1\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -                cfg.training.epochs : 30\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 200\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/entr_finetune\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -             cfg.training.overwrite : True\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -               cfg.training.shuffle : True\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 3\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.3\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 4\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.3\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 8747,\n",
            "\tvalid 1000,\n",
            "\ttest 200\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] â–in â– iz mir â–in â–october â–last â–year , â–fat ma â– ÅŸ ah in , â–minister â–of â–women â–and â–social â–w elf are , â–ple aded â–for â–r ig or ous â–life â–imp ris onment â–at â–the â–trial â–of â–s . Ã§ . â–and â–0 â–rel atives â–who â–were â–alle ged â–to â–have â–st ab bed â–his â–wife â–to â–death â–because â–she â–wanted â–a â–div or ce .\n",
            "\t[TRG] â–- â–i Ì‡ z mir â€™ de â–geÃ§en â–yÄ±lÄ±n â–ekim â–ayÄ±nda , â–boÅŸ an mak â–Ã¼zere â–olduÄŸu â–eÅŸ ini â–b Ä± Ã§ak layarak â–Ã¶ldÃ¼r dÃ¼ÄŸÃ¼ â–iddi asÄ±yla , â–aÄŸÄ±r laÅŸtÄ±r Ä±lmÄ±ÅŸ â–mÃ¼ eb bet â–hapis â–cez asÄ± â–istem iyle â–hakkÄ±nda â–dav a â–aÃ§ Ä±lan â–s . Ã§ . â–ve â–0 â–yak Ä±nÄ±n Ä±n â–yarg Ä±lan dÄ±ÄŸÄ± â–dav aya â–da â–aile â–ve â–sosyal â–politik alar â–bakanÄ± â–fat ma â–ÅŸah in , â–hukuk â–m Ã¼ÅŸ av iri â–bir sel â–kurt â–aracÄ±lÄ±ÄŸÄ± â–ile â–dav aya â–mÃ¼ da h il â–olmak â–ist edi .\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) â–. (5) â–, (6) â–the (7) â–' (8) â–and (9) â–\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) â–. (5) â–, (6) â–the (7) â–' (8) â–and (9) â–\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - Number of Src words (types): 9034\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - Number of Trg words (types): 9034\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=9034),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=9034))\n",
            "2022-01-28 08:46:49,965 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 1028\n",
            "\ttotal batch size (w. parallel & accumulation): 1028\n",
            "2022-01-28 08:46:49,966 - INFO - joeynmt.training - EPOCH 1\n",
            "2022-01-28 08:46:49,995 - INFO - joeynmt.training - Epoch   1: total training loss 0.00\n",
            "2022-01-28 08:46:49,995 - INFO - joeynmt.training - EPOCH 2\n",
            "2022-01-28 08:47:04,509 - INFO - joeynmt.training - Epoch   2, Step:   114100, Batch Loss:     3.014850, Tokens per Sec:     3210, Lr: 0.000300\n",
            "2022-01-28 08:47:18,757 - INFO - joeynmt.training - Epoch   2, Step:   114200, Batch Loss:     2.906295, Tokens per Sec:     3108, Lr: 0.000300\n",
            "2022-01-28 08:48:37,423 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2022-01-28 08:48:37,424 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-01-28 08:48:37,424 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-01-28 08:48:37,905 - INFO - joeynmt.training - Example #0\n",
            "2022-01-28 08:48:37,905 - INFO - joeynmt.training - \tSource:     â–defense â–w ants â–case â–dis miss ed â–on â–ground s â–that â–man ning ' s â–conf in ement â–was â–har sh\n",
            "2022-01-28 08:48:37,905 - INFO - joeynmt.training - \tReference:  â–savunma â–man n ing ' in â–hap s ed ilm esinin â–sert â–olduÄŸu â–gerek Ã§ esiyle â–dav anÄ±n â–kap an masÄ±nÄ± â–ist iyor\n",
            "2022-01-28 08:48:37,905 - INFO - joeynmt.training - \tHypothesis: â–savunma , â–insan Ä±n â–kaf asÄ±nÄ±n â–kar Ä±ÅŸtÄ±r Ä±lmasÄ± â–zarar lÄ± â–olduÄŸu â–yer lerde â–suÃ§ lam ayÄ± â–ist iyor .\n",
            "2022-01-28 08:48:37,905 - INFO - joeynmt.training - Example #1\n",
            "2022-01-28 08:48:37,906 - INFO - joeynmt.training - \tSource:     â–the â–army â–private â–is â–acc used â–of â–ste aling â–thous ands â–of â–class ified â–doc uments\n",
            "2022-01-28 08:48:37,906 - INFO - joeynmt.training - \tReference:  â–ordu daki â–er â–bin lerce â–gizli â–bel ge â–Ã§al mak la â–suÃ§ lan Ä±yor\n",
            "2022-01-28 08:48:37,906 - INFO - joeynmt.training - \tHypothesis: â–ordu â–Ã¶zel liÄŸi â–bin lerce â–sÄ±nÄ±f landÄ±r Ä±lmÄ±ÅŸ â–belg eyi â–Ã§al mak la â–suÃ§ landÄ± .\n",
            "2022-01-28 08:48:37,906 - INFO - joeynmt.training - Example #2\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - \tSource:     â–pro sec ut ors â–tried â–to â–establish â–fr iday â–that â–army â–private â–br ad ley â–man ning â–- - â–char ged â–in â–the â–largest â–le ak â–of â–class ified â–material â–in â–u . s . â–history â–- - â–miss ed â–multiple â–opportun ities â–to â–compl ain â–about â–the â–m ist reat ment â–he ' s â–alle ging â–he â–suffered â–in â–military â–cust ody .\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - \tReference:  â–sav cÄ±lar â–cum a â–gÃ¼nÃ¼ â–abd â–tarihinde ki â–en â–bÃ¼yÃ¼k â–gizli â–mat er yal â–s Ä±z Ä±nt Ä±sÄ±yla â–suÃ§ lanan â–ordu daki â–er â–br ad ley â–man n ing ' in â–askeri â–gÃ¶z altÄ± â–sÄ±rasÄ±nda â–yaÅŸadÄ±ÄŸÄ± â–kÃ¶tÃ¼ â–davranÄ±ÅŸ â–iddi alarÄ± â–hakkÄ±nda â–birden â–fazla â–ÅŸ ik ayet â–et me â–fÄ±r sat Ä±nÄ± â–kaÃ§ Ä±r dÄ± ÄŸÄ±nÄ± â–belirt meye â–Ã§alÄ±ÅŸtÄ± .\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - \tHypothesis: â–sav cÄ±lar , â–abd â–tarih inin â–en â–bÃ¼yÃ¼k â–sÄ±nÄ±f landÄ±r Ä±lmÄ±ÅŸ â–mat er yal in â–en â–bÃ¼yÃ¼k â–le ak esinde , â–abd â–tarih indeki â–askeri â–gÃ¶z alt Ä±na â–sÄ±k Ä±ntÄ± â–Ã§ek tiÄŸi â–cum a , â–asker inin â–tutuk landÄ± ÄŸÄ±nÄ± â–iddia â–eden â–bir â–Ã§ok â–fÄ±r sat â–kaÃ§ Ä±r dÄ± .\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - Example #3\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - \tSource:     â–while â–cross - ex am ining â–man ning â–at â–a â–pre - t rial â–he aring â–at â–f t . â–me ade , â–mary land , â–pro sec ut or â–maj . â–ash den â–fe in â–ass er ted â–that â–records â–of â–week ly â–vis its â–man ning â–had â–with â–unit â–officers â–during â–nine â–months â–of â–det ention â–at â–quant ico , â–virginia , â–show â–no â–compl ain ts â–about â–his â–treatment .\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - \tReference:  â–f t . â–me ade â–mar y land ' deki â–dur uÅŸ ma â–Ã¶ncesi â–c els ede â–sav cÄ± â–ma j . â–as h den â–fe in â–man n ing ' i â–Ã§ap raz â–sor gu lar ken , â–qu an ti co , â–vir g ini a ' da â–dokuz â–ay â–gÃ¶z alt Ä±nda â–kal dÄ±ÄŸÄ± â–sÃ¼r ece â–birim â–mem ur larÄ±yla â–haft alÄ±k â–ziyaret lerinin â–kayÄ±t larÄ±nÄ±n â–kÃ¶tÃ¼ â–davran Ä±lmasÄ± yla â–ilgili â–bir â–ÅŸ ik ayet â–gÃ¶ster med iÄŸini â–ortaya â–koy du .\n",
            "2022-01-28 08:48:37,908 - INFO - joeynmt.training - \tHypothesis: â–f t , â–b t , â–mar y land , â–mar y land , â–pr os ter th en â–f t , â–mar y land , â–pro ter in , â–f t t , â–pr os ed Ã¼r <unk> de , â–00 â–ay lÄ±k â–tespit â–edilen , â–vir g ini a , â–teda v isi â–hakkÄ±nda â–ÅŸ ik Ã¢ yet â–ver med iÄŸini â–belirt ti .\n",
            "2022-01-28 08:48:37,908 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   114200: bleu:  15.27, loss: 73264.8047, ppl:  10.9133, duration: 79.1501s\n",
            "2022-01-28 08:48:52,204 - INFO - joeynmt.training - Epoch   2, Step:   114300, Batch Loss:     2.934678, Tokens per Sec:     3201, Lr: 0.000300\n",
            "2022-01-28 08:49:06,575 - INFO - joeynmt.training - Epoch   2, Step:   114400, Batch Loss:     3.330844, Tokens per Sec:     3175, Lr: 0.000300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a translator for our new finetuned model"
      ],
      "metadata": {
        "id": "B5qWnAmQxLWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install AfroTranslate\n",
        "from afrotranslate import MasakhaneTranslate\n",
        "\n",
        "finetuned_translator = MasakhaneTranslate(model_path=\"/content/models/entr_finetune\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "4M6VB4QexK0P",
        "outputId": "2cac592d-106d-44da-fa4a-e18bbe00bdc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-8affe51e2add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinetuned_translator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMasakhaneTranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/models/entr_finetune\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/afrotranslate/translator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, model_name, version, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \"\"\"\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/afrotranslate/translator.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_path, model_name, version, device)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mckpt_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_candidates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No checkpoint file under model directory'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_candidates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WARNING: More than one checkpoint under model directory. Taking first:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_candidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: ('No checkpoint file under model directory', '/content/models/entr_finetune')"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if you like, you can go back to Part 5 to test how your finetuned model performs on the test set"
      ],
      "metadata": {
        "id": "5tIKZG18A3KT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VIs2sJBWxsfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l929HimrxS0a"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Part 8 (homework) - Training a model from scratch\n",
        "\n",
        "## Retrieve your data & make a parallel corpus\n",
        "\n",
        "In this part we will use open corpus available from OPUS repository to train a translation model. We will first download the data, create training, development, testing sets from it and then use JoeyNMT to train a baseline model. \n",
        "\n",
        "In the next cell, you need to set the languages you want to work with and specify which corpus you want to use to train. \n",
        "\n",
        "To select a corpus go to https://opus.nlpl.eu/, enter your language pair and select one that you think is more appropriate (size, domain)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Cn3tgQLzUxwn"
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"tr\"\n",
        "\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO5IY_RywQyA",
        "outputId": "06663965-1bc8-4ee6-f622-528cac71dff0"
      },
      "source": [
        "# This will save it to a folder in our gdrive instead!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p \"/content/drive/My Drive/mt-workshop/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/mt-workshop/%s-%s-%s\" % (source_language, target_language, tag)\n",
        "\n",
        "!echo $gdrive_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/mt-workshop/en-tr-baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gA75Fs9ys8Y9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2977aa26-bec6-4e69-b72a-654090a41441"
      },
      "source": [
        "# Install opus-tools (Warning! This is not really python)\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opustools-pkg\n",
            "  Downloading opustools_pkg-0.0.52-py3-none-any.whl (80 kB)\n",
            "\u001b[?25l\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆ                            | 10 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 20 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 30 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 40 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 51 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž       | 61 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 71 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80 kB 5.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: opustools-pkg\n",
            "Successfully installed opustools-pkg-0.0.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xq-tDZVks7ZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f74b9853-c632-42cb-9c14-19f0a2583d6d"
      },
      "source": [
        "# TODO: Indicate here the ID of the corpus you want to use from OPUS\n",
        "opus_corpus = \"TED2020\" \n",
        "os.environ[\"corpus\"] = opus_corpus\n",
        "\n",
        "# Downloading our corpus \n",
        "! opus_read -d $corpus -s $src -t $tgt -wm moses -w $corpus.$src $corpus.$tgt -q\n",
        "\n",
        "# Extract the corpus file\n",
        "! gunzip ${corpus}_latest_xml_$src-$tgt.xml.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/TED2020/latest/xml/en-tr.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "   2 MB https://object.pouta.csc.fi/OPUS-TED2020/v1/xml/en-tr.xml.gz\n",
            "  47 MB https://object.pouta.csc.fi/OPUS-TED2020/v1/xml/en.zip\n",
            "  37 MB https://object.pouta.csc.fi/OPUS-TED2020/v1/xml/tr.zip\n",
            "\n",
            "  85 MB Total size\n",
            "./TED2020_latest_xml_en-tr.xml.gz ... 100% of 2 MB\n",
            "./TED2020_latest_xml_en.zip ... 100% of 47 MB\n",
            "./TED2020_latest_xml_tr.zip ... 100% of 37 MB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sazl7hv9xZFg"
      },
      "source": [
        "# Read the corpus into python lists\n",
        "source_file = opus_corpus + '.' + source_language\n",
        "target_file = opus_corpus + '.' + target_language\n",
        "\n",
        "src_all = [sentence.strip() for sentence in open(source_file).readlines()]\n",
        "tgt_all = [sentence.strip() for sentence in open(target_file).readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjcOBlYMxxTC",
        "outputId": "71fffa11-d629-4780-94fb-c1b511acb6b4"
      },
      "source": [
        "# Let's take a peek at the files\n",
        "print(\"Source size:\", len(src_all))\n",
        "print(\"Target size:\", len(tgt_all))\n",
        "print(\"--------\")\n",
        "\n",
        "peek_size = 5\n",
        "for i in range(peek_size):\n",
        "  print(\"Sent #\", i)\n",
        "  print(\"SRC:\", src_all[i])\n",
        "  print(\"TGT:\", tgt_all[i])\n",
        "  print(\"---------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source size: 374378\n",
            "Target size: 374378\n",
            "--------\n",
            "Sent # 0\n",
            "SRC: Thank you so much , Chris .\n",
            "TGT: Ã‡ok teÅŸekkÃ¼r ederim Chris .\n",
            "---------\n",
            "Sent # 1\n",
            "SRC: And it 's truly a great honor to have the opportunity to come to this stage twice ; I 'm extremely grateful .\n",
            "TGT: Bu sahnede ikinci kez yer alma fÄ±rsatÄ±na sahip olmak gerÃ§ekten bÃ¼yÃ¼k bir onur . Ã‡ok minnettarÄ±m .\n",
            "---------\n",
            "Sent # 2\n",
            "SRC: I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "TGT: Bu konferansta Ã§ok mutlu oldum , ve anlattÄ±klarÄ±mla ilgili gÃ¼zel yorumlarÄ±nÄ±z iÃ§in sizlere Ã§ok teÅŸekkÃ¼r ederim .\n",
            "---------\n",
            "Sent # 3\n",
            "SRC: And I say that sincerely , partly because ( Mock sob ) I need that .\n",
            "TGT: Bunu iÃ§tenlikle sÃ¶ylÃ¼yorum , Ã§Ã¼nkÃ¼ ... ( AÄŸlama taklidi ) Buna ihtiyacÄ±m var .\n",
            "---------\n",
            "Sent # 4\n",
            "SRC: ( Laughter ) Put yourselves in my position .\n",
            "TGT: ( Kahkahalar ) Kendinizi benim yerime koyun !\n",
            "---------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkuK3B4p2AkN"
      },
      "source": [
        "## Making training, development and testing sets\n",
        "\n",
        "We need to pick training, development and testing sets from our corpus. Training set will contain the sentences that we'll teach our model. Development set will be used to see how our model is progressing during the training. And finally, testing set will be used to evaluate the model.\n",
        "\n",
        "You can optionally load your own testing set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkcE9T75y9za",
        "outputId": "5a45acc5-04ac-4334-f53b-2518fd0c6f4c"
      },
      "source": [
        "# TODO: Determine ratios of each set\n",
        "all_size = len(src_all)\n",
        "dev_size = 1000\n",
        "test_size = 1000\n",
        "train_size = all_size - test_size - dev_size\n",
        "\n",
        "src_train = src_all[0:train_size]\n",
        "tgt_train = tgt_all[0:train_size]\n",
        "\n",
        "src_dev = src_all[train_size:train_size+dev_size]\n",
        "tgt_dev = tgt_all[train_size:train_size+dev_size]\n",
        "\n",
        "src_test = src_all[train_size+dev_size:all_size]\n",
        "tgt_test = tgt_all[train_size+dev_size:all_size]\n",
        "\n",
        "print(\"Set sizes\")\n",
        "print(\"All:\", len(src_all))\n",
        "print(\"Train:\", len(src_train))\n",
        "print(\"Dev:\", len(src_dev))\n",
        "print(\"Test:\", len(src_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Set sizes\n",
            "All: 374378\n",
            "Train: 372378\n",
            "Dev: 1000\n",
            "Test: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# Preprocessing the Data into Subword BPE Tokens\n",
        "\n",
        "- One of the most powerful improvements for neural machine translation is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- BPE tokenization limits the number of vocabulary into a certain size by smartly dividing words into subwords\n",
        "\n",
        "- This is especially useful for agglutinative languages (like Turkish) where vocabulary is effectively endless. \n",
        "\n",
        "- Below you have the scripts for doing BPE tokenization of our data. We use bpemb library that has pre-trained BPE models to convert our data into subwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xYKgReL6A76",
        "outputId": "b61b48f6-873b-4c78-9b45-14e5f81071be"
      },
      "source": [
        "! pip install bpemb\n",
        "from bpemb import BPEmb\n",
        "\n",
        "BPE_VOCAB_SIZE = 5000\n",
        "bpemb_src = BPEmb(lang=source_language, vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=False)\n",
        "bpemb_tgt = BPEmb(lang=target_language, vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bpemb\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bpemb) (4.41.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from bpemb) (3.6.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2 MB 10.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bpemb) (2.23.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (5.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Installing collected packages: sentencepiece, bpemb\n",
            "Successfully installed bpemb-0.3.3 sentencepiece-0.1.96\n",
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs5000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 315918/315918 [00:00<00:00, 557183.59B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/tr/tr.wiki.bpe.vs5000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 315775/315775 [00:00<00:00, 713720.23B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_RWTDq169WQ",
        "outputId": "e2c68f65-1b1a-4b3b-de0a-e8383b822473"
      },
      "source": [
        "# Testing BPE encoding\n",
        "encoded_tokens = bpemb_src.encode(\"This is a test sentence to demonstrate how BPE encoding works for our source language.\")\n",
        "print(encoded_tokens)\n",
        "\n",
        "encoded_string = \" \".join(encoded_tokens)\n",
        "print(encoded_string)\n",
        "\n",
        "decoded_string = bpemb_src.decode(encoded_tokens)\n",
        "print(decoded_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['â–', 'T', 'h', 'is', 'â–is', 'â–a', 'â–test', 'â–sent', 'ence', 'â–to', 'â–demonstr', 'ate', 'â–how', 'â–', 'BPE', 'â–enc', 'od', 'ing', 'â–works', 'â–for', 'â–our', 'â–source', 'â–language', '.']\n",
            "â– T h is â–is â–a â–test â–sent ence â–to â–demonstr ate â–how â– BPE â–enc od ing â–works â–for â–our â–source â–language .\n",
            "This is a test sentence to demonstrate how BPE encoding works for our source language.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMMbGLEX8pct"
      },
      "source": [
        "# Shortcut functions to encode and decode\n",
        "def encode_bpe(string, lang, to_lower=True):\n",
        "  if to_lower:\n",
        "    string = string.lower()\n",
        "  if lang == source_language:\n",
        "    return \" \".join(bpemb_src.encode(string))\n",
        "  elif lang == target_language:\n",
        "    return \" \".join(bpemb_tgt.encode(string))\n",
        "  else:\n",
        "    return \"\"\n",
        "\n",
        "def decode_bpe(string, lang):\n",
        "  tokens = string.strip().split()\n",
        "  if lang == source_language:\n",
        "    return bpemb_src.decode(tokens)\n",
        "  elif lang == target_language:\n",
        "    return bpemb_tgt.decode(tokens)\n",
        "  else:\n",
        "    return \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fMMGYQ27ZUN"
      },
      "source": [
        "# Let's encode all our sets with BPE\n",
        "src_train_bpe = [encode_bpe(sentence, source_language) for sentence in src_train]\n",
        "tgt_train_bpe = [encode_bpe(sentence, target_language) for sentence in tgt_train]\n",
        "\n",
        "src_dev_bpe = [encode_bpe(sentence, source_language) for sentence in src_dev]\n",
        "tgt_dev_bpe = [encode_bpe(sentence, target_language) for sentence in tgt_dev]\n",
        "\n",
        "src_test_bpe = [encode_bpe(sentence, source_language) for sentence in src_test]\n",
        "tgt_test_bpe = [encode_bpe(sentence, target_language) for sentence in tgt_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRQ7hNIS0D0z"
      },
      "source": [
        "# Now let's write all our sets into separate files\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_train, tgt_train):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_dev, tgt_dev):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"test.\"+source_language, \"w\") as src_file, open(\"test.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_test, tgt_test):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"train.bpe.\"+source_language, \"w\") as src_file, open(\"train.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_train_bpe, tgt_train_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"dev.bpe.\"+source_language, \"w\") as src_file, open(\"dev.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_dev_bpe, tgt_dev_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"test.bpe.\"+source_language, \"w\") as src_file, open(\"test.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_test_bpe, tgt_test_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "hxxBOCA-xXhy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75594eef-8053-4f99-e1e7-b07c1c54003b"
      },
      "source": [
        "# Doublecheck the files. There should be no extra quotation marks or weird characters.\n",
        "! head -n5 train.*\n",
        "! head -n5 dev.*\n",
        "! head -n5 test.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.bpe.en <==\n",
            "â–than k â–you â–so â–much â–, â–ch ris â–.\n",
            "â–and â–it â–' s â–tr u ly â–a â–great â–honor â–to â–have â–the â–opportun ity â–to â–come â–to â–this â–stage â–twice â–; â–i â–' m â–extrem ely â–gr ate ful â–.\n",
            "â–i â–have â–been â–bl own â–away â–by â–this â–conference â–, â–and â–i â–want â–to â–than k â–all â–of â–you â–for â–the â–many â–n ice â–com ments â–about â–what â–i â–had â–to â–say â–the â–other â–night â–.\n",
            "â–and â–i â–say â–that â–s inc er ely â–, â–part ly â–because â–( â–m ock â–so b â– ) â–i â–need â–that â–.\n",
            "â–( â–la ugh ter â– ) â–put â–y ours elves â–in â–my â–position â–.\n",
            "\n",
            "==> train.bpe.tr <==\n",
            "â–Ã§ok â–teÅŸ ek kÃ¼r â–eder im â–chris â–.\n",
            "â–bu â–sahne de â–ikinci â–kez â–yer â–al ma â–fÄ±r sat Ä±na â–sahip â–olmak â–ger Ã§ ekten â–bÃ¼yÃ¼k â–bir â–onur â–. â–Ã§ok â–min net tar Ä±m â–.\n",
            "â–bu â–konfer ans ta â–Ã§ok â–mut lu â–ol d um â–, â–ve â–anlat t Ä±klarÄ± m la â–ilgili â–gÃ¼zel â–yorum larÄ±n Ä±z â–iÃ§in â–s iz lere â–Ã§ok â–teÅŸ ek kÃ¼r â–eder im â–.\n",
            "â–bunu â–iÃ§ ten likle â–sÃ¶y l Ã¼yor um â–, â–Ã§Ã¼nkÃ¼ â–... â–( â–aÄŸ lama â–tak li di â–) â–buna â–ihtiy ac Ä±m â–var â–.\n",
            "â–( â–kah k ah alar â–) â–kend in izi â–benim â–yer ime â–koy un â– !\n",
            "\n",
            "==> train.en <==\n",
            "Thank you so much , Chris .\n",
            "And it 's truly a great honor to have the opportunity to come to this stage twice ; I 'm extremely grateful .\n",
            "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "And I say that sincerely , partly because ( Mock sob ) I need that .\n",
            "( Laughter ) Put yourselves in my position .\n",
            "\n",
            "==> train.tr <==\n",
            "Ã‡ok teÅŸekkÃ¼r ederim Chris .\n",
            "Bu sahnede ikinci kez yer alma fÄ±rsatÄ±na sahip olmak gerÃ§ekten bÃ¼yÃ¼k bir onur . Ã‡ok minnettarÄ±m .\n",
            "Bu konferansta Ã§ok mutlu oldum , ve anlattÄ±klarÄ±mla ilgili gÃ¼zel yorumlarÄ±nÄ±z iÃ§in sizlere Ã§ok teÅŸekkÃ¼r ederim .\n",
            "Bunu iÃ§tenlikle sÃ¶ylÃ¼yorum , Ã§Ã¼nkÃ¼ ... ( AÄŸlama taklidi ) Buna ihtiyacÄ±m var .\n",
            "( Kahkahalar ) Kendinizi benim yerime koyun !\n",
            "==> dev.bpe.en <==\n",
            "â–it â–' s â–a â–very â–big â–signal â–; â–it â–' s â–sent â–to â–the â–bra in â–says â–, â–\" â–go â–and â–e at â–. â–\"\n",
            "â–you â–have â–stop â–sign als â–- - â–we â–have â–up â–to â–eight â–stop â–sign als â–.\n",
            "â–at â–least â–in â–my â–case â–, â–they â–are â–not â–list ened â–to â–.\n",
            "â–( â–la ugh ter â– ) â–so â–what â–happ ens â–if â–the â–big â–bra in â–in â–the â–integr ation â–over r ides â–the â–signal â– ?\n",
            "â–so â–if â–you â–over r ide â–the â–hun ger â–signal â–, â–you â–can â–have â–a â–dis order â–, â–which â–is â–called â–an ore x ia â–.\n",
            "\n",
            "==> dev.bpe.tr <==\n",
            "â–bu â–Ã§ok â–gÃ¼Ã§lÃ¼ â–bir â–sin yal dir â–. â–bey ine â–gider â–ve â–der â–ki â–, â–\" â–git â–ve â–ye â–. â–\"\n",
            "â–ayrÄ±ca â–dur â–sin yal leri â–de â–vardÄ±r . â–hemen â–hemen â–sekiz â–tane â–farklÄ± â–dur â–sin yal imiz â–var â–.\n",
            "â–ama â–benim â–g ib iler â–bu â–sin yal leri â–pek â–de â–din lem iyor lar â–.\n",
            "â–( â–gÃ¼l Ã¼ÅŸ meler â–) â–p eki â–, â–eÄŸer â–bÃ¼yÃ¼k â–bey in â–bu â–gÃ¶nder ilen â–sin yal i â–gÃ¶r mez den â–gelir se â–ne â–olur â– ?\n",
            "â–eÄŸer â–aÃ§ lÄ±k â–sin yal ini â–gÃ¶r mez den â–gelir sen iz â–an or ek si â–den en â–hast alÄ± ÄŸa â–tut ulur sun uz â–.\n",
            "\n",
            "==> dev.en <==\n",
            "It 's a very big signal ; it 's sent to the brain says , \" Go and eat . \"\n",
            "You have stop signals -- we have up to eight stop signals .\n",
            "At least in my case , they are not listened to .\n",
            "( Laughter ) So what happens if the big brain in the integration overrides the signal ?\n",
            "So if you override the hunger signal , you can have a disorder , which is called anorexia .\n",
            "\n",
            "==> dev.tr <==\n",
            "Bu Ã§ok gÃ¼Ã§lÃ¼ bir sinyaldir . Beyine gider ve der ki , \" Git ve ye . \"\n",
            "AyrÄ±ca dur sinyalleri de vardÄ±r. hemen hemen sekiz tane farklÄ± dur sinyalimiz var .\n",
            "Ama benim gibiler bu sinyalleri pek de dinlemiyorlar .\n",
            "( GÃ¼lÃ¼ÅŸmeler ) Peki , eÄŸer bÃ¼yÃ¼k beyin bu gÃ¶nderilen sinyali gÃ¶rmezden gelirse ne olur ?\n",
            "EÄŸer aÃ§lÄ±k sinyalini gÃ¶rmezden gelirseniz anoreksi denen hastalÄ±ÄŸa tutulursunuz .\n",
            "==> test.bpe.en <==\n",
            "â–it â–' s â–something â–called â–the â–re ward â–sched ule â–.\n",
            "â–and â–by â–this â–, â–i â–mean â–look ing â–at â–what â–mill ions â–upon â–mill ions â–of â–people â–have â–done â–and â–care ful ly â–cal ib r ating â–the â–rate â–, â–the â–nature â–, â–the â–type â–, â–the â–int ensity â–of â–re wards â–in â–games â–to â–keep â–them â–eng aged â–over â–st ag ger ing â–amount s â–of â–time â–and â–effort â–.\n",
            "â–now â–, â–to â–t ry â–and â–expl ain â–this â–in â–s ort â–of â–real â–terms â–, â–i â–want â–to â–talk â–about â–a â–kind â–of â–t ask â–that â–might â–fall â–to â–you â–in â–so â–many â–games â–.\n",
            "â–go â–and â–get â–a â–certain â–amount â–of â–a â–certain â–little â–game - y â–it em â–.\n",
            "â–let â–' s â–say â–, â–for â–the â–s ake â–of â–arg ument â–, â–my â–mission â–is â–to â–get â– 15 â–p ies â–and â–i â–can â–get â– 15 â–p ies â–by â–kill ing â–these â–c ute â–, â–little â–mon st ers â–.\n",
            "\n",
            "==> test.bpe.tr <==\n",
            "â–Ã¶dÃ¼l â–tak v imi â–den iyor â–.\n",
            "â–bununla â–, â–milyon lar ca â–insan Ä±n â–ne â–yaptÄ± ÄŸÄ±na â–ve â–sÃ¼r es iz â–zaman â–ve â–Ã§ aba â–har c ay arak â–oyun lara â–baÄŸlÄ± â–kal malar Ä±nÄ± â–saÄŸlayan â–Ã¶dÃ¼l lerin â–tÃ¼rÃ¼ â–, â–Ã§eÅŸ idi â–ve â–Ã¶lÃ§ Ã¼s Ã¼nÃ¼n â–ay ar lan masÄ±na â–dikkat le â–bak mayÄ± â–k ast ed iyor um â–.\n",
            "â–ÅŸim di â–, â–bunu â–gerÃ§ek â–anlam da â–den em ek â–ve â–aÃ§Ä±klam ak â–iÃ§in â–birÃ§ok â–oy unda â–karÅŸÄ±laÅŸ abilece ÄŸin iz â–bir â–gÃ¶rev â–hakkÄ±nda â–konuÅŸ mak â–ist iyor um â–.\n",
            "â–gi di p â–kÃ¼Ã§Ã¼k â–bir â–oyun â–nes n esinden â–belirli â–miktar da â–getir in â–.\n",
            "â–Ã¶rnek â–olmasÄ± â–iÃ§in â–far z ed el im â–ki â–benim â–gÃ¶rev im â– 15 â–tane â–Ã§ Ã¶r ek â–getir mek â–ve â–ben â– 15 â–tane â–Ã§ Ã¶r eÄŸi â–ÅŸir in â–, â–kÃ¼Ã§Ã¼k â–can av ar larÄ± â–Ã¶ldÃ¼r erek â–getir ebilir im â–.\n",
            "\n",
            "==> test.en <==\n",
            "It 's something called the reward schedule .\n",
            "And by this , I mean looking at what millions upon millions of people have done and carefully calibrating the rate , the nature , the type , the intensity of rewards in games to keep them engaged over staggering amounts of time and effort .\n",
            "Now , to try and explain this in sort of real terms , I want to talk about a kind of task that might fall to you in so many games .\n",
            "Go and get a certain amount of a certain little game-y item .\n",
            "Let 's say , for the sake of argument , my mission is to get 15 pies and I can get 15 pies by killing these cute , little monsters .\n",
            "\n",
            "==> test.tr <==\n",
            "Ã–dÃ¼l Takvimi deniyor .\n",
            "Bununla , milyonlarca insanÄ±n ne yaptÄ±ÄŸÄ±na ve sÃ¼resiz zaman ve Ã§aba harcayarak oyunlara baÄŸlÄ± kalmalarÄ±nÄ± saÄŸlayan Ã¶dÃ¼llerin tÃ¼rÃ¼ , Ã§eÅŸidi ve Ã¶lÃ§Ã¼sÃ¼nÃ¼n ayarlanmasÄ±na dikkatle bakmayÄ± kastediyorum .\n",
            "Åžimdi , bunu gerÃ§ek anlamda denemek ve aÃ§Ä±klamak iÃ§in birÃ§ok oyunda karÅŸÄ±laÅŸabileceÄŸiniz bir gÃ¶rev hakkÄ±nda konuÅŸmak istiyorum .\n",
            "Gidip kÃ¼Ã§Ã¼k bir oyun nesnesinden belirli miktarda getirin .\n",
            "Ã–rnek olmasÄ± iÃ§in farzedelim ki benim gÃ¶revim 15 tane Ã§Ã¶rek getirmek ve ben 15 tane Ã§Ã¶reÄŸi ÅŸirin , kÃ¼Ã§Ã¼k canavarlarÄ± Ã¶ldÃ¼rerek getirebilirim .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "IlMitUHR8Qy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "593e67cf-6fd0-437b-c3f3-f5991f564715"
      },
      "source": [
        "# If creating data for the first time, move all prepared data to the mounted location in google drive\n",
        "! mkdir \"$gdrive_path\"/data\n",
        "! cp train.* \"$gdrive_path\"/data\n",
        "! cp test.* \"$gdrive_path\"/data\n",
        "! cp dev.* \"$gdrive_path\"/data\n",
        "! ls \"$gdrive_path\"/data  #See the contents of the drive directory"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev.bpe.en  dev.en  test.bpe.en  test.en  train.bpe.en\ttrain.en\n",
            "dev.bpe.tr  dev.tr  test.bpe.tr  test.tr  train.bpe.tr\ttrain.tr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwNhkXPXtFtx"
      },
      "source": [
        "# OR... If continuing from previous run, load files from drive\n",
        "! cp \"$gdrive_path\"/data/dev.* .\n",
        "! cp \"$gdrive_path\"/data/train.* .\n",
        "! cp \"$gdrive_path\"/data/test.* ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iBRMm4kMxZ8L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe0350bc-e88f-482c-ac33-d905b748b4a7"
      },
      "source": [
        "#IMPORTANT: Restart runtime if you have installed AfroTranslate\n",
        "\n",
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'joeynmt' already exists and is not an empty directory.\n",
            "Processing /content/joeynmt\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (1.20.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (57.4.0)\n",
            "Collecting torch>=1.9.0\n",
            "  Using cached torch-1.10.2-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (2.7.0)\n",
            "Collecting torchtext>=0.10.0\n",
            "  Using cached torchtext-0.11.2-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n",
            "Requirement already satisfied: sacrebleu>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (2.0.0)\n",
            "Requirement already satisfied: subword-nmt in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (0.3.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (0.11.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (6.0)\n",
            "Requirement already satisfied: pylint>=2.9.6 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (2.12.2)\n",
            "Requirement already satisfied: six>=1.12 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (1.12.0)\n",
            "Requirement already satisfied: wrapt==1.11.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (1.11.1)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (2.4.1)\n",
            "Requirement already satisfied: toml>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (0.10.2)\n",
            "Requirement already satisfied: astroid<2.10,>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (2.9.3)\n",
            "Requirement already satisfied: isort<6,>=4.2.5 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (5.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (3.10.0.2)\n",
            "Requirement already satisfied: mccabe<0.7,>=0.6 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (0.6.1)\n",
            "Requirement already satisfied: typed-ast<2.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.10,>=2.9.0->pylint>=2.9.6->joeynmt==1.5.1) (1.5.2)\n",
            "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.10,>=2.9.0->pylint>=2.9.6->joeynmt==1.5.1) (1.7.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.5.1) (2019.12.20)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.5.1) (0.4.4)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.5.1) (2.3.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.5.1) (0.8.9)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (1.43.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (3.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (1.35.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.5.1) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.5.1) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.5.1) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.5.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.5.1) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.5.1) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.5.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.5.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.5.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.5.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.5.1) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.5.1) (3.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext>=0.10.0->joeynmt==1.5.1) (4.62.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.5.1) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.5.1) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.5.1) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.5.1) (0.11.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.5.1) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.5.1) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.5.1) (2018.9)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.7/dist-packages (from subword-nmt->joeynmt==1.5.1) (4.0.3)\n",
            "Building wheels for collected packages: joeynmt\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-1.5.1-py3-none-any.whl size=86003 sha256=f0fd9a53198708427af3f8e21bba478d2a83c8bf1f6b08083466061fbae5bf19\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c7_9zzy7/wheels/0a/f4/bf/6c9d3b8efbfece6cd209f865be37382b02e7c3584df2e28ca4\n",
            "Successfully built joeynmt\n",
            "Installing collected packages: torch, torchtext, joeynmt\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.8.0+cu101\n",
            "    Uninstalling torch-1.8.0+cu101:\n",
            "      Successfully uninstalled torch-1.8.0+cu101\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.9.0\n",
            "    Uninstalling torchtext-0.9.0:\n",
            "      Successfully uninstalled torchtext-0.9.0\n",
            "  Attempting uninstall: joeynmt\n",
            "    Found existing installation: joeynmt 1.3\n",
            "    Uninstalling joeynmt-1.3:\n",
            "      Successfully uninstalled joeynmt-1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.10.2 which is incompatible.\n",
            "afrotranslate 0.0.6 requires joeynmt==1.3, but you have joeynmt 1.5.1 which is incompatible.\u001b[0m\n",
            "Successfully installed joeynmt-1.5.1 torch-1.10.2 torchtext-0.11.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu101\n",
            "  Using cached https://download.pytorch.org/whl/cu101/torch-1.8.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.5 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.20.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.2\n",
            "    Uninstalling torch-1.10.2:\n",
            "      Successfully uninstalled torch-1.10.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "torchtext 0.11.2 requires torch==1.10.2, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "joeynmt 1.5.1 requires torch>=1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "afrotranslate 0.0.6 requires joeynmt==1.3, but you have joeynmt 1.5.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0+cu101\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "H-TyjtmXB1mL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c45c71b7-9fcd-42e6-ea03-2c29c0bd3115"
      },
      "source": [
        "#Move everything important under joeynmt directory\n",
        "os.environ[\"data_path\"] = os.path.join(\"joeynmt\", \"data\", source_language + target_language)\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! head -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman\n",
        "\n",
        "# Backup vocab to drive\n",
        "! cp joeynmt/data/$src$tgt/vocab.txt \"$gdrive_path\"/data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev.bpe.en  dev.en  test.bpe.en  test.en  train.bpe.en\ttrain.en\n",
            "dev.bpe.tr  dev.tr  test.bpe.tr  test.tr  train.bpe.tr\ttrain.tr\n",
            "Combined BPE Vocab\n",
            "774\n",
            "531\n",
            "883\n",
            "6397\n",
            "794\n",
            "431\n",
            "381\n",
            "1414\n",
            "761\n",
            "548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PIs1lY2hxMsl"
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"models/entr_transformer/1000.ckpt\"\n",
        "    load_model: \"{gdrive_path}/models/{name}_transformer/1000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "6ZBPFwT94WpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8be06a9-c20c-41e6-f44a-7bffdf87dd8a"
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 16:52:38,026 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-28 16:52:38,058 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-28 16:52:46,845 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-28 16:52:48,172 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-28 16:52:48,225 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-28 16:52:48,238 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-28 16:52:48,238 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-28 16:52:48,484 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-28 16:52:48.642085: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-28 16:52:49,639 - INFO - joeynmt.training - Total params: 13372928\n",
            "2021-07-28 16:52:52,915 - INFO - joeynmt.training - Loading model from /content/drive/My Drive/mt-workshop/en-tr-baseline/models/entr_transformer/1000.ckpt\n",
            "2021-07-28 16:52:53,369 - INFO - joeynmt.helpers - cfg.name                           : entr_transformer\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.src                       : en\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.trg                       : tr\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.train                     : data/entr/train.bpe\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.dev                       : data/entr/dev.bpe\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.test                      : data/entr/test.bpe\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : data/entr/vocab.txt\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : data/entr/vocab.txt\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/drive/My Drive/mt-workshop/en-tr-baseline/models/entr_transformer/1000.ckpt\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1000\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/entr_transformer\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 370216,\n",
            "\tvalid 1000,\n",
            "\ttest 1000\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] â–than k â–you â–so â–much â–, â–ch ris â–.\n",
            "\t[TRG] â–Ã§ok â–teÅŸ ek kÃ¼r â–eder im â–chris â–.\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) â–. (5) â–, (6) â–the (7) â–' (8) â–and (9) â–\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) â–. (5) â–, (6) â–the (7) â–' (8) â–and (9) â–\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - Number of Src words (types): 9034\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - Number of Trg words (types): 9034\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=9034),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=9034))\n",
            "2021-07-28 16:52:53,383 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-28 16:52:53,383 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-28 16:53:08,820 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     5.040347, Tokens per Sec:    15405, Lr: 0.000300\n",
            "2021-07-28 16:53:23,770 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.679378, Tokens per Sec:    16521, Lr: 0.000300\n",
            "2021-07-28 16:53:39,046 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     4.687664, Tokens per Sec:    16342, Lr: 0.000300\n",
            "2021-07-28 16:53:54,321 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.884373, Tokens per Sec:    15794, Lr: 0.000300\n",
            "2021-07-28 16:54:09,629 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     4.775616, Tokens per Sec:    15877, Lr: 0.000300\n",
            "2021-07-28 16:54:25,084 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     3.958221, Tokens per Sec:    15485, Lr: 0.000300\n",
            "2021-07-28 16:54:41,036 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     4.736360, Tokens per Sec:    15393, Lr: 0.000300\n",
            "2021-07-28 16:54:57,260 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.919059, Tokens per Sec:    14830, Lr: 0.000300\n",
            "2021-07-28 16:55:13,384 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     4.417824, Tokens per Sec:    14910, Lr: 0.000300\n",
            "2021-07-28 16:55:29,522 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.567585, Tokens per Sec:    15001, Lr: 0.000300\n",
            "2021-07-28 16:56:17,824 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-28 16:56:17,825 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-28 16:56:18,299 - INFO - joeynmt.training - Example #0\n",
            "2021-07-28 16:56:18,299 - INFO - joeynmt.training - \tSource:     â–it â–' s â–a â–very â–big â–signal â–; â–it â–' s â–sent â–to â–the â–bra in â–says â–, â–\" â–go â–and â–e at â–. â–\"\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tReference:  â–bu â–Ã§ok â–gÃ¼Ã§lÃ¼ â–bir â–sin yal dir â–. â–bey ine â–gider â–ve â–der â–ki â–, â–\" â–git â–ve â–ye â–. â–\"\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tHypothesis: â–bu â–yÃ¼zden â–, â–\" â–bu â–yÃ¼zden â–, â–\" â–d ed iÄŸ in iz â–, â–\" â–d ed iÄŸ im â–, â–\"\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - Example #1\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tSource:     â–you â–have â–stop â–sign als â–- - â–we â–have â–up â–to â–eight â–stop â–sign als â–.\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tReference:  â–ayrÄ±ca â–dur â–sin yal leri â–de â–vardÄ±r . â–hemen â–hemen â–sekiz â–tane â–farklÄ± â–dur â–sin yal imiz â–var â–.\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tHypothesis: â–bu â–yÃ¼zden â–, â–bu â–yÃ¼zden â–, â–bu â–yÃ¼zden â–, â–bu â–yÃ¼zden â–, â–bu â–yÃ¼zden â–, â–bu â–yÃ¼zden â–, â–bu â–yÃ¼zden â–bir â–ÅŸey â–.\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - Example #2\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tSource:     â–at â–least â–in â–my â–case â–, â–they â–are â–not â–list ened â–to â–.\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tReference:  â–ama â–benim â–g ib iler â–bu â–sin yal leri â–pek â–de â–din lem iyor lar â–.\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tHypothesis: â–bu â–yÃ¼zden â–, â–bu â–yÃ¼zden â–, â–bu â–yÃ¼zden â–, â–bu â–yÃ¼zden â–, â–bu â–yÃ¼zden â–,\n",
            "2021-07-28 16:56:18,301 - INFO - joeynmt.training - Example #3\n",
            "2021-07-28 16:56:18,301 - INFO - joeynmt.training - \tSource:     â–( â–la ugh ter â– ) â–so â–what â–happ ens â–if â–the â–big â–bra in â–in â–the â–integr ation â–over r ides â–the â–signal â– ?\n",
            "2021-07-28 16:56:18,301 - INFO - joeynmt.training - \tReference:  â–( â–gÃ¼l Ã¼ÅŸ meler â–) â–p eki â–, â–eÄŸer â–bÃ¼yÃ¼k â–bey in â–bu â–gÃ¶nder ilen â–sin yal i â–gÃ¶r mez den â–gelir se â–ne â–olur â– ?\n",
            "2021-07-28 16:56:18,301 - INFO - joeynmt.training - \tHypothesis: â–( â–gÃ¼l Ã¼ÅŸ meler â–) â–bu â–yÃ¼zden â–, â–bu â–yÃ¼zden â–bu â–yÃ¼zden â–bu â–yÃ¼zden â–bu â–ÅŸekilde â–nasÄ±l â–nasÄ±l â–nasÄ±l â–yap abilir â–mi â– ?\n",
            "2021-07-28 16:56:18,301 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     2000: bleu:   2.31, loss: 104788.8047, ppl:  65.9925, duration: 48.7782s\n",
            "2021-07-28 16:56:34,324 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     4.015805, Tokens per Sec:    15108, Lr: 0.000300\n",
            "2021-07-28 16:56:50,301 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     4.364112, Tokens per Sec:    14957, Lr: 0.000300\n",
            "2021-07-28 16:57:06,377 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     3.913055, Tokens per Sec:    15505, Lr: 0.000300\n",
            "2021-07-28 16:57:22,392 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     4.053391, Tokens per Sec:    15228, Lr: 0.000300\n",
            "2021-07-28 16:57:38,524 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     4.355020, Tokens per Sec:    15342, Lr: 0.000300\n",
            "2021-07-28 16:57:54,767 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.925006, Tokens per Sec:    15665, Lr: 0.000300\n",
            "2021-07-28 16:58:10,709 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     3.960259, Tokens per Sec:    15288, Lr: 0.000300\n",
            "2021-07-28 16:58:26,716 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     3.953107, Tokens per Sec:    15453, Lr: 0.000300\n",
            "2021-07-28 16:58:42,729 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     3.973259, Tokens per Sec:    15140, Lr: 0.000300\n",
            "2021-07-28 16:58:58,866 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     4.405763, Tokens per Sec:    15363, Lr: 0.000300\n",
            "2021-07-28 16:59:45,427 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-28 16:59:45,428 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-28 16:59:45,862 - INFO - joeynmt.training - Example #0\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tSource:     â–it â–' s â–a â–very â–big â–signal â–; â–it â–' s â–sent â–to â–the â–bra in â–says â–, â–\" â–go â–and â–e at â–. â–\"\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tReference:  â–bu â–Ã§ok â–gÃ¼Ã§lÃ¼ â–bir â–sin yal dir â–. â–bey ine â–gider â–ve â–der â–ki â–, â–\" â–git â–ve â–ye â–. â–\"\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tHypothesis: â–bu â–, â–bir â–ÅŸey â–, â–\" â–bu â–, â–\" â–bir â–ÅŸey â–, â–\" â–d ed iÄŸ im â–gibi â–, â–\"\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - Example #1\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tSource:     â–you â–have â–stop â–sign als â–- - â–we â–have â–up â–to â–eight â–stop â–sign als â–.\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tReference:  â–ayrÄ±ca â–dur â–sin yal leri â–de â–vardÄ±r . â–hemen â–hemen â–sekiz â–tane â–farklÄ± â–dur â–sin yal imiz â–var â–.\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tHypothesis: â– 2 0 â–' de â–, â– 2 0 â–' de â–, â– 2 0 â–' de â–, â– 2 0 â–' de â–.\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - Example #2\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tSource:     â–at â–least â–in â–my â–case â–, â–they â–are â–not â–list ened â–to â–.\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - \tReference:  â–ama â–benim â–g ib iler â–bu â–sin yal leri â–pek â–de â–din lem iyor lar â–.\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - \tHypothesis: â–i Ì‡ n san lar â–, â–benim â–iÃ§in â–, â–benim â–iÃ§in â–, â–benim â–iÃ§in â–Ã§ok â–iyi â–deÄŸil â–.\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - Example #3\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - \tSource:     â–( â–la ugh ter â– ) â–so â–what â–happ ens â–if â–the â–big â–bra in â–in â–the â–integr ation â–over r ides â–the â–signal â– ?\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - \tReference:  â–( â–gÃ¼l Ã¼ÅŸ meler â–) â–p eki â–, â–eÄŸer â–bÃ¼yÃ¼k â–bey in â–bu â–gÃ¶nder ilen â–sin yal i â–gÃ¶r mez den â–gelir se â–ne â–olur â– ?\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - \tHypothesis: â–( â–gÃ¼l Ã¼ÅŸ meler â–) â–bu â–, â–bu â–, â–bu â–, â–bey in iz in â–en â–az Ä±ndan â–daha â–fazla â–insan Ä±n â–nasÄ±l â– ?\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     3000: bleu:   3.29, loss: 96139.1641, ppl:  46.6989, duration: 46.9973s\n",
            "2021-07-28 17:00:01,804 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     3.237124, Tokens per Sec:    14911, Lr: 0.000300\n",
            "2021-07-28 17:00:17,811 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     3.741183, Tokens per Sec:    15031, Lr: 0.000300\n",
            "2021-07-28 17:00:34,045 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     4.225142, Tokens per Sec:    15262, Lr: 0.000300\n",
            "2021-07-28 17:00:49,975 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     4.210491, Tokens per Sec:    15347, Lr: 0.000300\n",
            "2021-07-28 17:01:06,026 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     4.105839, Tokens per Sec:    15412, Lr: 0.000300\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
            "    main()\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 35, in main\n",
            "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 805, in train\n",
            "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 440, in train_and_validate\n",
            "    self.optimizer.step()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\", line 89, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\", line 119, in step\n",
            "    group['eps'])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\", line 92, in adam\n",
            "    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "MBoDS09JM807"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "! mkdir -p \"$gdrive_path/models/${src}${tgt}_transformer/\"\n",
        "! cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qNinL_DvYhI"
      },
      "source": [
        "# OR... If continuing from previous work, load models from google drive to notebook storage  \n",
        "! mkdir -p joeynmt/models/${src}${tgt}_transformer\n",
        "! cp -r \"$gdrive_path/models/${src}${tgt}_transformer\" joeynmt/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "n94wlrCjVc17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af45ae97-0cc7-4b8c-8474-6a02108ffbba"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 1000\tLoss: 4971087.00000\tPPL: 85.69424\tbleu: 2.36058\tLR: 0.00030000\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "66WhRE9lIhoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24c5e2f-99a4-4ac2-e3aa-c8a0f86679fe"
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 16:28:56,802 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-28 16:28:56,802 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-28 16:28:57,990 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-28 16:28:58,003 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-28 16:28:58,014 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-28 16:28:58,045 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
            "2021-07-28 16:29:01,412 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-28 16:29:01,642 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-28 16:29:01,712 - INFO - joeynmt.prediction - Decoding on dev set (data/entr/dev.bpe.tr)...\n",
            "2021-07-28 16:30:08,794 - INFO - joeynmt.prediction -  dev bleu[13a]:   0.97 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-28 16:30:08,795 - INFO - joeynmt.prediction - Decoding on test set (data/entr/test.bpe.tr)...\n",
            "2021-07-28 16:31:30,255 - INFO - joeynmt.prediction - test bleu[13a]:   0.75 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-RmEmRGD7Sb"
      },
      "source": [
        "# Fine-tuning to domain\n",
        "\n",
        "One important technique in neural machine translation is in-domain adaptation or fine-tuning. This introduces the model a certain domain we're interested to do translations in. \n",
        "\n",
        "One simple way of doing this is having a pre-trained model and continuing training from it on our in-domain training set. \n",
        "\n",
        "In this example we're going to fine-tune our model to news. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXNGfXzZEbS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1853ccfc-bb8d-48ad-c56a-41e52441b9cd"
      },
      "source": [
        "fine_corpus = \"WMT-News\" \n",
        "os.environ[\"fine\"] = fine_corpus\n",
        "\n",
        "# Downloading our corpus \n",
        "! opus_read -d $fine -s $src -t $tgt -wm moses -w $fine.$src $fine.$tgt -q\n",
        "\n",
        "# Extract the corpus file\n",
        "! gunzip ${fine}_latest_xml_$src-$tgt.xml.gz\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/WMT-News/latest/xml/en-tr.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "  92 KB https://object.pouta.csc.fi/OPUS-WMT-News/v2019/xml/en-tr.xml.gz\n",
            "  63 MB https://object.pouta.csc.fi/OPUS-WMT-News/v2019/xml/en.zip\n",
            "   2 MB https://object.pouta.csc.fi/OPUS-WMT-News/v2019/xml/tr.zip\n",
            "\n",
            "  66 MB Total size\n",
            "./WMT-News_latest_xml_en-tr.xml.gz ... 100% of 92 KB\n",
            "./WMT-News_latest_xml_en.zip ... 100% of 63 MB\n",
            "./WMT-News_latest_xml_tr.zip ... 100% of 2 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjVYnALNFcyP"
      },
      "source": [
        "# Read the corpus into python lists\n",
        "source_file = fine_corpus + '.' + source_language\n",
        "target_file = fine_corpus + '.' + target_language\n",
        "\n",
        "fine_src_all_bpe = [encode_bpe(sentence.strip(),'en') for sentence in open(source_file).readlines()]\n",
        "fine_tgt_all_bpe = [encode_bpe(sentence.strip(), 'tr') for sentence in open(target_file).readlines()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maxV-WTBFwbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f475eb09-2054-4c7e-8c63-f3483a4d3e66"
      },
      "source": [
        "# Let's take a peek at the files\n",
        "print(\"Source size:\", len(fine_src_all_bpe))\n",
        "print(\"Target size:\", len(fine_tgt_all_bpe))\n",
        "print(\"--------\")\n",
        "\n",
        "peek_size = 5\n",
        "for i in range(peek_size):\n",
        "  print(\"Sent #\", i)\n",
        "  print(\"SRC:\", decode_bpe(fine_src_all_bpe[i], 'en'))\n",
        "  print(\"TGT:\", decode_bpe(fine_tgt_all_bpe[i],'tr'))\n",
        "  print(\"---------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source size: 20016\n",
            "Target size: 20016\n",
            "--------\n",
            "Sent # 0\n",
            "SRC: two people drowned in floods in trabzon\n",
            "TGT: trabzon ' da sel iki kiÅŸiyi yuttu\n",
            "---------\n",
            "Sent # 1\n",
            "SRC: the ikisu creek overflowed on account of heavy rainfall in the district of yomra in trabzon .\n",
            "TGT: trabzon â€™ un yomra ilÃ§esinde etkili olan saÄŸanak yaÄŸÄ±ÅŸ nedeniyle iÌ‡kisu deresi taÅŸtÄ± .\n",
            "---------\n",
            "Sent # 2\n",
            "SRC: two women disappeared in the floodwaters in the village of tasdelen and the road to the village of sayvan was closed .\n",
            "TGT: taÅŸdelen kÃ¶yÃ¼nde sele kapÄ±lan iki kadÄ±n kaybolurken , sayvan kÃ¶yÃ¼ yolu ulaÅŸÄ±ma kapandÄ± .\n",
            "---------\n",
            "Sent # 3\n",
            "SRC: the body of one of the women drowned in the floods was found .\n",
            "TGT: selde kayÄ±p olan iki kadÄ±ndan birinin cesedine ulaÅŸÄ±ldÄ± .\n",
            "---------\n",
            "Sent # 4\n",
            "SRC: there was precipitation in the highlands of yomra at around 15 : 00 .\n",
            "TGT: yaÄŸÄ±ÅŸ , yomra â€™ nÄ±n yÃ¼ksek kesiminde saat 15.00 sÄ±ralarÄ±nda etkili oldu .\n",
            "---------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HcjsY5UF_Pk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30845b8-1a5c-4aa4-e435-4ee9af689b1c"
      },
      "source": [
        "# Allocate train, dev, test portions\n",
        "all_size = len(fine_src_all_bpe)\n",
        "dev_size = 500\n",
        "test_size = 500\n",
        "train_size = all_size - test_size - dev_size\n",
        "\n",
        "fine_src_train_bpe = fine_src_all_bpe[0:train_size]\n",
        "fine_tgt_train_bpe = fine_tgt_all_bpe[0:train_size]\n",
        "\n",
        "fine_src_dev_bpe = fine_src_all_bpe[train_size:train_size+dev_size]\n",
        "fine_tgt_dev_bpe = fine_tgt_all_bpe[train_size:train_size+dev_size]\n",
        "\n",
        "fine_src_test_bpe = fine_src_all_bpe[train_size+dev_size:all_size]\n",
        "fine_tgt_test_bpe = fine_tgt_all_bpe[train_size+dev_size:all_size]\n",
        "\n",
        "print(\"Set sizes\")\n",
        "print(\"All:\", len(fine_src_all_bpe))\n",
        "print(\"Train:\", len(fine_src_train_bpe))\n",
        "print(\"Dev:\", len(fine_src_dev_bpe))\n",
        "print(\"Test:\", len(fine_src_test_bpe))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Set sizes\n",
            "All: 20016\n",
            "Train: 19016\n",
            "Dev: 500\n",
            "Test: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvweRBGDGeXL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "88e89b04-0777-4639-a85e-68a7f24228ef"
      },
      "source": [
        "# Store sentences as files\n",
        "with open(\"finetrain.bpe.\"+source_language, \"w\") as src_file, open(\"finetrain.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(fine_src_train_bpe, fine_tgt_train_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"finedev.bpe.\"+source_language, \"w\") as src_file, open(\"finedev.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(fine_src_dev_bpe, fine_tgt_dev_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"finetest.bpe.\"+source_language, \"w\") as src_file, open(\"finetest.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(fine_src_test_bpe, fine_tgt_test_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-d15f6d523ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Store sentences as files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finetrain.bpe.\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msource_language\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finetrain.bpe.\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtarget_language\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtgt_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfine_src_train_bpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfine_tgt_train_bpe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msrc_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtgt_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fine_src_train_bpe' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbz1Hf-0G-59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4afdfa-9382-4bcf-f62a-04d524c8ff22"
      },
      "source": [
        "# If creating data for the first time, move all prepared data to the mounted location in google drive\n",
        "! mkdir -p \"$gdrive_path\"/data\n",
        "! cp finetrain.* \"$gdrive_path\"/data\n",
        "! cp finetest.* \"$gdrive_path\"/data\n",
        "! cp finedev.* \"$gdrive_path\"/data\n",
        "! ls \"$gdrive_path\"/data  #See the contents of the drive directory"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory â€˜/content/drive/My Drive/mt-workshop/en-tr-baseline/dataâ€™: File exists\n",
            "dev.bpe.en  finedev.bpe.en   finetrain.bpe.en  test.en\t     train.en\n",
            "dev.bpe.tr  finedev.bpe.tr   finetrain.bpe.tr  test.tr\t     train.tr\n",
            "dev.en\t    finetest.bpe.en  test.bpe.en       train.bpe.en\n",
            "dev.tr\t    finetest.bpe.tr  test.bpe.tr       train.bpe.tr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZP6K7qVT9n4"
      },
      "source": [
        "# OR... If continuing from previous run, load finetuning data from drive\n",
        "! cp \"$gdrive_path\"/data/finedev.* .\n",
        "! cp \"$gdrive_path\"/data/finetrain.* .\n",
        "! cp \"$gdrive_path\"/data/finetest.* .\n",
        "! cp \"$gdrive_path\"/data/finetest.* ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mjN3UUoHJN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f60f15f-3bb2-40fd-d060-2300f92ad697"
      },
      "source": [
        "# #Move everything important under joeynmt directory\n",
        "os.environ[\"data_path\"] = os.path.join(\"joeynmt\", \"data\", source_language + target_language)\n",
        "\n",
        "# Move fine-tuning data to data directory\n",
        "! mkdir -p $data_path\n",
        "! cp finetrain.* $data_path\n",
        "! cp finetest.* $data_path\n",
        "! cp finedev.* $data_path\n",
        "! ls $data_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finedev.bpe.en\tfinetest.bpe.en  finetrain.bpe.en\n",
            "finedev.bpe.tr\tfinetest.bpe.tr  finetrain.bpe.tr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dlh1pfvr4qw"
      },
      "source": [
        "# Also, load models from google drive to notebook storage  \n",
        "! mkdir -p joeynmt/models/${src}${tgt}_transformer\n",
        "! cp -r \"$gdrive_path/models/${src}${tgt}_transformer\" joeynmt/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mdL95STIzPK"
      },
      "source": [
        "# Let's create a config file for finetuning training\n",
        "# Changes from previous config are dataset names, model name, batch size and learning rate\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/finetrain.bpe\"\n",
        "    dev:   \"data/{name}/finedev.bpe\"\n",
        "    test:  \"data/{name}/finetest.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    load_model: \"{gdrive_path}/models/{name}_transformer/best.ckpt\" # Load base model from its best scoring checkpoint (from gdrive)\n",
        "    #load_model: \"models/{name}_transformer/best.ckpt\" # Load base model from its best scoring checkpoint (from local)\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0001\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 1028\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}_finetune.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU2ehibpJdrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2769748-9456-4919-9fc7-3081d3f97347"
      },
      "source": [
        "# Test our model on our domain before fine-tuning\n",
        "! cd joeynmt; python3 -m joeynmt test \"configs/transformer_${src}${tgt}_finetune.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-29 09:54:48,631 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-29 09:54:48,631 - INFO - joeynmt.data - Building vocabulary...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
            "    main()\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 38, in main\n",
            "    output_path=args.output_path, save_attention=args.save_attention)\n",
            "  File \"/content/joeynmt/joeynmt/prediction.py\", line 293, in test\n",
            "    data_cfg=cfg[\"data\"], datasets=[\"dev\", \"test\"])\n",
            "  File \"/content/joeynmt/joeynmt/data.py\", line 112, in load_data\n",
            "    dataset=train_data, vocab_file=src_vocab_file)\n",
            "  File \"/content/joeynmt/joeynmt/vocabulary.py\", line 161, in build_vocab\n",
            "    vocab = Vocabulary(file=vocab_file)\n",
            "  File \"/content/joeynmt/joeynmt/vocabulary.py\", line 40, in __init__\n",
            "    self._from_file(file)\n",
            "  File \"/content/joeynmt/joeynmt/vocabulary.py\", line 61, in _from_file\n",
            "    with open(file, \"r\", encoding='utf-8') as open_file:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'data/entr/vocab.txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-ir9d-LKez3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cb76d07-b083-4bb8-96fa-7888b8dcb796"
      },
      "source": [
        "# Train to our domain\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "! cd joeynmt; python3 -m joeynmt train \"configs/transformer_${src}${tgt}_finetune.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 17:35:43,552 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-28 17:35:43,585 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-28 17:35:52,185 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-28 17:35:53,413 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-28 17:35:53,459 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-28 17:35:53,469 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-28 17:35:53,469 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-28 17:35:53,695 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-28 17:35:53.930981: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-28 17:35:54,906 - INFO - joeynmt.training - Total params: 13372928\n",
            "2021-07-28 17:35:58,161 - INFO - joeynmt.training - Loading model from /content/drive/My Drive/mt-workshop/en-tr-baseline/models/entr_transformer/best.ckpt\n",
            "2021-07-28 17:35:58,607 - INFO - joeynmt.helpers - cfg.name                           : entr_transformer_finetune\n",
            "2021-07-28 17:35:58,607 - INFO - joeynmt.helpers - cfg.data.src                       : en\n",
            "2021-07-28 17:35:58,607 - INFO - joeynmt.helpers - cfg.data.trg                       : tr\n",
            "2021-07-28 17:35:58,607 - INFO - joeynmt.helpers - cfg.data.train                     : data/entr/finetrain.bpe\n",
            "2021-07-28 17:35:58,607 - INFO - joeynmt.helpers - cfg.data.dev                       : data/entr/finedev.bpe\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.test                      : data/entr/finetest.bpe\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : data/entr/vocab.txt\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : data/entr/vocab.txt\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/drive/My Drive/mt-workshop/en-tr-baseline/models/entr_transformer/best.ckpt\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0001\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.batch_size            : 1028\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1000\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/entr_transformer\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 370216,\n",
            "\tvalid 1000,\n",
            "\ttest 1000\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] â–than k â–you â–so â–much â–, â–ch ris â–.\n",
            "\t[TRG] â–Ã§ok â–teÅŸ ek kÃ¼r â–eder im â–chris â–.\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) â–. (5) â–, (6) â–the (7) â–' (8) â–and (9) â–\n",
            "2021-07-28 17:35:58,612 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) â–. (5) â–, (6) â–the (7) â–' (8) â–and (9) â–\n",
            "2021-07-28 17:35:58,612 - INFO - joeynmt.helpers - Number of Src words (types): 9034\n",
            "2021-07-28 17:35:58,612 - INFO - joeynmt.helpers - Number of Trg words (types): 9034\n",
            "2021-07-28 17:35:58,612 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=9034),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=9034))\n",
            "2021-07-28 17:35:58,619 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 1028\n",
            "\ttotal batch size (w. parallel & accumulation): 1028\n",
            "2021-07-28 17:35:58,620 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-28 17:36:05,528 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     5.201994, Tokens per Sec:     9767, Lr: 0.000300\n",
            "2021-07-28 17:36:11,767 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.912281, Tokens per Sec:    10804, Lr: 0.000300\n",
            "2021-07-28 17:36:18,021 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     4.973733, Tokens per Sec:    11041, Lr: 0.000300\n",
            "2021-07-28 17:36:24,255 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.809206, Tokens per Sec:    10934, Lr: 0.000300\n",
            "2021-07-28 17:36:30,681 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     4.819091, Tokens per Sec:    10656, Lr: 0.000300\n",
            "2021-07-28 17:36:37,252 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     4.839234, Tokens per Sec:    10245, Lr: 0.000300\n",
            "2021-07-28 17:36:43,827 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     3.323047, Tokens per Sec:    10317, Lr: 0.000300\n",
            "2021-07-28 17:36:50,434 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     4.597960, Tokens per Sec:    10237, Lr: 0.000300\n",
            "2021-07-28 17:36:56,883 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     4.473081, Tokens per Sec:    10245, Lr: 0.000300\n",
            "2021-07-28 17:37:03,243 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     4.721494, Tokens per Sec:    10846, Lr: 0.000300\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
            "    main()\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 35, in main\n",
            "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 805, in train\n",
            "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 475, in train_and_validate\n",
            "    valid_duration = self._validate(valid_data, epoch_no)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 563, in _validate\n",
            "    n_gpu=self.n_gpu\n",
            "  File \"/content/joeynmt/joeynmt/prediction.py\", line 125, in validate_on_data\n",
            "    n_best=n_best)\n",
            "  File \"/content/joeynmt/joeynmt/search.py\", line 444, in run_batch\n",
            "    encoder_hidden=encoder_hidden)\n",
            "  File \"/content/joeynmt/joeynmt/search.py\", line 39, in greedy\n",
            "    src_mask, max_output_length, model, encoder_output, encoder_hidden)\n",
            "  File \"/content/joeynmt/joeynmt/search.py\", line 146, in transformer_greedy\n",
            "    trg_mask=trg_mask\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/model.py\", line 104, in forward\n",
            "    outputs, hidden, att_probs, att_vectors = self._decode(**kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/model.py\", line 178, in _decode\n",
            "    **_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/decoders.py\", line 540, in forward\n",
            "    src_mask=src_mask, trg_mask=trg_mask)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/transformer_layers.py\", line 264, in forward\n",
            "    h1 = self.trg_trg_att(x_norm, x_norm, x_norm, mask=trg_mask)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/transformer_layers.py\", line 73, in forward\n",
            "    scores = scores.masked_fill(~mask.unsqueeze(1), float('-inf'))\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFYE2yj1N-di"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "! mkdir -p \"$gdrive_path/models/${src}${tgt}_transformer_finetune/\"\n",
        "! cp -r joeynmt/models/${src}${tgt}_transformer_finetune/* \"$gdrive_path/models/${src}${tgt}_transformer_finetune/\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd1_yOXANu3-"
      },
      "source": [
        "# Test again to see how our model improved\n",
        "#! cd joeynmt; python3 -m joeynmt test \"configs/transformer_${src}${tgt}_finetune.yaml\"\n",
        "\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer_finetune/config.yaml\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}