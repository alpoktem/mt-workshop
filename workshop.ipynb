{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "starter_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Machine Translation for Translators Workshop\n",
        "Localization summer school '21"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4fXCKCf36IK"
      },
      "source": [
        "### Note before beginning:\n",
        "\n",
        "#### - This coding template is based on Masakhane's starter notebook (https://github.com/masakhane-io/masakhane-mt)\n",
        "#### - The idea is that you should be able to make minimal changes to this in order to get SOME result for your own translation corpus. \n",
        "#### - The TL;DR: Go to the **\"TODO\"** comments which will tell you what to update to get up and running\n",
        "#### - If you actually want to have a clue what you're doing, read the text and peek at the links\n",
        "#### - With 100 epochs, it should take around 7 hours to run in Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l929HimrxS0a"
      },
      "source": [
        "## Retrieve your data & make a parallel corpus\n",
        "\n",
        "In this workshop we will use open corpus available from OPUS repository to train a translation model. We will first download the data, create training, development, testing sets from it and then use JoeyNMT to train a baseline model. \n",
        "\n",
        "In the next cell, you need to set the languages you want to work with and specify which corpus you want to use to train. \n",
        "\n",
        "To select a corpus go to https://opus.nlpl.eu/, enter your language pair and select one that you think is more appropriate (size, domain)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Cn3tgQLzUxwn"
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"tr\"\n",
        "opus_corpus = \"TED2020\" \n",
        "lc = False  # If True, lowercase the data.\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"corpus\"] = opus_corpus\n",
        "os.environ[\"tag\"] = tag"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO5IY_RywQyA",
        "outputId": "52776109-acd1-41c3-9638-56321c2ecb25"
      },
      "source": [
        "# This will save it to a folder in our gdrive instead!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s-%s\" % (source_language, target_language, tag)\n",
        "\n",
        "!echo $gdrive_path"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/masakhane/en-tr-baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gA75Fs9ys8Y9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b18020b-8aed-45a3-b73a-e342dd3c0262"
      },
      "source": [
        "# Install opus-tools (Warning! This is not really python)\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opustools-pkg\n",
            "  Downloading opustools_pkg-0.0.52-py3-none-any.whl (80 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |████████                        | 20 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 80 kB 3.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: opustools-pkg\n",
            "Successfully installed opustools-pkg-0.0.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xq-tDZVks7ZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1946d41-6b0c-4409-ed7a-72cbb0ec487c"
      },
      "source": [
        "# Downloading our corpus \n",
        "! opus_read -d $corpus -s $src -t $tgt -wm moses -w $corpus.$src $corpus.$tgt -q\n",
        "\n",
        "# Extract the corpus file\n",
        "! gunzip ${corpus}_latest_xml_$src-$tgt.xml.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/TED2020/latest/xml/en-tr.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "   2 MB https://object.pouta.csc.fi/OPUS-TED2020/v1/xml/en-tr.xml.gz\n",
            "  47 MB https://object.pouta.csc.fi/OPUS-TED2020/v1/xml/en.zip\n",
            "  37 MB https://object.pouta.csc.fi/OPUS-TED2020/v1/xml/tr.zip\n",
            "\n",
            "  86 MB Total size\n",
            "./TED2020_latest_xml_en-tr.xml.gz ... 100% of 2 MB\n",
            "./TED2020_latest_xml_en.zip ... 100% of 47 MB\n",
            "./TED2020_latest_xml_tr.zip ... 100% of 37 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sazl7hv9xZFg"
      },
      "source": [
        "# Read the corpus into python lists\n",
        "source_file = opus_corpus + '.' + source_language\n",
        "target_file = opus_corpus + '.' + target_language\n",
        "\n",
        "src_all = [sentence.strip() for sentence in open(source_file).readlines()]\n",
        "tgt_all = [sentence.strip() for sentence in open(target_file).readlines()]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjcOBlYMxxTC",
        "outputId": "f578fdab-e065-40b5-f2ec-62e03f6ac64e"
      },
      "source": [
        "# Let's take a peek at the files\n",
        "print(\"Source size:\", len(src_all))\n",
        "print(\"Target size:\", len(tgt_all))\n",
        "print(\"--------\")\n",
        "\n",
        "peek_size = 5\n",
        "for i in range(peek_size):\n",
        "  print(\"Sent #\", i)\n",
        "  print(\"SRC:\", src_all[i])\n",
        "  print(\"TGT:\", tgt_all[i])\n",
        "  print(\"---------\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source size: 374378\n",
            "Target size: 374378\n",
            "--------\n",
            "Sent # 0\n",
            "SRC: Thank you so much , Chris .\n",
            "TGT: Çok teşekkür ederim Chris .\n",
            "---------\n",
            "Sent # 1\n",
            "SRC: And it 's truly a great honor to have the opportunity to come to this stage twice ; I 'm extremely grateful .\n",
            "TGT: Bu sahnede ikinci kez yer alma fırsatına sahip olmak gerçekten büyük bir onur . Çok minnettarım .\n",
            "---------\n",
            "Sent # 2\n",
            "SRC: I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "TGT: Bu konferansta çok mutlu oldum , ve anlattıklarımla ilgili güzel yorumlarınız için sizlere çok teşekkür ederim .\n",
            "---------\n",
            "Sent # 3\n",
            "SRC: And I say that sincerely , partly because ( Mock sob ) I need that .\n",
            "TGT: Bunu içtenlikle söylüyorum , çünkü ... ( Ağlama taklidi ) Buna ihtiyacım var .\n",
            "---------\n",
            "Sent # 4\n",
            "SRC: ( Laughter ) Put yourselves in my position .\n",
            "TGT: ( Kahkahalar ) Kendinizi benim yerime koyun !\n",
            "---------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkuK3B4p2AkN"
      },
      "source": [
        "## Making training, development and testing sets\n",
        "\n",
        "We need to pick training, development and testing sets from our corpus. Training set will contain the sentences that we'll teach our model. Development set will be used to see how our model is progressing during the training. And finally, testing set will be used to evaluate the model.\n",
        "\n",
        "You can optionally load your own testing set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkcE9T75y9za",
        "outputId": "0e68ae75-aa3e-433d-d235-9e5e9c55b6b1"
      },
      "source": [
        "# TODO: Determine ratios of each set\n",
        "train_ratio = 0.8\n",
        "dev_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "\n",
        "all_size = len(src_all)\n",
        "train_size = int(all_size * train_ratio)\n",
        "dev_size = int(all_size * dev_ratio)\n",
        "test_size = all_size -train_size - dev_size\n",
        "\n",
        "src_train = src_all[0:train_size]\n",
        "tgt_train = tgt_all[0:train_size]\n",
        "\n",
        "src_dev = src_all[train_size:train_size+dev_size]\n",
        "tgt_dev = tgt_all[train_size:train_size+dev_size]\n",
        "\n",
        "src_test = src_all[train_size+dev_size:all_size]\n",
        "tgt_test = tgt_all[train_size+dev_size:all_size]\n",
        "\n",
        "print(\"Set sizes\")\n",
        "print(\"All:\", all_size)\n",
        "print(\"Train:\", train_size)\n",
        "print(\"Dev:\", dev_size)\n",
        "print(\"Test:\", test_size)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Set sizes\n",
            "All: 374378\n",
            "Train: 299502\n",
            "Dev: 37437\n",
            "Test: 37439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# Preprocessing the Data into Subword BPE Tokens\n",
        "\n",
        "- One of the most powerful improvements for neural machine translation is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- BPE tokenization limits the number of vocabulary into a certain size by smartly dividing words into subwords\n",
        "\n",
        "- This is especially useful for agglutinative languages (like Turkish) where vocabulary is effectively endless. \n",
        "\n",
        "- Below you have the scripts for doing BPE tokenization of our data. We use bpemb library that has pre-trained BPE models to convert our data into subwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xYKgReL6A76",
        "outputId": "d75be721-01ee-4c7f-bd9c-dfc4431c6fdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! pip install bpemb\n",
        "from bpemb import BPEmb\n",
        "\n",
        "BPE_VOCAB_SIZE = 5000\n",
        "bpemb_src = BPEmb(lang=source_language, vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=False)\n",
        "bpemb_tgt = BPEmb(lang=target_language, vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bpemb\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bpemb) (2.23.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from bpemb) (3.6.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bpemb) (4.41.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (5.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2021.5.30)\n",
            "Installing collected packages: sentencepiece, bpemb\n",
            "Successfully installed bpemb-0.3.3 sentencepiece-0.1.96\n",
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs5000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 315918/315918 [00:01<00:00, 298993.58B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/tr/tr.wiki.bpe.vs5000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 315775/315775 [00:01<00:00, 293777.40B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_RWTDq169WQ",
        "outputId": "433ff20d-7592-4e3f-c0ee-4295d1a9608f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Testing BPE encoding\n",
        "encoded_tokens = bpemb_src.encode(\"This is a test sentence to demonstrate how BPE encoding works for our source language.\")\n",
        "print(encoded_tokens)\n",
        "\n",
        "encoded_string = \" \".join(encoded_tokens)\n",
        "print(encoded_string)\n",
        "\n",
        "decoded_string = bpemb_src.decode(encoded_tokens)\n",
        "print(decoded_string)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁', 'T', 'h', 'is', '▁is', '▁a', '▁test', '▁sent', 'ence', '▁to', '▁demonstr', 'ate', '▁how', '▁', 'BPE', '▁enc', 'od', 'ing', '▁works', '▁for', '▁our', '▁source', '▁language', '.']\n",
            "▁ T h is ▁is ▁a ▁test ▁sent ence ▁to ▁demonstr ate ▁how ▁ BPE ▁enc od ing ▁works ▁for ▁our ▁source ▁language .\n",
            "This is a test sentence to demonstrate how BPE encoding works for our source language.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMMbGLEX8pct"
      },
      "source": [
        "# Shortcut functions to encode and decode\n",
        "def encode_bpe(string, lang):\n",
        "  if lang == source_language:\n",
        "    return \" \".join(bpemb_src.encode(string))\n",
        "  elif lang == target_language:\n",
        "    return \" \".join(bpemb_tgt.encode(string))\n",
        "  else:\n",
        "    return \"\"\n",
        "\n",
        "def decode_bpe(string, lang):\n",
        "  tokens = string.strip().split()\n",
        "  if lang == source_language:\n",
        "    return bpemb_src.decode(tokens)\n",
        "  elif lang == target_language:\n",
        "    return bpemb_tgt.decode(tokens)\n",
        "  else:\n",
        "    return \"\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fMMGYQ27ZUN"
      },
      "source": [
        "# Let's encode all our sets with BPE\n",
        "src_train_bpe = [encode_bpe(sentence, source_language) for sentence in src_train]\n",
        "tgt_train_bpe = [encode_bpe(sentence, target_language) for sentence in tgt_train]\n",
        "\n",
        "src_dev_bpe = [encode_bpe(sentence, source_language) for sentence in src_dev]\n",
        "tgt_dev_bpe = [encode_bpe(sentence, target_language) for sentence in tgt_dev]\n",
        "\n",
        "src_test_bpe = [encode_bpe(sentence, source_language) for sentence in src_test]\n",
        "tgt_test_bpe = [encode_bpe(sentence, target_language) for sentence in tgt_test]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRQ7hNIS0D0z"
      },
      "source": [
        "# Now let's write all our sets into separate files\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_train, tgt_train):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_dev, tgt_dev):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"test.\"+source_language, \"w\") as src_file, open(\"test.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_test, tgt_test):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"train.bpe.\"+source_language, \"w\") as src_file, open(\"train.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_train_bpe, tgt_train_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"dev.bpe.\"+source_language, \"w\") as src_file, open(\"dev.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_dev_bpe, tgt_dev_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"test.bpe.\"+source_language, \"w\") as src_file, open(\"test.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_test_bpe, tgt_test_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "hxxBOCA-xXhy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2514c4e-ec70-44a0-8bdc-20bd851d8c45"
      },
      "source": [
        "# Doublecheck the files. There should be no extra quotation marks or weird characters.\n",
        "! head -n5 train.*\n",
        "! head -n5 dev.*\n",
        "! head -n5 test.*"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.bpe.en <==\n",
            "▁ T h ank ▁you ▁so ▁much ▁, ▁ C h ris ▁.\n",
            "▁ A nd ▁it ▁' s ▁tr u ly ▁a ▁great ▁honor ▁to ▁have ▁the ▁opportun ity ▁to ▁come ▁to ▁this ▁stage ▁twice ▁; ▁ I ▁' m ▁extrem ely ▁gr ate ful ▁.\n",
            "▁ I ▁have ▁been ▁bl own ▁away ▁by ▁this ▁conference ▁, ▁and ▁ I ▁want ▁to ▁than k ▁all ▁of ▁you ▁for ▁the ▁many ▁n ice ▁com ments ▁about ▁what ▁ I ▁had ▁to ▁say ▁the ▁other ▁night ▁.\n",
            "▁ A nd ▁ I ▁say ▁that ▁s inc er ely ▁, ▁part ly ▁because ▁( ▁ M ock ▁so b ▁ ) ▁ I ▁need ▁that ▁.\n",
            "▁( ▁ L augh ter ▁ ) ▁ P ut ▁y ours elves ▁in ▁my ▁position ▁.\n",
            "\n",
            "==> train.bpe.tr <==\n",
            "▁ Ç ok ▁teş ek kür ▁eder im ▁ C h ris ▁.\n",
            "▁ B u ▁sahne de ▁ikinci ▁kez ▁yer ▁al ma ▁fır sat ına ▁sahip ▁olmak ▁ger ç ekten ▁büyük ▁bir ▁onur ▁. ▁ Ç ok ▁min net tar ım ▁.\n",
            "▁ B u ▁konfer ans ta ▁çok ▁mut lu ▁ol d um ▁, ▁ve ▁anlat t ıkları m la ▁ilgili ▁güzel ▁yorum ların ız ▁için ▁s iz lere ▁çok ▁teş ek kür ▁eder im ▁.\n",
            "▁ B unu ▁iç ten likle ▁söy l üyor um ▁, ▁çünkü ▁... ▁( ▁ A ğ lama ▁tak li di ▁) ▁ B una ▁ihtiy ac ım ▁var ▁.\n",
            "▁( ▁ K ah k ah alar ▁) ▁ K end in izi ▁benim ▁yer ime ▁koy un ▁ !\n",
            "\n",
            "==> train.en <==\n",
            "Thank you so much , Chris .\n",
            "And it 's truly a great honor to have the opportunity to come to this stage twice ; I 'm extremely grateful .\n",
            "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "And I say that sincerely , partly because ( Mock sob ) I need that .\n",
            "( Laughter ) Put yourselves in my position .\n",
            "\n",
            "==> train.tr <==\n",
            "Çok teşekkür ederim Chris .\n",
            "Bu sahnede ikinci kez yer alma fırsatına sahip olmak gerçekten büyük bir onur . Çok minnettarım .\n",
            "Bu konferansta çok mutlu oldum , ve anlattıklarımla ilgili güzel yorumlarınız için sizlere çok teşekkür ederim .\n",
            "Bunu içtenlikle söylüyorum , çünkü ... ( Ağlama taklidi ) Buna ihtiyacım var .\n",
            "( Kahkahalar ) Kendinizi benim yerime koyun !\n",
            "==> dev.bpe.en <==\n",
            "▁ T he y ▁decided ▁to ▁take ▁a ▁block - by - bl ock - by - bl ock ▁str ate gy ▁.\n",
            "▁ S o ▁within ▁the ▁neighbor hood ▁of ▁ B right m oor ▁, ▁you ▁' ll ▁find ▁a ▁ 2 1 - bl ock ▁mic r one igh bor hood ▁called ▁ B right m oor ▁ F arm way ▁.\n",
            "▁ N ow ▁, ▁what ▁was ▁a ▁not or ious ▁, ▁uns af e ▁, ▁und ers erved ▁community ▁has ▁transform ed ▁into ▁a ▁wel com ing ▁, ▁beaut if ul ▁, ▁sa fe ▁farm way ▁, ▁l ush ▁with ▁par ks ▁and ▁gard ens ▁and ▁far ms ▁and ▁green h ous es ▁.\n",
            "▁ T h is ▁t ight - kn it ▁community ▁also ▁came ▁together ▁recently ▁, ▁and ▁they ▁purchased ▁an ▁aband oned ▁building ▁, ▁an ▁aband oned ▁building ▁that ▁was ▁in ▁dis rep air ▁and ▁in ▁fore cl os ure ▁.\n",
            "▁ A nd ▁with ▁the ▁help ▁of ▁friends ▁and ▁families ▁and ▁vol un te ers ▁, ▁they ▁were ▁able ▁to ▁take ▁down ▁the ▁bul let pro of ▁glass ▁, ▁they ▁were ▁able ▁to ▁cle an ▁up ▁the ▁ground s ▁and ▁they ▁transform ed ▁that ▁building ▁into ▁a ▁community ▁kit chen ▁, ▁into ▁a ▁c af e ▁, ▁into ▁a ▁store f ront ▁.\n",
            "\n",
            "==> dev.bpe.tr <==\n",
            "▁ O n lar ▁bl ok - b l ok ▁iş leme ▁str ate j isinde ▁karar ▁k ıldı lar ▁.\n",
            "▁ D ol ay ısıyla ▁ B right mo or ▁ M ah all esi ▁' nde ▁ 2 1 ▁bl ok tan ▁oluşan ▁ B right mo or ▁ F ar m way ▁adında ▁mik ro ▁mahall eler ▁gör ür s ün üz ▁.\n",
            "▁ Ş im diler de ▁, ▁kötü ▁ş ö h ret li ▁, ▁güvenlik siz ▁, ▁yet ersiz ▁hizmet ▁almış ▁bu ▁topluluk ▁dav et kar ▁, ▁güzel ▁ve ▁güven li ▁bir ▁tar la ▁yol una ▁, ▁yem y eşil ▁park ları ▁, ▁bahç eleri ▁, ▁tar l aları ▁ve ▁ser aları ▁ile ▁birlikte ▁dön üştü ▁.\n",
            "▁ B u ▁birbirine ▁sık ı ca ▁bağlı ▁topluluk ▁kısa ▁süre ▁önce ▁bir ▁araya ▁geldi ▁ve ▁terk ▁edilmiş ▁bir ▁bin a ▁satın ▁aldı lar ▁, ▁yık ık ▁dök ük ▁ve ▁ip ot ekli ▁terk ▁edilmiş ▁bir ▁bin a ▁.\n",
            "▁ V e ▁dost ların ▁, ▁ail elerin ▁ve ▁gön ül lü lerin ▁yardım larıyla ▁beraber ▁, ▁kur ş un ▁geçir mez ▁cam ı ▁yık abil diler ▁, ▁yer leri ▁temiz ley eb il diler ▁ve ▁bur ayı ▁bir ▁toplum ▁mut f ağ ına ▁, ▁bir ▁kaf eye ▁, ▁v it r ine ▁çevir diler ▁.\n",
            "\n",
            "==> dev.en <==\n",
            "They decided to take a block-by-block-by-block strategy .\n",
            "So within the neighborhood of Brightmoor , you 'll find a 21-block microneighborhood called Brightmoor Farmway .\n",
            "Now , what was a notorious , unsafe , underserved community has transformed into a welcoming , beautiful , safe farmway , lush with parks and gardens and farms and greenhouses .\n",
            "This tight-knit community also came together recently , and they purchased an abandoned building , an abandoned building that was in disrepair and in foreclosure .\n",
            "And with the help of friends and families and volunteers , they were able to take down the bulletproof glass , they were able to clean up the grounds and they transformed that building into a community kitchen , into a cafe , into a storefront .\n",
            "\n",
            "==> dev.tr <==\n",
            "Onlar blok-blok işleme stratejisinde karar kıldılar .\n",
            "Dolayısıyla Brightmoor Mahallesi 'nde 21 bloktan oluşan Brightmoor Farmway adında mikro mahalleler görürsünüz .\n",
            "Şimdilerde , kötü şöhretli , güvenliksiz , yetersiz hizmet almış bu topluluk davetkar , güzel ve güvenli bir tarla yoluna , yemyeşil parkları , bahçeleri , tarlaları ve seraları ile birlikte dönüştü .\n",
            "Bu birbirine sıkıca bağlı topluluk kısa süre önce bir araya geldi ve terk edilmiş bir bina satın aldılar , yıkık dökük ve ipotekli terk edilmiş bir bina .\n",
            "Ve dostların , ailelerin ve gönüllülerin yardımlarıyla beraber , kurşun geçirmez camı yıkabildiler , yerleri temizleyebildiler ve burayı bir toplum mutfağına , bir kafeye , vitrine çevirdiler .\n",
            "==> test.bpe.en <==\n",
            "▁ M y ▁own ▁jour ney ▁to ▁work ▁with ▁these ▁children ▁started ▁as ▁a ▁te en ag er ▁.\n",
            "▁ I ▁was ▁ 15 ▁when ▁ I ▁was ▁g ang - ra ped ▁by ▁eight ▁men ▁.\n",
            "▁ I ▁don ▁' t ▁rem ember ▁the ▁ra pe ▁part ▁of ▁it ▁so ▁much ▁as ▁much ▁as ▁the ▁an ger ▁part ▁of ▁it ▁.\n",
            "▁ Y es ▁, ▁there ▁were ▁eight ▁men ▁who ▁def iled ▁me ▁, ▁ra ped ▁me ▁, ▁but ▁that ▁did n ▁' t ▁go ▁into ▁my ▁cons c ious ness ▁.\n",
            "▁ I ▁never ▁felt ▁like ▁a ▁vict im ▁, ▁then ▁or ▁now ▁.\n",
            "\n",
            "==> test.bpe.tr <==\n",
            "▁ B u ▁çocuk larla ▁çalışma ▁hik ay em ▁ben ▁genç ▁kız ken ▁başladı ▁.\n",
            "▁ 8 ▁erkek ▁tarafından ▁top lu ▁tec av üz e ▁uğr adı ğ ım da ▁ 15 ▁yaşında y d ım ▁.\n",
            "▁ T ec av üz ▁kısm ını ▁, ▁ö f ke ▁kısm ını ▁kadar ▁net ▁an ım say am ıyor um ▁.\n",
            "▁ E vet ▁, ▁ 8 ▁erkek ▁b eni ▁kir let ti ▁, ▁ır z ım a ▁geçti ▁, ▁ama ▁bu ▁bilin c ime ▁işlem edi ▁.\n",
            "▁ H iç ▁bir ▁zaman ▁, ▁ne ▁o ▁zaman ▁n ede ▁şim di ▁bir ▁kur ban ▁gibi ▁hiss et med im ▁.\n",
            "\n",
            "==> test.en <==\n",
            "My own journey to work with these children started as a teenager .\n",
            "I was 15 when I was gang-raped by eight men .\n",
            "I don 't remember the rape part of it so much as much as the anger part of it .\n",
            "Yes , there were eight men who defiled me , raped me , but that didn 't go into my consciousness .\n",
            "I never felt like a victim , then or now .\n",
            "\n",
            "==> test.tr <==\n",
            "Bu çocuklarla çalışma hikayem ben genç kızken başladı .\n",
            "8 erkek tarafından toplu tecavüze uğradığımda 15 yaşındaydım .\n",
            "Tecavüz kısmını , öfke kısmını kadar net anımsayamıyorum .\n",
            "Evet , 8 erkek beni kirletti , ırzıma geçti , ama bu bilincime işlemedi .\n",
            "Hiç bir zaman , ne o zaman nede şimdi bir kurban gibi hissetmedim .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iBRMm4kMxZ8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66d678d0-5e88-4ba2-a1df-92d6d9a38180"
      },
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install .\n",
        "# Install Pytorch with GPU support v1.7.1.\n",
        "! pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 3127, done.\u001b[K\n",
            "remote: Counting objects: 100% (176/176), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 3127 (delta 101), reused 142 (delta 91), pack-reused 2951\u001b[K\n",
            "Receiving objects: 100% (3127/3127), 8.09 MiB | 9.19 MiB/s, done.\n",
            "Resolving deltas: 100% (2130/2130), done.\n",
            "Processing /content/joeynmt\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
            "Collecting numpy==1.20.1\n",
            "  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 95 kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.2.0)\n",
            "Collecting torch==1.8.0\n",
            "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.5 MB 14 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
            "Collecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 2.4 MB/s \n",
            "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
            "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 32.3 MB/s \n",
            "\u001b[?25hCollecting pylint\n",
            "  Downloading pylint-2.9.5-py3-none-any.whl (375 kB)\n",
            "\u001b[K     |████████████████████████████████| 375 kB 23.8 MB/s \n",
            "\u001b[?25hCollecting six==1.12\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting wrapt==1.11.1\n",
            "  Downloading wrapt-1.11.1.tar.gz (27 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
            "Collecting mccabe<0.7,>=0.6\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Collecting astroid<2.7,>=2.6.5\n",
            "  Downloading astroid-2.6.5-py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 43.1 MB/s \n",
            "\u001b[?25hCollecting isort<6,>=4.2.5\n",
            "  Downloading isort-5.9.2-py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 46.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
            "Collecting lazy-object-proxy>=1.4.0\n",
            "  Downloading lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0\n",
            "  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 44.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
            "Building wheels for collected packages: joeynmt, wrapt\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=85116 sha256=ff8f08a906da86047fb37d05a4da136fe01c14416bc3f6d876803512e3cf84b7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-m6vmha7t/wheels/0a/f4/bf/6c9d3b8efbfece6cd209f865be37382b02e7c3584df2e28ca4\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68435 sha256=d280aefc86dd4555ec626354de4f48f91d46c3d8ae05a41a96758db02a4f1542\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/58/9d/da8bad4545585ca52311498ff677647c95c7b690b3040171f8\n",
            "Successfully built joeynmt wrapt\n",
            "Installing collected packages: six, wrapt, typed-ast, numpy, lazy-object-proxy, torch, portalocker, mccabe, isort, astroid, torchtext, subword-nmt, sacrebleu, pyyaml, pylint, joeynmt\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0 which is incompatible.\n",
            "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.20.1 which is incompatible.\n",
            "tensorflow 2.5.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "tensorflow 2.5.0 requires wrapt~=1.12.1, but you have wrapt 1.11.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-python-client 1.12.8 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed astroid-2.6.5 isort-5.9.2 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.9.5 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torch-1.8.0 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.8.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 763.5 MB 16 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.7.4.3)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.8.0\n",
            "    Uninstalling torch-1.8.0:\n",
            "      Successfully uninstalled torch-1.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "H-TyjtmXB1mL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "522d4b02-98bc-4ecf-ab50-0ab33a75b656"
      },
      "source": [
        "## TO DELETE?\n",
        "\n",
        "os.environ[\"data_path\"] = os.path.join(\"joeynmt\", \"data\", source_language + target_language)\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev.bpe.en  dev.en  test.bpe.en  test.en  train.bpe.en\ttrain.en\n",
            "dev.bpe.tr  dev.tr  test.bpe.tr  test.tr  train.bpe.tr\ttrain.tr\n",
            "Combined BPE Vocab\n",
            "▁spanish\n",
            "SMW\n",
            "KBI\n",
            "IS\n",
            "▁anton\n",
            "1784\n",
            "ò\n",
            "عسل\n",
            "مسكين\n",
            "T8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "IlMitUHR8Qy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff05d5f5-8669-48eb-a83d-4655b0ea33aa"
      },
      "source": [
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\"  #See the contents of the drive directory"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev.bpe.en  dev.en  test.bpe.en  test.en  train.bpe.en\ttrain.en\n",
            "dev.bpe.tr  dev.tr  test.bpe.tr  test.tr  train.bpe.tr\ttrain.tr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PIs1lY2hxMsl"
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: False               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "6ZBPFwT94WpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48a80833-0648-4577-9a0e-e1251ea6df69"
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 12:41:07,414 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-28 12:41:07,439 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-28 12:41:15,383 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-28 12:41:17,439 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-28 12:41:17,959 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-28 12:41:19,884 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-28 12:41:19,884 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-28 12:41:20,186 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-28 12:41:20.439783: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-28 12:41:22,372 - INFO - joeynmt.training - Total params: 13813760\n",
            "2021-07-28 12:41:24,627 - INFO - joeynmt.helpers - cfg.name                           : entr_transformer\n",
            "2021-07-28 12:41:24,627 - INFO - joeynmt.helpers - cfg.data.src                       : en\n",
            "2021-07-28 12:41:24,627 - INFO - joeynmt.helpers - cfg.data.trg                       : tr\n",
            "2021-07-28 12:41:24,627 - INFO - joeynmt.helpers - cfg.data.train                     : data/entr/train.bpe\n",
            "2021-07-28 12:41:24,627 - INFO - joeynmt.helpers - cfg.data.dev                       : data/entr/dev.bpe\n",
            "2021-07-28 12:41:24,627 - INFO - joeynmt.helpers - cfg.data.test                      : data/entr/test.bpe\n",
            "2021-07-28 12:41:24,627 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-28 12:41:24,627 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-28 12:41:24,628 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-28 12:41:24,628 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : data/entr/vocab.txt\n",
            "2021-07-28 12:41:24,628 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : data/entr/vocab.txt\n",
            "2021-07-28 12:41:24,628 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-28 12:41:24,628 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-28 12:41:24,628 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-28 12:41:24,628 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-28 12:41:24,628 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-28 12:41:24,628 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-28 12:41:24,629 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-28 12:41:24,629 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-28 12:41:24,629 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-28 12:41:24,629 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-28 12:41:24,629 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-28 12:41:24,629 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-28 12:41:24,629 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-28 12:41:24,629 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-28 12:41:24,629 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-28 12:41:24,630 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-28 12:41:24,630 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-28 12:41:24,630 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-28 12:41:24,630 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
            "2021-07-28 12:41:24,630 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-28 12:41:24,630 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-28 12:41:24,630 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-28 12:41:24,630 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
            "2021-07-28 12:41:24,631 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1000\n",
            "2021-07-28 12:41:24,631 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
            "2021-07-28 12:41:24,631 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-28 12:41:24,631 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/entr_transformer\n",
            "2021-07-28 12:41:24,631 - INFO - joeynmt.helpers - cfg.training.overwrite             : False\n",
            "2021-07-28 12:41:24,631 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-28 12:41:24,631 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-28 12:41:24,631 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-28 12:41:24,631 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-28 12:41:24,632 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-28 12:41:24,632 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-28 12:41:24,632 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-28 12:41:24,632 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-28 12:41:24,632 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-28 12:41:24,632 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-28 12:41:24,632 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-28 12:41:24,632 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-28 12:41:24,632 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-28 12:41:24,632 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-28 12:41:24,633 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-28 12:41:24,633 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-28 12:41:24,633 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-28 12:41:24,633 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-28 12:41:24,633 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-28 12:41:24,633 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-28 12:41:24,633 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-28 12:41:24,633 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-28 12:41:24,634 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-28 12:41:24,634 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-28 12:41:24,634 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-28 12:41:24,634 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-28 12:41:24,634 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-28 12:41:24,634 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-28 12:41:24,634 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-28 12:41:24,634 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-28 12:41:24,635 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 296768,\n",
            "\tvalid 37437,\n",
            "\ttest 37439\n",
            "2021-07-28 12:41:24,635 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] ▁ T h ank ▁you ▁so ▁much ▁, ▁ C h ris ▁.\n",
            "\t[TRG] ▁ Ç ok ▁teş ek kür ▁eder im ▁ C h ris ▁.\n",
            "2021-07-28 12:41:24,635 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ▁ (5) ▁. (6) ▁, (7) ▁the (8) ▁' (9) ▁to\n",
            "2021-07-28 12:41:24,635 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ▁ (5) ▁. (6) ▁, (7) ▁the (8) ▁' (9) ▁to\n",
            "2021-07-28 12:41:24,635 - INFO - joeynmt.helpers - Number of Src words (types): 10756\n",
            "2021-07-28 12:41:24,636 - INFO - joeynmt.helpers - Number of Trg words (types): 10756\n",
            "2021-07-28 12:41:24,636 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=10756),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=10756))\n",
            "2021-07-28 12:41:24,651 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-28 12:41:24,651 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-28 12:41:59,555 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     5.799444, Tokens per Sec:     7181, Lr: 0.000300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "MBoDS09JM807"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "n94wlrCjVc17"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "66WhRE9lIhoD"
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}