{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "starter_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Machine Translation for Translators Workshop\n",
        "Localization summer school '21"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4fXCKCf36IK"
      },
      "source": [
        "### Note before beginning:\n",
        "\n",
        "#### - This coding template is based on Masakhane's starter notebook (https://github.com/masakhane-io/masakhane-mt)\n",
        "#### - The idea is that you should be able to make minimal changes to this in order to get SOME result for your own translation corpus. \n",
        "#### - The TL;DR: Go to the **\"TODO\"** comments which will tell you what to update to get up and running\n",
        "#### - If you actually want to have a clue what you're doing, read the text and peek at the links\n",
        "#### - With 100 epochs, it should take around 7 hours to run in Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l929HimrxS0a"
      },
      "source": [
        "## Retrieve your data & make a parallel corpus\n",
        "\n",
        "In this workshop we will use open corpus available from OPUS repository to train a translation model. We will first download the data, create training, development, testing sets from it and then use JoeyNMT to train a baseline model. \n",
        "\n",
        "In the next cell, you need to set the languages you want to work with and specify which corpus you want to use to train. \n",
        "\n",
        "To select a corpus go to https://opus.nlpl.eu/, enter your language pair and select one that you think is more appropriate (size, domain)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Cn3tgQLzUxwn"
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"tr\"\n",
        "opus_corpus = \"TED2020\" \n",
        "lc = False  # If True, lowercase the data.\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"corpus\"] = opus_corpus\n",
        "os.environ[\"tag\"] = tag"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO5IY_RywQyA"
      },
      "source": [
        "# This will save it to a folder in our gdrive instead!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s-%s\" % (source_language, target_language, tag)\n",
        "\n",
        "!echo $gdrive_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gA75Fs9ys8Y9",
        "outputId": "859efe7d-aa33-45c7-e863-2b65858e5130",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install opus-tools (Warning! This is not really python)\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opustools-pkg in /usr/local/lib/python3.7/dist-packages (0.0.52)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xq-tDZVks7ZD",
        "outputId": "1990a60b-ab2c-4600-8eac-fcf90c19027b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Downloading our corpus \n",
        "! opus_read -d $corpus -s $src -t $tgt -wm moses -w $corpus.$src $corpus.$tgt -q\n",
        "\n",
        "# Extract the corpus file\n",
        "! gunzip ${corpus}_latest_xml_$src-$tgt.xml.gz"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/TED2020/latest/xml/en-tr.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "        ./TED2020_latest_xml_en.zip already exists\n",
            "        ./TED2020_latest_xml_tr.zip already exists\n",
            "   2 MB https://object.pouta.csc.fi/OPUS-TED2020/v1/xml/en-tr.xml.gz\n",
            "\n",
            "   2 MB Total size\n",
            "./TED2020_latest_xml_en-tr.xml.gz ... 100% of 2 MB\n",
            "gzip: TED2020_latest_xml_en-tr.xml already exists; do you wish to overwrite (y or n)? n\n",
            "\tnot overwritten\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sazl7hv9xZFg"
      },
      "source": [
        "# Read the corpus into python lists\n",
        "source_file = opus_corpus + '.' + source_language\n",
        "target_file = opus_corpus + '.' + target_language\n",
        "\n",
        "src_all = [sentence.strip() for sentence in open(source_file).readlines()]\n",
        "tgt_all = [sentence.strip() for sentence in open(target_file).readlines()]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjcOBlYMxxTC",
        "outputId": "4035cd38-9987-45ad-9d55-0201151d4a6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Let's take a peek at the files\n",
        "print(\"Source size:\", len(src_all))\n",
        "print(\"Target size:\", len(tgt_all))\n",
        "print(\"--------\")\n",
        "peek_size = 5\n",
        "for i in range(peek_size):\n",
        "  print(\"Sent #\", i)\n",
        "  print(\"SRC:\", src_all[i])\n",
        "  print(\"TGT:\", tgt_all[i])\n",
        "  print(\"---------\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source size: 374378\n",
            "Target size: 374378\n",
            "--------\n",
            "Sent # 0\n",
            "SRC: Thank you so much , Chris .\n",
            "TGT: Çok teşekkür ederim Chris .\n",
            "---------\n",
            "Sent # 1\n",
            "SRC: And it 's truly a great honor to have the opportunity to come to this stage twice ; I 'm extremely grateful .\n",
            "TGT: Bu sahnede ikinci kez yer alma fırsatına sahip olmak gerçekten büyük bir onur . Çok minnettarım .\n",
            "---------\n",
            "Sent # 2\n",
            "SRC: I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "TGT: Bu konferansta çok mutlu oldum , ve anlattıklarımla ilgili güzel yorumlarınız için sizlere çok teşekkür ederim .\n",
            "---------\n",
            "Sent # 3\n",
            "SRC: And I say that sincerely , partly because ( Mock sob ) I need that .\n",
            "TGT: Bunu içtenlikle söylüyorum , çünkü ... ( Ağlama taklidi ) Buna ihtiyacım var .\n",
            "---------\n",
            "Sent # 4\n",
            "SRC: ( Laughter ) Put yourselves in my position .\n",
            "TGT: ( Kahkahalar ) Kendinizi benim yerime koyun !\n",
            "---------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkuK3B4p2AkN"
      },
      "source": [
        "## Making training, development and testing sets\n",
        "\n",
        "We need to pick training, development and testing sets from our corpus. Training set will contain the sentences that we'll teach our model. Development set will be used to see how our model is progressing during the training. And finally, testing set will be used to evaluate the model.\n",
        "\n",
        "You can optionally load your own testing set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkcE9T75y9za",
        "outputId": "3766b83f-e373-40d8-b2ef-d1fd5fc9246c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO: Determine ratios of each set\n",
        "train_ratio = 0.8\n",
        "dev_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "\n",
        "all_size = len(src_all)\n",
        "train_size = int(all_size * train_ratio)\n",
        "dev_size = int(all_size * dev_ratio)\n",
        "test_size = all_size -train_size - dev_size\n",
        "\n",
        "print(\"All:\", all_size)\n",
        "print(\"Train:\", train_size)\n",
        "print(\"Dev:\", dev_size)\n",
        "print(\"Test:\", test_size)\n",
        "\n",
        "src_train = src_all[0:train_size]\n",
        "tgt_train = tgt_all[0:train_size]\n",
        "\n",
        "src_dev = src_all[train_size:train_size+dev_size]\n",
        "tgt_dev = tgt_all[train_size:train_size+dev_size]\n",
        "\n",
        "src_test = src_all[train_size+dev_size:all_size]\n",
        "tgt_test = tgt_all[train_size+dev_size:all_size]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All: 374378\n",
            "Train: 299502\n",
            "Dev: 37437\n",
            "Test: 37439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRQ7hNIS0D0z"
      },
      "source": [
        "# Now let's write all our sets into separate files\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as tgt_file:\n",
        "  for src, tgt in zip(src_train, tgt_train):\n",
        "    src_file.write(src+\"\\n\")\n",
        "    tgt_file.write(tgt+\"\\n\")\n",
        "\n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as tgt_file:\n",
        "  for src, tgt in zip(src_dev, tgt_dev):\n",
        "    src_file.write(src+\"\\n\")\n",
        "    tgt_file.write(tgt+\"\\n\")\n",
        "\n",
        "with open(\"test.\"+source_language, \"w\") as src_file, open(\"test.\"+target_language, \"w\") as tgt_file:\n",
        "  for src, tgt in zip(src_test, tgt_test):\n",
        "    src_file.write(src+\"\\n\")\n",
        "    tgt_file.write(tgt+\"\\n\")"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "hxxBOCA-xXhy",
        "outputId": "e808ecd8-0fa7-4900-d753-6ef71101ac9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Doublecheck the files. There should be no extra quotation marks or weird characters.\n",
        "! head train.*\n",
        "! head dev.*\n",
        "! head test.*"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.en <==\n",
            "Thank you so much , Chris .\n",
            "And it 's truly a great honor to have the opportunity to come to this stage twice ; I 'm extremely grateful .\n",
            "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "And I say that sincerely , partly because ( Mock sob ) I need that .\n",
            "( Laughter ) Put yourselves in my position .\n",
            "( Laughter ) I flew on Air Force Two for eight years .\n",
            "( Laughter ) Now I have to take off my shoes or boots to get on an airplane !\n",
            "( Laughter ) ( Applause ) I 'll tell you one quick story to illustrate what that 's been like for me .\n",
            "( Laughter ) It 's a true story -- every bit of this is true .\n",
            "Soon after Tipper and I left the -- ( Mock sob ) White House -- ( Laughter ) we were driving from our home in Nashville to a little farm we have 50 miles east of Nashville .\n",
            "\n",
            "==> train.tr <==\n",
            "Çok teşekkür ederim Chris .\n",
            "Bu sahnede ikinci kez yer alma fırsatına sahip olmak gerçekten büyük bir onur . Çok minnettarım .\n",
            "Bu konferansta çok mutlu oldum , ve anlattıklarımla ilgili güzel yorumlarınız için sizlere çok teşekkür ederim .\n",
            "Bunu içtenlikle söylüyorum , çünkü ... ( Ağlama taklidi ) Buna ihtiyacım var .\n",
            "( Kahkahalar ) Kendinizi benim yerime koyun !\n",
            "Sekiz yıl boyunca Air Force İki ile uçtum .\n",
            "Şimdi ise bir uçağa binerken ayakkabılarımı çıkarmak zorunda kalıyorum !\n",
            "( Kahkahalar ) ( Alkış ) Bunun ne demek olduğunu anlayabilmeniz için kısa bir hikaye anlatacağım .\n",
            "Bu gerçek bir hikaye --- tamamiyle gerçek .\n",
            "Tipper ve ben Beyaz Saray 'dan -- ( Ağlama taklidi ) -- ayrıldıktan bir süre sonra -- ( Kahkahalar ) -- Nashville 'deki evimizden Nasville 'nin 50 mil ( 80 Kilometre ) uzağındaki çiftliğimize\n",
            "==> dev.en <==\n",
            "They decided to take a block-by-block-by-block strategy .\n",
            "So within the neighborhood of Brightmoor , you 'll find a 21-block microneighborhood called Brightmoor Farmway .\n",
            "Now , what was a notorious , unsafe , underserved community has transformed into a welcoming , beautiful , safe farmway , lush with parks and gardens and farms and greenhouses .\n",
            "This tight-knit community also came together recently , and they purchased an abandoned building , an abandoned building that was in disrepair and in foreclosure .\n",
            "And with the help of friends and families and volunteers , they were able to take down the bulletproof glass , they were able to clean up the grounds and they transformed that building into a community kitchen , into a cafe , into a storefront .\n",
            "Now the farmers and the food artisans who live in Brightmoor , they have a place where they can make and sell their product .\n",
            "And the people in the community have some place where they can buy healthy , fresh food .\n",
            "Urban agriculture -- and this is my third example -- can be used as a way to lift up the business cooperative model .\n",
            "The 1,500 farms and gardens I told you about earlier ?\n",
            "Keep Growing Detroit is a nonprofit organization that had a lot to do with those farms .\n",
            "\n",
            "==> dev.tr <==\n",
            "Onlar blok-blok işleme stratejisinde karar kıldılar .\n",
            "Dolayısıyla Brightmoor Mahallesi 'nde 21 bloktan oluşan Brightmoor Farmway adında mikro mahalleler görürsünüz .\n",
            "Şimdilerde , kötü şöhretli , güvenliksiz , yetersiz hizmet almış bu topluluk davetkar , güzel ve güvenli bir tarla yoluna , yemyeşil parkları , bahçeleri , tarlaları ve seraları ile birlikte dönüştü .\n",
            "Bu birbirine sıkıca bağlı topluluk kısa süre önce bir araya geldi ve terk edilmiş bir bina satın aldılar , yıkık dökük ve ipotekli terk edilmiş bir bina .\n",
            "Ve dostların , ailelerin ve gönüllülerin yardımlarıyla beraber , kurşun geçirmez camı yıkabildiler , yerleri temizleyebildiler ve burayı bir toplum mutfağına , bir kafeye , vitrine çevirdiler .\n",
            "Artık Brightmoor 'da yaşayan çiftçi ve esnafların , ürünlerini koyup satabilecekleri bir yerleri var .\n",
            "Ve burada yaşayan halkın artık sağlıklı ürün ve taze gıdalar alabileceği bir yer var .\n",
            "Kentsel tarım -- ve bu üçüncü örneğim -- işbirliği modelini geliştirmenin bir yolu olarak kullanılabilir .\n",
            "Az önce bahsettiğim 1.500 tarla ve bahçe ...\n",
            "Keep Growing Detroit , bu tarlarla çok bağlantılı kâr amacı gütmeyen bir organizasyon .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iBRMm4kMxZ8L"
      },
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install .\n",
        "# Install Pytorch with GPU support v1.7.1.\n",
        "! pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# Preprocessing the Data into Subword BPE Tokens\n",
        "\n",
        "- One of the most powerful improvements for agglutinative languages (a feature of most Bantu languages) is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- It was also shown that by optimizing the umber of BPE codes we significantly improve results for low-resourced languages [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021) [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
        "\n",
        "- Below we have the scripts for doing BPE tokenization of our data. We use 4000 tokens as recommended by [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021). You do not need to change anything. Simply running the below will be suitable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "H-TyjtmXB1mL"
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "from os import path\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "\n",
        "# Learn BPEs on the training data.\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "! cp bpe.codes.4000 $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\"\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE Xhosa Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "IlMitUHR8Qy-"
      },
      "source": [
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PIs1lY2hxMsl"
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: False               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "6ZBPFwT94WpI"
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "MBoDS09JM807"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "n94wlrCjVc17"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "66WhRE9lIhoD"
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}