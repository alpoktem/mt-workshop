{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Machine translation workshop",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Machine Translation for Translators Workshop\n",
        "\n",
        "In this notebook, we'll learn:\n",
        "\n",
        "- Running a pre-trained English-Turkish MT model \n",
        "- Byte-pair encoding (BPE)\n",
        "- Translating a document\n",
        "- Creating training data from translation memory (TMX)\n",
        "- Creating a test set and calculating BLEU\n",
        "- Training a model from scratch\n",
        "- Domain adaptation on our pre-trained model\n",
        "\n",
        "NOTE: This coding template is partially based on Masakhane's starter notebook (https://github.com/masakhane-io/masakhane-mt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Machine translation with JoeyNMT and AfroTranslate\n",
        "\n",
        "For this workshop we will use 🐨 JoeyNMT. \n",
        "\n",
        "JoeyNMT is an open-source, minimalist neural machine translation toolkit for educational purposes. [code](https://github.com/joeynmt/joeynmt), [documentation](https://joeynmt.readthedocs.io/en/latest/).\n",
        "\n",
        "We will also use a Python package called [AfroTranslate](https://github.com/hgilles06/AfroTranslate) to easily interact with JoeyNMT.\n",
        "\n",
        "Let's start by installing AfroTranslate. Since it's dependent on JoeyNMT, it'll automatically install it for us.\n",
        "\n"
      ],
      "metadata": {
        "id": "k-xY0ng0Qx-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install AfroTranslate"
      ],
      "metadata": {
        "id": "tqxIcN9FQj4q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73f9ccc3-3fac-4abc-8349-c7cda5ead7de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting AfroTranslate\n",
            "  Downloading AfroTranslate-0.0.6-py3-none-any.whl (12 kB)\n",
            "Collecting joeynmt==1.3\n",
            "  Downloading joeynmt-1.3-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting spacy==3.2.1\n",
            "  Downloading spacy-3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 19.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader==0.4 in /usr/local/lib/python3.7/dist-packages (from AfroTranslate) (0.4)\n",
            "Requirement already satisfied: wrapt==1.11.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (1.11.1)\n",
            "Collecting six==1.12\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (7.1.2)\n",
            "Collecting torch==1.8.0\n",
            "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.5 MB 14 kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (0.11.2)\n",
            "Collecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 23.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: subword-nmt in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (0.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (0.16.0)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (2.7.0)\n",
            "Requirement already satisfied: sacrebleu>=1.3.6 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (2.0.0)\n",
            "Collecting numpy==1.20.1\n",
            "  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 192 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pylint in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (2.12.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (6.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (57.4.0)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 33.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (21.3)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 45.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (1.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (2.23.0)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (3.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (3.10.0.2)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (0.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (2.11.3)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 50.9 MB/s \n",
            "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (4.62.3)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (2.0.6)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy==3.2.1->AfroTranslate) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.2.1->AfroTranslate) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy==3.2.1->AfroTranslate) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1->AfroTranslate) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1->AfroTranslate) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1->AfroTranslate) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1->AfroTranslate) (2.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt==1.3->AfroTranslate) (0.4.4)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt==1.3->AfroTranslate) (0.8.9)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt==1.3->AfroTranslate) (2.3.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt==1.3->AfroTranslate) (2019.12.20)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.43.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (4.10.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (3.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy==3.2.1->AfroTranslate) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.2.1->AfroTranslate) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3->AfroTranslate) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3->AfroTranslate) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3->AfroTranslate) (0.11.0)\n",
            "Requirement already satisfied: mccabe<0.7,>=0.6 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3->AfroTranslate) (0.6.1)\n",
            "Requirement already satisfied: isort<6,>=4.2.5 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3->AfroTranslate) (5.10.1)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3->AfroTranslate) (2.4.1)\n",
            "Requirement already satisfied: toml>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3->AfroTranslate) (0.10.2)\n",
            "Requirement already satisfied: astroid<2.10,>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3->AfroTranslate) (2.9.3)\n",
            "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.10,>=2.9.0->pylint->joeynmt==1.3->AfroTranslate) (1.7.1)\n",
            "Requirement already satisfied: typed-ast<2.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.10,>=2.9.0->pylint->joeynmt==1.3->AfroTranslate) (1.5.2)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3->AfroTranslate) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3->AfroTranslate) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3->AfroTranslate) (2018.9)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.7/dist-packages (from subword-nmt->joeynmt==1.3->AfroTranslate) (4.0.3)\n",
            "Installing collected packages: six, numpy, catalogue, typer, torch, srsly, pydantic, torchtext, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy, joeynmt, AfroTranslate\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Attempting uninstall: joeynmt\n",
            "    Found existing installation: joeynmt 1.5.1\n",
            "    Uninstalling joeynmt-1.5.1:\n",
            "      Successfully uninstalled joeynmt-1.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.20.1 which is incompatible.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.8.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-python-client 1.12.10 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed AfroTranslate-0.0.6 catalogue-2.0.6 joeynmt-1.3 langcodes-3.3.0 numpy-1.20.1 pathy-0.6.1 pydantic-1.8.2 six-1.12.0 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 torch-1.8.0 torchtext-0.9.0 typer-0.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AfroTranslate comes with direct links to many African languages ([list](https://github.com/masakhane-io/masakhane-mt/tree/master/benchmarks)). \n",
        "\n",
        "Let's use it to load its Tigrinya model and use it."
      ],
      "metadata": {
        "id": "_L0NWncARvl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import translator\n",
        "from afrotranslate import MasakhaneTranslate"
      ],
      "metadata": {
        "id": "r83MXvgZ4prs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create translator object\n",
        "translator = MasakhaneTranslate(model_name=\"en-ti\")\n",
        "\n",
        "#translate \n",
        "translator.translate(\"I love you so much!\")"
      ],
      "metadata": {
        "id": "TE2EArLXSWhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also load a model of our own.\n",
        "\n",
        "Let's first download the model stored in the cloud.\n",
        "\n",
        "[Direct link](https://drive.google.com/file/d/1E_bdkdnYW4wTSdujsDiCqBlBYngvjX0m/view?usp=sharing)"
      ],
      "metadata": {
        "id": "G4Typ_EGYGaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute this to download directly to Colab\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id=\"1E_bdkdnYW4wTSdujsDiCqBlBYngvjX0m\",\n",
        "    \t\t                            dest_path=\"models/entr/entr.zip\",\n",
        "    \t\t                            unzip=True)"
      ],
      "metadata": {
        "id": "y_XH3nSmYM1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's load our English-Turkish translator model using AfroTranslate"
      ],
      "metadata": {
        "id": "DfTj0kRsVWvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cTzqhVbKVgRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2cd8TXgSVwS3",
        "outputId": "455178e5-234f-4170-9d45-7f2e1f7550d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'▁ ut an ç ▁.'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AThMQfxCaRg2",
        "outputId": "2d3e74e3-610c-4768-b913-3a7be0087c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'▁ ut an ç ▁.'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results seem to be a bit strange. It's because the models were trained with BPE encoded words. "
      ],
      "metadata": {
        "id": "bjn4LYzZcXtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Byte-pair encoding (BPE)\n",
        "\n",
        "- One of the most powerful improvements for neural machine translation is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- BPE tokenization limits the number of vocabulary into a certain size by smartly dividing words into subwords\n",
        "\n",
        "- This is especially useful for agglutinative languages (like Turkish) where vocabulary is effectively endless. \n",
        "\n",
        "- Below you have the scripts for doing BPE tokenization of our data. We use bpemb library that has pre-trained BPE models to convert our text into subwords.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vMdOfnCOaUgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install bpemb"
      ],
      "metadata": {
        "id": "3GFu_jc5b34b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bpemb import BPEmb\n",
        "\n",
        "BPE_VOCAB_SIZE = 5000\n",
        "bpemb_en = BPEmb(lang=\"en\", vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=True)\n",
        "bpemb_tr = BPEmb(lang=\"tr\", vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=True)"
      ],
      "metadata": {
        "id": "r0ZErPmDbuVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use it now and see how it looks like"
      ],
      "metadata": {
        "id": "Ne7uYQ0_cIuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PhBPgoEwcIgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "slwFSJ85cOTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's go back using our model with properly encoded strings.\n",
        "\n",
        "Don't forget that `bpemb.encode` outputs a list but we need to input a string to our translator.\n",
        "\n",
        "You can use `' '.join(list)` to convert a list to string."
      ],
      "metadata": {
        "id": "Gpvp4BmhcVTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QC59yVXFc4nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w9d05981_2SH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zkClamJ9k9Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And to make that output readable, we need to decode it using the right BPE model"
      ],
      "metadata": {
        "id": "BTAGGdVrdt6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hQ0HmSuQdyrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a translator function that'll reduce our work..."
      ],
      "metadata": {
        "id": "rwMtdgVCm7Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_entr(tr_in):\n",
        "  #..."
      ],
      "metadata": {
        "id": "msoiIxUYkagg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_entr(\"Hello! What's up?\")"
      ],
      "metadata": {
        "id": "l3x1V1gdlyR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 (Challenge) - Translating a document\n",
        "\n",
        "For this exercise, we're going to create a script that translates a word document automatically. \n",
        "\n",
        "We're going to reuse some of the code from last session."
      ],
      "metadata": {
        "id": "utP_op_9ekwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "fpWFnsoZf0S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download our document to translate.\n",
        "\n",
        "[Direct link](https://docs.google.com/document/d/1jwNdh4n_m4M-0Z4j23lH4xyeWfTyHPY-/edit?usp=sharing&ouid=114670265863192986077&rtpof=true&sd=true)"
      ],
      "metadata": {
        "id": "Ho5hJMT5gjHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute this to download directly to Colab\n",
        "gdd.download_file_from_google_drive(file_id=\"1jwNdh4n_m4M-0Z4j23lH4xyeWfTyHPY-\", dest_path=\"/content/MT.docx\")"
      ],
      "metadata": {
        "id": "I6_TtPMWgFlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import docx and use it to read our document"
      ],
      "metadata": {
        "id": "h50CETebfHGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3Ui1asSqgFc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bj_F7IIMhLov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what's inside"
      ],
      "metadata": {
        "id": "MtbXSjQifLoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1PaC1Qj9hNU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's translate paragraphs one by one and put the translations into another list\n",
        "\n"
      ],
      "metadata": {
        "id": "13ZqOe07iw22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zipTAjg9ivyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "E2YWXgytjfsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, write the translated paragraphs into a newly created document."
      ],
      "metadata": {
        "id": "AREK4gqmfXzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wCfQVVv2niZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4 - Creating parallel data from TMX\n",
        "\n",
        "Sometimes, we would like to adapt a model into our style or a domain that we translate in. If we have done plenty of translations already, we could use our translation memory to later enhance a model. For that we need to convert our translation memory to a classic parallel data format. \n",
        "\n",
        "The classic parallel data format is two files, each of them containing the sentences at each line in different languages. \n",
        "\n",
        "A translation memory contains already parallel data in this sense. Although, it is not in the format that we want. \n",
        "\n",
        "We can automate this conversion using Python scripting. \n",
        "\n",
        "Since TMX parsing is a bit complicated for our level, we're going to use this code by [Yasmin Moslem](https://github.com/ymoslem/file-converters/blob/main/TMX2MT/TMX2MT-ElementTree.py)."
      ],
      "metadata": {
        "id": "eFh05S37o_Cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml\n",
        "import xml.etree.ElementTree as ET\n",
        "import sys\n",
        "import re\n",
        "import os\n",
        "\n",
        "def xml_to_parallel(file, source, target):\n",
        "  source_file = os.path.splitext(file)[0] + \".\" + source\n",
        "  target_file = os.path.splitext(file)[0] + \".\" + target\n",
        "\n",
        "  tree = ET.parse(file)  \n",
        "  root = tree.getroot()\n",
        "\n",
        "  langs = []\n",
        "\n",
        "  for tu in root.iter('tu'):\n",
        "      for tuv in tu.iter('tuv'):\n",
        "          lang = list(tuv.attrib.values())\n",
        "          langs.append(lang[0].lower())\n",
        "\n",
        "  langs = set(langs)\n",
        "\n",
        "  if source in langs and target in langs:\n",
        "      with open(source_file, \"w+\", encoding='utf-8') as source_file, open(target_file, \"w+\", encoding='utf-8') as target_file:\n",
        "          for tu in root.iter('tu'):\n",
        "              for tuv in tu.iter('tuv'):\n",
        "                  lang = list(tuv.attrib.values())\n",
        "                  #print(lang[0])\n",
        "                  if lang[0].lower() == source.lower():\n",
        "                      for seg in tuv.iter('seg'):\n",
        "                          source_text = ET.tostring(seg, 'utf-8', method=\"xml\")\n",
        "                          source_text = source_text.decode(\"utf-8\")\n",
        "                          source_text = re.sub('<.*?>|&lt;.*?&gt;|&?(amp|nbsp|quot);|{}', ' ', source_text)\n",
        "                          source_text = re.sub(r'[ ]{2,}', ' ', source_text).strip()\n",
        "                          source_file.write(str(source_text) + \"\\n\")\n",
        "                          #print(source_text)\n",
        "                  elif lang[0].lower() == target.lower():\n",
        "                      for seg in tuv.iter('seg'):\n",
        "                          target_text = ET.tostring(seg, 'utf-8', method=\"xml\")\n",
        "                          target_text = target_text.decode(\"utf-8\")\n",
        "                          target_text = re.sub('<.*?>|&lt;.*?&gt;|&quot;|&apos;|{}', ' ', target_text)\n",
        "                          target_text = re.sub(r'[ ]{2,}', ' ', target_text).strip()\n",
        "                          target_file.write(str(target_text) + \"\\n\")\n",
        "                          #print(target_text)"
      ],
      "metadata": {
        "id": "KFjWrzT6pSfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have a translation memory that contains News translations."
      ],
      "metadata": {
        "id": "hFsaWt3PqOFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdd.download_file_from_google_drive(file_id=\"1xh6k91VFfiWpUnjrs-9mU_tkRkdS-INm\", dest_path=\"/content/news.en-tr.tmx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hywRVrX6qf7C",
        "outputId": "bc019595-db7f-4295-eb08-c3f488029858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 1xh6k91VFfiWpUnjrs-9mU_tkRkdS-INm into /content/news.en-tr.tmx... Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We just need to call our function now.\n",
        "\n"
      ],
      "metadata": {
        "id": "9GAitwTSr0B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z1JhQUr_qphF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can download and check the parallel data that we created.\n",
        "\n",
        "(You can see them in the files panel once you hit refresh)"
      ],
      "metadata": {
        "id": "d3FXTYl5rcRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5 - Creating a test set and calculating BLEU\n",
        "\n",
        "In this part, we'll see how our model performs on a test set we create from our in-domain data. \n",
        "\n",
        "Usually, we don't want use all our in-domain data for training. We allocate a portion of it for testing purposes and we make sure that we don't mix this in the training data. Because if we do, it will be sort-of cheating and the results we get won't reflect the generalized quality of the model.\n",
        "\n",
        "Let's see how big is our data first:"
      ],
      "metadata": {
        "id": "o0DE_xMbrhB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute this to see the size of news dataset\n",
        "#NOTE: This is not Python! \n",
        "!wc news.en-tr.en news.en-tr.tr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yELXCDMugfmO",
        "outputId": "ffb51300-60d4-4a6a-d493-5e9efceff2ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  10007  197808 1180320 news.en-tr.tmx.en\n",
            "  10007  149199 1255282 news.en-tr.tmx.tr\n",
            "  20014  347007 2435602 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's take a portion of it, say the final 200 samples as testing data, and the rest as training data."
      ],
      "metadata": {
        "id": "pAUVcrWXhEdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n -200 news.en-tr.en > news.en-tr.train.en \n",
        "!tail -n 200 news.en-tr.en > news.en-tr.test.en \n",
        "\n",
        "!head -n -200 news.en-tr.en > news.en-tr.train.tr \n",
        "!tail -n 200 news.en-tr.en > news.en-tr.test.tr "
      ],
      "metadata": {
        "id": "41N-3DswhdfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc news.en-tr.test.en news.en-tr.test.tr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkr7k2JUiBAm",
        "outputId": "b01c827c-0b0a-4a78-daea-602849885641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   9987  197548 1178714 news.en-tr.train.en\n",
            "   9987  149030 1253863 news.en-tr.train.tr\n",
            "  19974  346578 2432577 total\n",
            "  20  260 1606 news.en-tr.test.en\n",
            "  20  169 1419 news.en-tr.test.tr\n",
            "  40  429 3025 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, now we have a separate training and testing set. Let's see how our generic English-Turkish model performs on it.\n",
        "\n",
        "We'll first translate the English portion of our test set using our model."
      ],
      "metadata": {
        "id": "JY3QPWqMi7pS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#You can use this function to read a file line by line into a list\n",
        "def read_file_lines_to_list(filename):\n",
        "  return [l[:-1] for l in open(filename, 'r').readlines()]\n",
        "\n",
        "#You can use this function to write a list of strings into a file\n",
        "def write_list_to_file(strlist, filename):\n",
        "  with open(filename, 'w') as f:\n",
        "    for s in strlist:\n",
        "      f.write(s+\"\\n\")\n",
        "      \n",
        "\n",
        "#You can use this function to see first n elements in a list\n",
        "def get_first_n(l, n):\n",
        "  for i, elem in enumerate(l):\n",
        "    print(elem)\n",
        "    if i == n:\n",
        "      break"
      ],
      "metadata": {
        "id": "sSLuFu5Yi7Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7P3uPK_Aj4DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the same code we used before to translate the sentences in our list"
      ],
      "metadata": {
        "id": "FWtFz5T2kfEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cz7Tbj_8kksT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the translations look like"
      ],
      "metadata": {
        "id": "fRDTHCTzl9Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_yQXULL1l5cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's write the translations to a textfile."
      ],
      "metadata": {
        "id": "EffswOIdn62d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1UKNZXSDn_h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For doing BLEU evaluations, we can use the SacreBLEU package"
      ],
      "metadata": {
        "id": "yB2yMe2InBa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-88G48FDnOb0",
        "outputId": "455ab2e9-2cbf-4417-c386-4a2d7358bf3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.9)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2019.12.20)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2.3.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.4.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.20.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sacrebleu ______ -i ________"
      ],
      "metadata": {
        "id": "XV1B0T_rnvc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 7 - Fine-tuning our model\n"
      ],
      "metadata": {
        "id": "w95KvrhTqlW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a discrepancy between AfroTranslate and JoeyNMT. \n",
        "\n",
        "We need to restart our runtime at this step and install JoeyNMT and bpemb again. \n",
        "\n",
        "Don't worry, the files we have prepared will stay in their places. "
      ],
      "metadata": {
        "id": "yiwcjrJV5_V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "metadata": {
        "id": "3HsYgpZf5-kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bpemb import BPEmb\n",
        "\n",
        "BPE_VOCAB_SIZE = 5000\n",
        "bpemb_en = BPEmb(lang=\"en\", vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=True)\n",
        "bpemb_tr = BPEmb(lang=\"tr\", vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=True)"
      ],
      "metadata": {
        "id": "MyEFTPox6r99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BPE encode the in-domain training data\n",
        "news_en_bpe = [' '.join(bpemb_en.encode(l[:-1])) for l in open('news.en-tr.en', 'r').readlines()]\n",
        "news_tr_bpe = [' '.join(bpemb_tr.encode(l[:-1])) for l in open('news.en-tr.tr', 'r').readlines()]\n",
        "\n",
        "#Allocate first 1000 samples as development set\n",
        "news_dev_en_bpe = news_en_bpe[0:1000]\n",
        "news_dev_tr_bpe = news_tr_bpe[0:1000]\n",
        "\n",
        "#Allocate last 200 samples as test set\n",
        "news_test_en_bpe = news_en_bpe[-200:]\n",
        "news_test_tr_bpe = news_tr_bpe[-200:]\n",
        "\n",
        "#Allocate rest as training data\n",
        "news_train_en_bpe = news_en_bpe[1000:-200]\n",
        "news_train_tr_bpe = news_tr_bpe[1000:-200]"
      ],
      "metadata": {
        "id": "LXxiFiZLq1zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_list_to_file(news_train_en_bpe, \"news.en-tr.train.BPE.en\")\n",
        "write_list_to_file(news_train_tr_bpe, \"news.en-tr.train.BPE.tr\")\n",
        "write_list_to_file(news_dev_en_bpe, \"news.en-tr.dev.BPE.en\")\n",
        "write_list_to_file(news_dev_tr_bpe, \"news.en-tr.dev.BPE.tr\")\n",
        "write_list_to_file(news_test_en_bpe, \"news.en-tr.test.BPE.en\")\n",
        "write_list_to_file(news_test_tr_bpe, \"news.en-tr.test.BPE.tr\")"
      ],
      "metadata": {
        "id": "i50AAey2sb-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a JoeyNMT config file for finetuning training\n",
        "# Changes from previous config are dataset names, model name, batch size and learning rate\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"entr_finetune\"\n",
        "\n",
        "data:\n",
        "    src: \"en\"\n",
        "    trg: \"tr\"\n",
        "    train: \"/content/news.en-tr.train.BPE\"\n",
        "    dev:   \"/content/news.en-tr.dev.BPE\"\n",
        "    test:  \"/content/news.en-tr.test.BPE\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 150\n",
        "    src_vocab: \"/content/models/entr/src_vocab.txt\"\n",
        "    trg_vocab: \"/content/models/entr/trg_vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    load_model: \"/content/models/entr/best.ckpt\" # Load base model from its best scoring checkpoint (from local)\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0001\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 128\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 64\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 100          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"/content/models/entr_finetune\"\n",
        "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_best_ckpts: 3\n",
        "    save_latest_ckpt: True\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\"\n",
        "with open(\"/content/entr_finetune.yaml\",'w') as f:\n",
        "    f.write(config)"
      ],
      "metadata": {
        "id": "s_0Ukw5sqscX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cd joeynmt; python3 -m joeynmt train \"/content/entr_finetune.yaml\""
      ],
      "metadata": {
        "id": "ElUNLotpwEFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c3794e9-65d6-418a-be62-df478b3b74b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-28 08:46:42,471 - INFO - root - Hello! This is Joey-NMT (version 1.5.1).\n",
            "2022-01-28 08:46:42,497 - INFO - joeynmt.data - Loading training data...\n",
            "2022-01-28 08:46:42,681 - INFO - joeynmt.data - Building vocabulary...\n",
            "2022-01-28 08:46:44,037 - INFO - joeynmt.data - Loading dev data...\n",
            "2022-01-28 08:46:44,055 - INFO - joeynmt.data - Loading test data...\n",
            "2022-01-28 08:46:44,058 - INFO - joeynmt.data - Data loaded.\n",
            "2022-01-28 08:46:44,058 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2022-01-28 08:46:44,344 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2022-01-28 08:46:46,757 - INFO - joeynmt.training - Total params: 13372928\n",
            "2022-01-28 08:46:49,639 - INFO - joeynmt.training - Loading model from /content/models/entr/best.ckpt\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                           cfg.name : entr_transformer\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                       cfg.data.src : en\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                       cfg.data.trg : tr\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                     cfg.data.train : /content/news.en-tr.train.BPE\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                       cfg.data.dev : /content/news.en-tr.dev.BPE\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                      cfg.data.test : /content/news.en-tr.test.BPE\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                     cfg.data.level : bpe\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -                 cfg.data.lowercase : False\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -           cfg.data.max_sent_length : 100\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -                 cfg.data.src_vocab : /content/models/entr/src_vocab.txt\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -                 cfg.data.trg_vocab : /content/models/entr/trg_vocab.txt\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -            cfg.training.load_model : /content/models/entr/best.ckpt\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -              cfg.training.patience : 5\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -  cfg.training.learning_rate_factor : 0.5\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 1000\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -            cfg.training.batch_size : 1028\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -            cfg.training.batch_type : token\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 3600\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 1\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -                cfg.training.epochs : 30\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 200\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/entr_finetune\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -             cfg.training.overwrite : True\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -               cfg.training.shuffle : True\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 3\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.3\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 4\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.3\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 8747,\n",
            "\tvalid 1000,\n",
            "\ttest 200\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] ▁in ▁ iz mir ▁in ▁october ▁last ▁year , ▁fat ma ▁ ş ah in , ▁minister ▁of ▁women ▁and ▁social ▁w elf are , ▁ple aded ▁for ▁r ig or ous ▁life ▁imp ris onment ▁at ▁the ▁trial ▁of ▁s . ç . ▁and ▁0 ▁rel atives ▁who ▁were ▁alle ged ▁to ▁have ▁st ab bed ▁his ▁wife ▁to ▁death ▁because ▁she ▁wanted ▁a ▁div or ce .\n",
            "\t[TRG] ▁- ▁i ̇ z mir ’ de ▁geçen ▁yılın ▁ekim ▁ayında , ▁boş an mak ▁üzere ▁olduğu ▁eş ini ▁b ı çak layarak ▁öldür düğü ▁iddi asıyla , ▁ağır laştır ılmış ▁mü eb bet ▁hapis ▁cez ası ▁istem iyle ▁hakkında ▁dav a ▁aç ılan ▁s . ç . ▁ve ▁0 ▁yak ının ın ▁yarg ılan dığı ▁dav aya ▁da ▁aile ▁ve ▁sosyal ▁politik alar ▁bakanı ▁fat ma ▁şah in , ▁hukuk ▁m üş av iri ▁bir sel ▁kurt ▁aracılığı ▁ile ▁dav aya ▁mü da h il ▁olmak ▁ist edi .\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ▁. (5) ▁, (6) ▁the (7) ▁' (8) ▁and (9) ▁\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ▁. (5) ▁, (6) ▁the (7) ▁' (8) ▁and (9) ▁\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - Number of Src words (types): 9034\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - Number of Trg words (types): 9034\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=9034),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=9034))\n",
            "2022-01-28 08:46:49,965 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 1028\n",
            "\ttotal batch size (w. parallel & accumulation): 1028\n",
            "2022-01-28 08:46:49,966 - INFO - joeynmt.training - EPOCH 1\n",
            "2022-01-28 08:46:49,995 - INFO - joeynmt.training - Epoch   1: total training loss 0.00\n",
            "2022-01-28 08:46:49,995 - INFO - joeynmt.training - EPOCH 2\n",
            "2022-01-28 08:47:04,509 - INFO - joeynmt.training - Epoch   2, Step:   114100, Batch Loss:     3.014850, Tokens per Sec:     3210, Lr: 0.000300\n",
            "2022-01-28 08:47:18,757 - INFO - joeynmt.training - Epoch   2, Step:   114200, Batch Loss:     2.906295, Tokens per Sec:     3108, Lr: 0.000300\n",
            "2022-01-28 08:48:37,423 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2022-01-28 08:48:37,424 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-01-28 08:48:37,424 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-01-28 08:48:37,905 - INFO - joeynmt.training - Example #0\n",
            "2022-01-28 08:48:37,905 - INFO - joeynmt.training - \tSource:     ▁defense ▁w ants ▁case ▁dis miss ed ▁on ▁ground s ▁that ▁man ning ' s ▁conf in ement ▁was ▁har sh\n",
            "2022-01-28 08:48:37,905 - INFO - joeynmt.training - \tReference:  ▁savunma ▁man n ing ' in ▁hap s ed ilm esinin ▁sert ▁olduğu ▁gerek ç esiyle ▁dav anın ▁kap an masını ▁ist iyor\n",
            "2022-01-28 08:48:37,905 - INFO - joeynmt.training - \tHypothesis: ▁savunma , ▁insan ın ▁kaf asının ▁kar ıştır ılması ▁zarar lı ▁olduğu ▁yer lerde ▁suç lam ayı ▁ist iyor .\n",
            "2022-01-28 08:48:37,905 - INFO - joeynmt.training - Example #1\n",
            "2022-01-28 08:48:37,906 - INFO - joeynmt.training - \tSource:     ▁the ▁army ▁private ▁is ▁acc used ▁of ▁ste aling ▁thous ands ▁of ▁class ified ▁doc uments\n",
            "2022-01-28 08:48:37,906 - INFO - joeynmt.training - \tReference:  ▁ordu daki ▁er ▁bin lerce ▁gizli ▁bel ge ▁çal mak la ▁suç lan ıyor\n",
            "2022-01-28 08:48:37,906 - INFO - joeynmt.training - \tHypothesis: ▁ordu ▁özel liği ▁bin lerce ▁sınıf landır ılmış ▁belg eyi ▁çal mak la ▁suç landı .\n",
            "2022-01-28 08:48:37,906 - INFO - joeynmt.training - Example #2\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - \tSource:     ▁pro sec ut ors ▁tried ▁to ▁establish ▁fr iday ▁that ▁army ▁private ▁br ad ley ▁man ning ▁- - ▁char ged ▁in ▁the ▁largest ▁le ak ▁of ▁class ified ▁material ▁in ▁u . s . ▁history ▁- - ▁miss ed ▁multiple ▁opportun ities ▁to ▁compl ain ▁about ▁the ▁m ist reat ment ▁he ' s ▁alle ging ▁he ▁suffered ▁in ▁military ▁cust ody .\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - \tReference:  ▁sav cılar ▁cum a ▁günü ▁abd ▁tarihinde ki ▁en ▁büyük ▁gizli ▁mat er yal ▁s ız ınt ısıyla ▁suç lanan ▁ordu daki ▁er ▁br ad ley ▁man n ing ' in ▁askeri ▁göz altı ▁sırasında ▁yaşadığı ▁kötü ▁davranış ▁iddi aları ▁hakkında ▁birden ▁fazla ▁ş ik ayet ▁et me ▁fır sat ını ▁kaç ır dı ğını ▁belirt meye ▁çalıştı .\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - \tHypothesis: ▁sav cılar , ▁abd ▁tarih inin ▁en ▁büyük ▁sınıf landır ılmış ▁mat er yal in ▁en ▁büyük ▁le ak esinde , ▁abd ▁tarih indeki ▁askeri ▁göz alt ına ▁sık ıntı ▁çek tiği ▁cum a , ▁asker inin ▁tutuk landı ğını ▁iddia ▁eden ▁bir ▁çok ▁fır sat ▁kaç ır dı .\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - Example #3\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - \tSource:     ▁while ▁cross - ex am ining ▁man ning ▁at ▁a ▁pre - t rial ▁he aring ▁at ▁f t . ▁me ade , ▁mary land , ▁pro sec ut or ▁maj . ▁ash den ▁fe in ▁ass er ted ▁that ▁records ▁of ▁week ly ▁vis its ▁man ning ▁had ▁with ▁unit ▁officers ▁during ▁nine ▁months ▁of ▁det ention ▁at ▁quant ico , ▁virginia , ▁show ▁no ▁compl ain ts ▁about ▁his ▁treatment .\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - \tReference:  ▁f t . ▁me ade ▁mar y land ' deki ▁dur uş ma ▁öncesi ▁c els ede ▁sav cı ▁ma j . ▁as h den ▁fe in ▁man n ing ' i ▁çap raz ▁sor gu lar ken , ▁qu an ti co , ▁vir g ini a ' da ▁dokuz ▁ay ▁göz alt ında ▁kal dığı ▁sür ece ▁birim ▁mem ur larıyla ▁haft alık ▁ziyaret lerinin ▁kayıt larının ▁kötü ▁davran ılması yla ▁ilgili ▁bir ▁ş ik ayet ▁göster med iğini ▁ortaya ▁koy du .\n",
            "2022-01-28 08:48:37,908 - INFO - joeynmt.training - \tHypothesis: ▁f t , ▁b t , ▁mar y land , ▁mar y land , ▁pr os ter th en ▁f t , ▁mar y land , ▁pro ter in , ▁f t t , ▁pr os ed ür <unk> de , ▁00 ▁ay lık ▁tespit ▁edilen , ▁vir g ini a , ▁teda v isi ▁hakkında ▁ş ik â yet ▁ver med iğini ▁belirt ti .\n",
            "2022-01-28 08:48:37,908 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   114200: bleu:  15.27, loss: 73264.8047, ppl:  10.9133, duration: 79.1501s\n",
            "2022-01-28 08:48:52,204 - INFO - joeynmt.training - Epoch   2, Step:   114300, Batch Loss:     2.934678, Tokens per Sec:     3201, Lr: 0.000300\n",
            "2022-01-28 08:49:06,575 - INFO - joeynmt.training - Epoch   2, Step:   114400, Batch Loss:     3.330844, Tokens per Sec:     3175, Lr: 0.000300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a translator for our new finetuned model"
      ],
      "metadata": {
        "id": "B5qWnAmQxLWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_translator = MasakhaneTranslate(model_path=\"/content/models/entr_finetune\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "4M6VB4QexK0P",
        "outputId": "2cac592d-106d-44da-fa4a-e18bbe00bdc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-8affe51e2add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinetuned_translator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMasakhaneTranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/models/entr_finetune\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/afrotranslate/translator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, model_name, version, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \"\"\"\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/afrotranslate/translator.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_path, model_name, version, device)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mckpt_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_candidates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No checkpoint file under model directory'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_candidates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WARNING: More than one checkpoint under model directory. Taking first:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_candidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: ('No checkpoint file under model directory', '/content/models/entr_finetune')"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's test on the finetuned model to see if it has improved on the test set"
      ],
      "metadata": {
        "id": "5tIKZG18A3KT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7tEJdHnZxskB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aQLZu7d_xsh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VIs2sJBWxsfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l929HimrxS0a"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Part 8 (homework) - Training a model from scratch\n",
        "\n",
        "## Retrieve your data & make a parallel corpus\n",
        "\n",
        "In this part we will use open corpus available from OPUS repository to train a translation model. We will first download the data, create training, development, testing sets from it and then use JoeyNMT to train a baseline model. \n",
        "\n",
        "In the next cell, you need to set the languages you want to work with and specify which corpus you want to use to train. \n",
        "\n",
        "To select a corpus go to https://opus.nlpl.eu/, enter your language pair and select one that you think is more appropriate (size, domain)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Cn3tgQLzUxwn"
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"tr\"\n",
        "\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO5IY_RywQyA",
        "outputId": "06663965-1bc8-4ee6-f622-528cac71dff0"
      },
      "source": [
        "# This will save it to a folder in our gdrive instead!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p \"/content/drive/My Drive/mt-workshop/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/mt-workshop/%s-%s-%s\" % (source_language, target_language, tag)\n",
        "\n",
        "!echo $gdrive_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/mt-workshop/en-tr-baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gA75Fs9ys8Y9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2977aa26-bec6-4e69-b72a-654090a41441"
      },
      "source": [
        "# Install opus-tools (Warning! This is not really python)\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opustools-pkg\n",
            "  Downloading opustools_pkg-0.0.52-py3-none-any.whl (80 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 20 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 80 kB 5.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: opustools-pkg\n",
            "Successfully installed opustools-pkg-0.0.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xq-tDZVks7ZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f74b9853-c632-42cb-9c14-19f0a2583d6d"
      },
      "source": [
        "# TODO: Indicate here the ID of the corpus you want to use from OPUS\n",
        "opus_corpus = \"TED2020\" \n",
        "os.environ[\"corpus\"] = opus_corpus\n",
        "\n",
        "# Downloading our corpus \n",
        "! opus_read -d $corpus -s $src -t $tgt -wm moses -w $corpus.$src $corpus.$tgt -q\n",
        "\n",
        "# Extract the corpus file\n",
        "! gunzip ${corpus}_latest_xml_$src-$tgt.xml.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/TED2020/latest/xml/en-tr.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "   2 MB https://object.pouta.csc.fi/OPUS-TED2020/v1/xml/en-tr.xml.gz\n",
            "  47 MB https://object.pouta.csc.fi/OPUS-TED2020/v1/xml/en.zip\n",
            "  37 MB https://object.pouta.csc.fi/OPUS-TED2020/v1/xml/tr.zip\n",
            "\n",
            "  85 MB Total size\n",
            "./TED2020_latest_xml_en-tr.xml.gz ... 100% of 2 MB\n",
            "./TED2020_latest_xml_en.zip ... 100% of 47 MB\n",
            "./TED2020_latest_xml_tr.zip ... 100% of 37 MB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sazl7hv9xZFg"
      },
      "source": [
        "# Read the corpus into python lists\n",
        "source_file = opus_corpus + '.' + source_language\n",
        "target_file = opus_corpus + '.' + target_language\n",
        "\n",
        "src_all = [sentence.strip() for sentence in open(source_file).readlines()]\n",
        "tgt_all = [sentence.strip() for sentence in open(target_file).readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjcOBlYMxxTC",
        "outputId": "71fffa11-d629-4780-94fb-c1b511acb6b4"
      },
      "source": [
        "# Let's take a peek at the files\n",
        "print(\"Source size:\", len(src_all))\n",
        "print(\"Target size:\", len(tgt_all))\n",
        "print(\"--------\")\n",
        "\n",
        "peek_size = 5\n",
        "for i in range(peek_size):\n",
        "  print(\"Sent #\", i)\n",
        "  print(\"SRC:\", src_all[i])\n",
        "  print(\"TGT:\", tgt_all[i])\n",
        "  print(\"---------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source size: 374378\n",
            "Target size: 374378\n",
            "--------\n",
            "Sent # 0\n",
            "SRC: Thank you so much , Chris .\n",
            "TGT: Çok teşekkür ederim Chris .\n",
            "---------\n",
            "Sent # 1\n",
            "SRC: And it 's truly a great honor to have the opportunity to come to this stage twice ; I 'm extremely grateful .\n",
            "TGT: Bu sahnede ikinci kez yer alma fırsatına sahip olmak gerçekten büyük bir onur . Çok minnettarım .\n",
            "---------\n",
            "Sent # 2\n",
            "SRC: I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "TGT: Bu konferansta çok mutlu oldum , ve anlattıklarımla ilgili güzel yorumlarınız için sizlere çok teşekkür ederim .\n",
            "---------\n",
            "Sent # 3\n",
            "SRC: And I say that sincerely , partly because ( Mock sob ) I need that .\n",
            "TGT: Bunu içtenlikle söylüyorum , çünkü ... ( Ağlama taklidi ) Buna ihtiyacım var .\n",
            "---------\n",
            "Sent # 4\n",
            "SRC: ( Laughter ) Put yourselves in my position .\n",
            "TGT: ( Kahkahalar ) Kendinizi benim yerime koyun !\n",
            "---------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkuK3B4p2AkN"
      },
      "source": [
        "## Making training, development and testing sets\n",
        "\n",
        "We need to pick training, development and testing sets from our corpus. Training set will contain the sentences that we'll teach our model. Development set will be used to see how our model is progressing during the training. And finally, testing set will be used to evaluate the model.\n",
        "\n",
        "You can optionally load your own testing set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkcE9T75y9za",
        "outputId": "5a45acc5-04ac-4334-f53b-2518fd0c6f4c"
      },
      "source": [
        "# TODO: Determine ratios of each set\n",
        "all_size = len(src_all)\n",
        "dev_size = 1000\n",
        "test_size = 1000\n",
        "train_size = all_size - test_size - dev_size\n",
        "\n",
        "src_train = src_all[0:train_size]\n",
        "tgt_train = tgt_all[0:train_size]\n",
        "\n",
        "src_dev = src_all[train_size:train_size+dev_size]\n",
        "tgt_dev = tgt_all[train_size:train_size+dev_size]\n",
        "\n",
        "src_test = src_all[train_size+dev_size:all_size]\n",
        "tgt_test = tgt_all[train_size+dev_size:all_size]\n",
        "\n",
        "print(\"Set sizes\")\n",
        "print(\"All:\", len(src_all))\n",
        "print(\"Train:\", len(src_train))\n",
        "print(\"Dev:\", len(src_dev))\n",
        "print(\"Test:\", len(src_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Set sizes\n",
            "All: 374378\n",
            "Train: 372378\n",
            "Dev: 1000\n",
            "Test: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# Preprocessing the Data into Subword BPE Tokens\n",
        "\n",
        "- One of the most powerful improvements for neural machine translation is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- BPE tokenization limits the number of vocabulary into a certain size by smartly dividing words into subwords\n",
        "\n",
        "- This is especially useful for agglutinative languages (like Turkish) where vocabulary is effectively endless. \n",
        "\n",
        "- Below you have the scripts for doing BPE tokenization of our data. We use bpemb library that has pre-trained BPE models to convert our data into subwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xYKgReL6A76",
        "outputId": "b61b48f6-873b-4c78-9b45-14e5f81071be"
      },
      "source": [
        "! pip install bpemb\n",
        "from bpemb import BPEmb\n",
        "\n",
        "BPE_VOCAB_SIZE = 5000\n",
        "bpemb_src = BPEmb(lang=source_language, vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=False)\n",
        "bpemb_tgt = BPEmb(lang=target_language, vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bpemb\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bpemb) (4.41.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from bpemb) (3.6.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 10.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bpemb) (2.23.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (5.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Installing collected packages: sentencepiece, bpemb\n",
            "Successfully installed bpemb-0.3.3 sentencepiece-0.1.96\n",
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs5000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 315918/315918 [00:00<00:00, 557183.59B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/tr/tr.wiki.bpe.vs5000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 315775/315775 [00:00<00:00, 713720.23B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_RWTDq169WQ",
        "outputId": "e2c68f65-1b1a-4b3b-de0a-e8383b822473"
      },
      "source": [
        "# Testing BPE encoding\n",
        "encoded_tokens = bpemb_src.encode(\"This is a test sentence to demonstrate how BPE encoding works for our source language.\")\n",
        "print(encoded_tokens)\n",
        "\n",
        "encoded_string = \" \".join(encoded_tokens)\n",
        "print(encoded_string)\n",
        "\n",
        "decoded_string = bpemb_src.decode(encoded_tokens)\n",
        "print(decoded_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁', 'T', 'h', 'is', '▁is', '▁a', '▁test', '▁sent', 'ence', '▁to', '▁demonstr', 'ate', '▁how', '▁', 'BPE', '▁enc', 'od', 'ing', '▁works', '▁for', '▁our', '▁source', '▁language', '.']\n",
            "▁ T h is ▁is ▁a ▁test ▁sent ence ▁to ▁demonstr ate ▁how ▁ BPE ▁enc od ing ▁works ▁for ▁our ▁source ▁language .\n",
            "This is a test sentence to demonstrate how BPE encoding works for our source language.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMMbGLEX8pct"
      },
      "source": [
        "# Shortcut functions to encode and decode\n",
        "def encode_bpe(string, lang, to_lower=True):\n",
        "  if to_lower:\n",
        "    string = string.lower()\n",
        "  if lang == source_language:\n",
        "    return \" \".join(bpemb_src.encode(string))\n",
        "  elif lang == target_language:\n",
        "    return \" \".join(bpemb_tgt.encode(string))\n",
        "  else:\n",
        "    return \"\"\n",
        "\n",
        "def decode_bpe(string, lang):\n",
        "  tokens = string.strip().split()\n",
        "  if lang == source_language:\n",
        "    return bpemb_src.decode(tokens)\n",
        "  elif lang == target_language:\n",
        "    return bpemb_tgt.decode(tokens)\n",
        "  else:\n",
        "    return \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fMMGYQ27ZUN"
      },
      "source": [
        "# Let's encode all our sets with BPE\n",
        "src_train_bpe = [encode_bpe(sentence, source_language) for sentence in src_train]\n",
        "tgt_train_bpe = [encode_bpe(sentence, target_language) for sentence in tgt_train]\n",
        "\n",
        "src_dev_bpe = [encode_bpe(sentence, source_language) for sentence in src_dev]\n",
        "tgt_dev_bpe = [encode_bpe(sentence, target_language) for sentence in tgt_dev]\n",
        "\n",
        "src_test_bpe = [encode_bpe(sentence, source_language) for sentence in src_test]\n",
        "tgt_test_bpe = [encode_bpe(sentence, target_language) for sentence in tgt_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRQ7hNIS0D0z"
      },
      "source": [
        "# Now let's write all our sets into separate files\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_train, tgt_train):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_dev, tgt_dev):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"test.\"+source_language, \"w\") as src_file, open(\"test.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_test, tgt_test):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"train.bpe.\"+source_language, \"w\") as src_file, open(\"train.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_train_bpe, tgt_train_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"dev.bpe.\"+source_language, \"w\") as src_file, open(\"dev.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_dev_bpe, tgt_dev_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"test.bpe.\"+source_language, \"w\") as src_file, open(\"test.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_test_bpe, tgt_test_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "hxxBOCA-xXhy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75594eef-8053-4f99-e1e7-b07c1c54003b"
      },
      "source": [
        "# Doublecheck the files. There should be no extra quotation marks or weird characters.\n",
        "! head -n5 train.*\n",
        "! head -n5 dev.*\n",
        "! head -n5 test.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.bpe.en <==\n",
            "▁than k ▁you ▁so ▁much ▁, ▁ch ris ▁.\n",
            "▁and ▁it ▁' s ▁tr u ly ▁a ▁great ▁honor ▁to ▁have ▁the ▁opportun ity ▁to ▁come ▁to ▁this ▁stage ▁twice ▁; ▁i ▁' m ▁extrem ely ▁gr ate ful ▁.\n",
            "▁i ▁have ▁been ▁bl own ▁away ▁by ▁this ▁conference ▁, ▁and ▁i ▁want ▁to ▁than k ▁all ▁of ▁you ▁for ▁the ▁many ▁n ice ▁com ments ▁about ▁what ▁i ▁had ▁to ▁say ▁the ▁other ▁night ▁.\n",
            "▁and ▁i ▁say ▁that ▁s inc er ely ▁, ▁part ly ▁because ▁( ▁m ock ▁so b ▁ ) ▁i ▁need ▁that ▁.\n",
            "▁( ▁la ugh ter ▁ ) ▁put ▁y ours elves ▁in ▁my ▁position ▁.\n",
            "\n",
            "==> train.bpe.tr <==\n",
            "▁çok ▁teş ek kür ▁eder im ▁chris ▁.\n",
            "▁bu ▁sahne de ▁ikinci ▁kez ▁yer ▁al ma ▁fır sat ına ▁sahip ▁olmak ▁ger ç ekten ▁büyük ▁bir ▁onur ▁. ▁çok ▁min net tar ım ▁.\n",
            "▁bu ▁konfer ans ta ▁çok ▁mut lu ▁ol d um ▁, ▁ve ▁anlat t ıkları m la ▁ilgili ▁güzel ▁yorum ların ız ▁için ▁s iz lere ▁çok ▁teş ek kür ▁eder im ▁.\n",
            "▁bunu ▁iç ten likle ▁söy l üyor um ▁, ▁çünkü ▁... ▁( ▁ağ lama ▁tak li di ▁) ▁buna ▁ihtiy ac ım ▁var ▁.\n",
            "▁( ▁kah k ah alar ▁) ▁kend in izi ▁benim ▁yer ime ▁koy un ▁ !\n",
            "\n",
            "==> train.en <==\n",
            "Thank you so much , Chris .\n",
            "And it 's truly a great honor to have the opportunity to come to this stage twice ; I 'm extremely grateful .\n",
            "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "And I say that sincerely , partly because ( Mock sob ) I need that .\n",
            "( Laughter ) Put yourselves in my position .\n",
            "\n",
            "==> train.tr <==\n",
            "Çok teşekkür ederim Chris .\n",
            "Bu sahnede ikinci kez yer alma fırsatına sahip olmak gerçekten büyük bir onur . Çok minnettarım .\n",
            "Bu konferansta çok mutlu oldum , ve anlattıklarımla ilgili güzel yorumlarınız için sizlere çok teşekkür ederim .\n",
            "Bunu içtenlikle söylüyorum , çünkü ... ( Ağlama taklidi ) Buna ihtiyacım var .\n",
            "( Kahkahalar ) Kendinizi benim yerime koyun !\n",
            "==> dev.bpe.en <==\n",
            "▁it ▁' s ▁a ▁very ▁big ▁signal ▁; ▁it ▁' s ▁sent ▁to ▁the ▁bra in ▁says ▁, ▁\" ▁go ▁and ▁e at ▁. ▁\"\n",
            "▁you ▁have ▁stop ▁sign als ▁- - ▁we ▁have ▁up ▁to ▁eight ▁stop ▁sign als ▁.\n",
            "▁at ▁least ▁in ▁my ▁case ▁, ▁they ▁are ▁not ▁list ened ▁to ▁.\n",
            "▁( ▁la ugh ter ▁ ) ▁so ▁what ▁happ ens ▁if ▁the ▁big ▁bra in ▁in ▁the ▁integr ation ▁over r ides ▁the ▁signal ▁ ?\n",
            "▁so ▁if ▁you ▁over r ide ▁the ▁hun ger ▁signal ▁, ▁you ▁can ▁have ▁a ▁dis order ▁, ▁which ▁is ▁called ▁an ore x ia ▁.\n",
            "\n",
            "==> dev.bpe.tr <==\n",
            "▁bu ▁çok ▁güçlü ▁bir ▁sin yal dir ▁. ▁bey ine ▁gider ▁ve ▁der ▁ki ▁, ▁\" ▁git ▁ve ▁ye ▁. ▁\"\n",
            "▁ayrıca ▁dur ▁sin yal leri ▁de ▁vardır . ▁hemen ▁hemen ▁sekiz ▁tane ▁farklı ▁dur ▁sin yal imiz ▁var ▁.\n",
            "▁ama ▁benim ▁g ib iler ▁bu ▁sin yal leri ▁pek ▁de ▁din lem iyor lar ▁.\n",
            "▁( ▁gül üş meler ▁) ▁p eki ▁, ▁eğer ▁büyük ▁bey in ▁bu ▁gönder ilen ▁sin yal i ▁gör mez den ▁gelir se ▁ne ▁olur ▁ ?\n",
            "▁eğer ▁aç lık ▁sin yal ini ▁gör mez den ▁gelir sen iz ▁an or ek si ▁den en ▁hast alı ğa ▁tut ulur sun uz ▁.\n",
            "\n",
            "==> dev.en <==\n",
            "It 's a very big signal ; it 's sent to the brain says , \" Go and eat . \"\n",
            "You have stop signals -- we have up to eight stop signals .\n",
            "At least in my case , they are not listened to .\n",
            "( Laughter ) So what happens if the big brain in the integration overrides the signal ?\n",
            "So if you override the hunger signal , you can have a disorder , which is called anorexia .\n",
            "\n",
            "==> dev.tr <==\n",
            "Bu çok güçlü bir sinyaldir . Beyine gider ve der ki , \" Git ve ye . \"\n",
            "Ayrıca dur sinyalleri de vardır. hemen hemen sekiz tane farklı dur sinyalimiz var .\n",
            "Ama benim gibiler bu sinyalleri pek de dinlemiyorlar .\n",
            "( Gülüşmeler ) Peki , eğer büyük beyin bu gönderilen sinyali görmezden gelirse ne olur ?\n",
            "Eğer açlık sinyalini görmezden gelirseniz anoreksi denen hastalığa tutulursunuz .\n",
            "==> test.bpe.en <==\n",
            "▁it ▁' s ▁something ▁called ▁the ▁re ward ▁sched ule ▁.\n",
            "▁and ▁by ▁this ▁, ▁i ▁mean ▁look ing ▁at ▁what ▁mill ions ▁upon ▁mill ions ▁of ▁people ▁have ▁done ▁and ▁care ful ly ▁cal ib r ating ▁the ▁rate ▁, ▁the ▁nature ▁, ▁the ▁type ▁, ▁the ▁int ensity ▁of ▁re wards ▁in ▁games ▁to ▁keep ▁them ▁eng aged ▁over ▁st ag ger ing ▁amount s ▁of ▁time ▁and ▁effort ▁.\n",
            "▁now ▁, ▁to ▁t ry ▁and ▁expl ain ▁this ▁in ▁s ort ▁of ▁real ▁terms ▁, ▁i ▁want ▁to ▁talk ▁about ▁a ▁kind ▁of ▁t ask ▁that ▁might ▁fall ▁to ▁you ▁in ▁so ▁many ▁games ▁.\n",
            "▁go ▁and ▁get ▁a ▁certain ▁amount ▁of ▁a ▁certain ▁little ▁game - y ▁it em ▁.\n",
            "▁let ▁' s ▁say ▁, ▁for ▁the ▁s ake ▁of ▁arg ument ▁, ▁my ▁mission ▁is ▁to ▁get ▁ 15 ▁p ies ▁and ▁i ▁can ▁get ▁ 15 ▁p ies ▁by ▁kill ing ▁these ▁c ute ▁, ▁little ▁mon st ers ▁.\n",
            "\n",
            "==> test.bpe.tr <==\n",
            "▁ödül ▁tak v imi ▁den iyor ▁.\n",
            "▁bununla ▁, ▁milyon lar ca ▁insan ın ▁ne ▁yaptı ğına ▁ve ▁sür es iz ▁zaman ▁ve ▁ç aba ▁har c ay arak ▁oyun lara ▁bağlı ▁kal malar ını ▁sağlayan ▁ödül lerin ▁türü ▁, ▁çeş idi ▁ve ▁ölç üs ünün ▁ay ar lan masına ▁dikkat le ▁bak mayı ▁k ast ed iyor um ▁.\n",
            "▁şim di ▁, ▁bunu ▁gerçek ▁anlam da ▁den em ek ▁ve ▁açıklam ak ▁için ▁birçok ▁oy unda ▁karşılaş abilece ğin iz ▁bir ▁görev ▁hakkında ▁konuş mak ▁ist iyor um ▁.\n",
            "▁gi di p ▁küçük ▁bir ▁oyun ▁nes n esinden ▁belirli ▁miktar da ▁getir in ▁.\n",
            "▁örnek ▁olması ▁için ▁far z ed el im ▁ki ▁benim ▁görev im ▁ 15 ▁tane ▁ç ör ek ▁getir mek ▁ve ▁ben ▁ 15 ▁tane ▁ç ör eği ▁şir in ▁, ▁küçük ▁can av ar ları ▁öldür erek ▁getir ebilir im ▁.\n",
            "\n",
            "==> test.en <==\n",
            "It 's something called the reward schedule .\n",
            "And by this , I mean looking at what millions upon millions of people have done and carefully calibrating the rate , the nature , the type , the intensity of rewards in games to keep them engaged over staggering amounts of time and effort .\n",
            "Now , to try and explain this in sort of real terms , I want to talk about a kind of task that might fall to you in so many games .\n",
            "Go and get a certain amount of a certain little game-y item .\n",
            "Let 's say , for the sake of argument , my mission is to get 15 pies and I can get 15 pies by killing these cute , little monsters .\n",
            "\n",
            "==> test.tr <==\n",
            "Ödül Takvimi deniyor .\n",
            "Bununla , milyonlarca insanın ne yaptığına ve süresiz zaman ve çaba harcayarak oyunlara bağlı kalmalarını sağlayan ödüllerin türü , çeşidi ve ölçüsünün ayarlanmasına dikkatle bakmayı kastediyorum .\n",
            "Şimdi , bunu gerçek anlamda denemek ve açıklamak için birçok oyunda karşılaşabileceğiniz bir görev hakkında konuşmak istiyorum .\n",
            "Gidip küçük bir oyun nesnesinden belirli miktarda getirin .\n",
            "Örnek olması için farzedelim ki benim görevim 15 tane çörek getirmek ve ben 15 tane çöreği şirin , küçük canavarları öldürerek getirebilirim .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "IlMitUHR8Qy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "593e67cf-6fd0-437b-c3f3-f5991f564715"
      },
      "source": [
        "# If creating data for the first time, move all prepared data to the mounted location in google drive\n",
        "! mkdir \"$gdrive_path\"/data\n",
        "! cp train.* \"$gdrive_path\"/data\n",
        "! cp test.* \"$gdrive_path\"/data\n",
        "! cp dev.* \"$gdrive_path\"/data\n",
        "! ls \"$gdrive_path\"/data  #See the contents of the drive directory"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev.bpe.en  dev.en  test.bpe.en  test.en  train.bpe.en\ttrain.en\n",
            "dev.bpe.tr  dev.tr  test.bpe.tr  test.tr  train.bpe.tr\ttrain.tr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwNhkXPXtFtx"
      },
      "source": [
        "# OR... If continuing from previous run, load files from drive\n",
        "! cp \"$gdrive_path\"/data/dev.* .\n",
        "! cp \"$gdrive_path\"/data/train.* .\n",
        "! cp \"$gdrive_path\"/data/test.* ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iBRMm4kMxZ8L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe0350bc-e88f-482c-ac33-d905b748b4a7"
      },
      "source": [
        "#IMPORTANT: Restart runtime if you have installed AfroTranslate\n",
        "\n",
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'joeynmt' already exists and is not an empty directory.\n",
            "Processing /content/joeynmt\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (1.20.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (57.4.0)\n",
            "Collecting torch>=1.9.0\n",
            "  Using cached torch-1.10.2-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (2.7.0)\n",
            "Collecting torchtext>=0.10.0\n",
            "  Using cached torchtext-0.11.2-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n",
            "Requirement already satisfied: sacrebleu>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (2.0.0)\n",
            "Requirement already satisfied: subword-nmt in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (0.3.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (0.11.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (6.0)\n",
            "Requirement already satisfied: pylint>=2.9.6 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (2.12.2)\n",
            "Requirement already satisfied: six>=1.12 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (1.12.0)\n",
            "Requirement already satisfied: wrapt==1.11.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (1.11.1)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (2.4.1)\n",
            "Requirement already satisfied: toml>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (0.10.2)\n",
            "Requirement already satisfied: astroid<2.10,>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (2.9.3)\n",
            "Requirement already satisfied: isort<6,>=4.2.5 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (5.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (3.10.0.2)\n",
            "Requirement already satisfied: mccabe<0.7,>=0.6 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (0.6.1)\n",
            "Requirement already satisfied: typed-ast<2.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.10,>=2.9.0->pylint>=2.9.6->joeynmt==1.5.1) (1.5.2)\n",
            "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.10,>=2.9.0->pylint>=2.9.6->joeynmt==1.5.1) (1.7.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.5.1) (2019.12.20)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.5.1) (0.4.4)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.5.1) (2.3.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.5.1) (0.8.9)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (1.43.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (3.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (1.35.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.5.1) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.5.1) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.5.1) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.5.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.5.1) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.5.1) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.5.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.5.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.5.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.5.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.5.1) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.5.1) (3.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext>=0.10.0->joeynmt==1.5.1) (4.62.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.5.1) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.5.1) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.5.1) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.5.1) (0.11.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.5.1) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.5.1) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.5.1) (2018.9)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.7/dist-packages (from subword-nmt->joeynmt==1.5.1) (4.0.3)\n",
            "Building wheels for collected packages: joeynmt\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-1.5.1-py3-none-any.whl size=86003 sha256=f0fd9a53198708427af3f8e21bba478d2a83c8bf1f6b08083466061fbae5bf19\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c7_9zzy7/wheels/0a/f4/bf/6c9d3b8efbfece6cd209f865be37382b02e7c3584df2e28ca4\n",
            "Successfully built joeynmt\n",
            "Installing collected packages: torch, torchtext, joeynmt\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.8.0+cu101\n",
            "    Uninstalling torch-1.8.0+cu101:\n",
            "      Successfully uninstalled torch-1.8.0+cu101\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.9.0\n",
            "    Uninstalling torchtext-0.9.0:\n",
            "      Successfully uninstalled torchtext-0.9.0\n",
            "  Attempting uninstall: joeynmt\n",
            "    Found existing installation: joeynmt 1.3\n",
            "    Uninstalling joeynmt-1.3:\n",
            "      Successfully uninstalled joeynmt-1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.10.2 which is incompatible.\n",
            "afrotranslate 0.0.6 requires joeynmt==1.3, but you have joeynmt 1.5.1 which is incompatible.\u001b[0m\n",
            "Successfully installed joeynmt-1.5.1 torch-1.10.2 torchtext-0.11.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu101\n",
            "  Using cached https://download.pytorch.org/whl/cu101/torch-1.8.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.5 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.20.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.2\n",
            "    Uninstalling torch-1.10.2:\n",
            "      Successfully uninstalled torch-1.10.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "torchtext 0.11.2 requires torch==1.10.2, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "joeynmt 1.5.1 requires torch>=1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "afrotranslate 0.0.6 requires joeynmt==1.3, but you have joeynmt 1.5.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0+cu101\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "H-TyjtmXB1mL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c45c71b7-9fcd-42e6-ea03-2c29c0bd3115"
      },
      "source": [
        "#Move everything important under joeynmt directory\n",
        "os.environ[\"data_path\"] = os.path.join(\"joeynmt\", \"data\", source_language + target_language)\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! head -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman\n",
        "\n",
        "# Backup vocab to drive\n",
        "! cp joeynmt/data/$src$tgt/vocab.txt \"$gdrive_path\"/data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev.bpe.en  dev.en  test.bpe.en  test.en  train.bpe.en\ttrain.en\n",
            "dev.bpe.tr  dev.tr  test.bpe.tr  test.tr  train.bpe.tr\ttrain.tr\n",
            "Combined BPE Vocab\n",
            "774\n",
            "531\n",
            "883\n",
            "6397\n",
            "794\n",
            "431\n",
            "381\n",
            "1414\n",
            "761\n",
            "548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PIs1lY2hxMsl"
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"models/entr_transformer/1000.ckpt\"\n",
        "    load_model: \"{gdrive_path}/models/{name}_transformer/1000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "6ZBPFwT94WpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8be06a9-c20c-41e6-f44a-7bffdf87dd8a"
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 16:52:38,026 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-28 16:52:38,058 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-28 16:52:46,845 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-28 16:52:48,172 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-28 16:52:48,225 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-28 16:52:48,238 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-28 16:52:48,238 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-28 16:52:48,484 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-28 16:52:48.642085: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-28 16:52:49,639 - INFO - joeynmt.training - Total params: 13372928\n",
            "2021-07-28 16:52:52,915 - INFO - joeynmt.training - Loading model from /content/drive/My Drive/mt-workshop/en-tr-baseline/models/entr_transformer/1000.ckpt\n",
            "2021-07-28 16:52:53,369 - INFO - joeynmt.helpers - cfg.name                           : entr_transformer\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.src                       : en\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.trg                       : tr\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.train                     : data/entr/train.bpe\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.dev                       : data/entr/dev.bpe\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.test                      : data/entr/test.bpe\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : data/entr/vocab.txt\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : data/entr/vocab.txt\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/drive/My Drive/mt-workshop/en-tr-baseline/models/entr_transformer/1000.ckpt\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1000\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/entr_transformer\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 370216,\n",
            "\tvalid 1000,\n",
            "\ttest 1000\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] ▁than k ▁you ▁so ▁much ▁, ▁ch ris ▁.\n",
            "\t[TRG] ▁çok ▁teş ek kür ▁eder im ▁chris ▁.\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ▁. (5) ▁, (6) ▁the (7) ▁' (8) ▁and (9) ▁\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ▁. (5) ▁, (6) ▁the (7) ▁' (8) ▁and (9) ▁\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - Number of Src words (types): 9034\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - Number of Trg words (types): 9034\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=9034),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=9034))\n",
            "2021-07-28 16:52:53,383 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-28 16:52:53,383 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-28 16:53:08,820 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     5.040347, Tokens per Sec:    15405, Lr: 0.000300\n",
            "2021-07-28 16:53:23,770 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.679378, Tokens per Sec:    16521, Lr: 0.000300\n",
            "2021-07-28 16:53:39,046 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     4.687664, Tokens per Sec:    16342, Lr: 0.000300\n",
            "2021-07-28 16:53:54,321 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.884373, Tokens per Sec:    15794, Lr: 0.000300\n",
            "2021-07-28 16:54:09,629 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     4.775616, Tokens per Sec:    15877, Lr: 0.000300\n",
            "2021-07-28 16:54:25,084 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     3.958221, Tokens per Sec:    15485, Lr: 0.000300\n",
            "2021-07-28 16:54:41,036 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     4.736360, Tokens per Sec:    15393, Lr: 0.000300\n",
            "2021-07-28 16:54:57,260 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.919059, Tokens per Sec:    14830, Lr: 0.000300\n",
            "2021-07-28 16:55:13,384 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     4.417824, Tokens per Sec:    14910, Lr: 0.000300\n",
            "2021-07-28 16:55:29,522 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.567585, Tokens per Sec:    15001, Lr: 0.000300\n",
            "2021-07-28 16:56:17,824 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-28 16:56:17,825 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-28 16:56:18,299 - INFO - joeynmt.training - Example #0\n",
            "2021-07-28 16:56:18,299 - INFO - joeynmt.training - \tSource:     ▁it ▁' s ▁a ▁very ▁big ▁signal ▁; ▁it ▁' s ▁sent ▁to ▁the ▁bra in ▁says ▁, ▁\" ▁go ▁and ▁e at ▁. ▁\"\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tReference:  ▁bu ▁çok ▁güçlü ▁bir ▁sin yal dir ▁. ▁bey ine ▁gider ▁ve ▁der ▁ki ▁, ▁\" ▁git ▁ve ▁ye ▁. ▁\"\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tHypothesis: ▁bu ▁yüzden ▁, ▁\" ▁bu ▁yüzden ▁, ▁\" ▁d ed iğ in iz ▁, ▁\" ▁d ed iğ im ▁, ▁\"\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - Example #1\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tSource:     ▁you ▁have ▁stop ▁sign als ▁- - ▁we ▁have ▁up ▁to ▁eight ▁stop ▁sign als ▁.\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tReference:  ▁ayrıca ▁dur ▁sin yal leri ▁de ▁vardır . ▁hemen ▁hemen ▁sekiz ▁tane ▁farklı ▁dur ▁sin yal imiz ▁var ▁.\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tHypothesis: ▁bu ▁yüzden ▁, ▁bu ▁yüzden ▁, ▁bu ▁yüzden ▁, ▁bu ▁yüzden ▁, ▁bu ▁yüzden ▁, ▁bu ▁yüzden ▁, ▁bu ▁yüzden ▁bir ▁şey ▁.\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - Example #2\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tSource:     ▁at ▁least ▁in ▁my ▁case ▁, ▁they ▁are ▁not ▁list ened ▁to ▁.\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tReference:  ▁ama ▁benim ▁g ib iler ▁bu ▁sin yal leri ▁pek ▁de ▁din lem iyor lar ▁.\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tHypothesis: ▁bu ▁yüzden ▁, ▁bu ▁yüzden ▁, ▁bu ▁yüzden ▁, ▁bu ▁yüzden ▁, ▁bu ▁yüzden ▁,\n",
            "2021-07-28 16:56:18,301 - INFO - joeynmt.training - Example #3\n",
            "2021-07-28 16:56:18,301 - INFO - joeynmt.training - \tSource:     ▁( ▁la ugh ter ▁ ) ▁so ▁what ▁happ ens ▁if ▁the ▁big ▁bra in ▁in ▁the ▁integr ation ▁over r ides ▁the ▁signal ▁ ?\n",
            "2021-07-28 16:56:18,301 - INFO - joeynmt.training - \tReference:  ▁( ▁gül üş meler ▁) ▁p eki ▁, ▁eğer ▁büyük ▁bey in ▁bu ▁gönder ilen ▁sin yal i ▁gör mez den ▁gelir se ▁ne ▁olur ▁ ?\n",
            "2021-07-28 16:56:18,301 - INFO - joeynmt.training - \tHypothesis: ▁( ▁gül üş meler ▁) ▁bu ▁yüzden ▁, ▁bu ▁yüzden ▁bu ▁yüzden ▁bu ▁yüzden ▁bu ▁şekilde ▁nasıl ▁nasıl ▁nasıl ▁yap abilir ▁mi ▁ ?\n",
            "2021-07-28 16:56:18,301 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     2000: bleu:   2.31, loss: 104788.8047, ppl:  65.9925, duration: 48.7782s\n",
            "2021-07-28 16:56:34,324 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     4.015805, Tokens per Sec:    15108, Lr: 0.000300\n",
            "2021-07-28 16:56:50,301 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     4.364112, Tokens per Sec:    14957, Lr: 0.000300\n",
            "2021-07-28 16:57:06,377 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     3.913055, Tokens per Sec:    15505, Lr: 0.000300\n",
            "2021-07-28 16:57:22,392 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     4.053391, Tokens per Sec:    15228, Lr: 0.000300\n",
            "2021-07-28 16:57:38,524 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     4.355020, Tokens per Sec:    15342, Lr: 0.000300\n",
            "2021-07-28 16:57:54,767 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.925006, Tokens per Sec:    15665, Lr: 0.000300\n",
            "2021-07-28 16:58:10,709 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     3.960259, Tokens per Sec:    15288, Lr: 0.000300\n",
            "2021-07-28 16:58:26,716 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     3.953107, Tokens per Sec:    15453, Lr: 0.000300\n",
            "2021-07-28 16:58:42,729 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     3.973259, Tokens per Sec:    15140, Lr: 0.000300\n",
            "2021-07-28 16:58:58,866 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     4.405763, Tokens per Sec:    15363, Lr: 0.000300\n",
            "2021-07-28 16:59:45,427 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-28 16:59:45,428 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-28 16:59:45,862 - INFO - joeynmt.training - Example #0\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tSource:     ▁it ▁' s ▁a ▁very ▁big ▁signal ▁; ▁it ▁' s ▁sent ▁to ▁the ▁bra in ▁says ▁, ▁\" ▁go ▁and ▁e at ▁. ▁\"\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tReference:  ▁bu ▁çok ▁güçlü ▁bir ▁sin yal dir ▁. ▁bey ine ▁gider ▁ve ▁der ▁ki ▁, ▁\" ▁git ▁ve ▁ye ▁. ▁\"\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tHypothesis: ▁bu ▁, ▁bir ▁şey ▁, ▁\" ▁bu ▁, ▁\" ▁bir ▁şey ▁, ▁\" ▁d ed iğ im ▁gibi ▁, ▁\"\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - Example #1\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tSource:     ▁you ▁have ▁stop ▁sign als ▁- - ▁we ▁have ▁up ▁to ▁eight ▁stop ▁sign als ▁.\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tReference:  ▁ayrıca ▁dur ▁sin yal leri ▁de ▁vardır . ▁hemen ▁hemen ▁sekiz ▁tane ▁farklı ▁dur ▁sin yal imiz ▁var ▁.\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tHypothesis: ▁ 2 0 ▁' de ▁, ▁ 2 0 ▁' de ▁, ▁ 2 0 ▁' de ▁, ▁ 2 0 ▁' de ▁.\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - Example #2\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tSource:     ▁at ▁least ▁in ▁my ▁case ▁, ▁they ▁are ▁not ▁list ened ▁to ▁.\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - \tReference:  ▁ama ▁benim ▁g ib iler ▁bu ▁sin yal leri ▁pek ▁de ▁din lem iyor lar ▁.\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - \tHypothesis: ▁i ̇ n san lar ▁, ▁benim ▁için ▁, ▁benim ▁için ▁, ▁benim ▁için ▁çok ▁iyi ▁değil ▁.\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - Example #3\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - \tSource:     ▁( ▁la ugh ter ▁ ) ▁so ▁what ▁happ ens ▁if ▁the ▁big ▁bra in ▁in ▁the ▁integr ation ▁over r ides ▁the ▁signal ▁ ?\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - \tReference:  ▁( ▁gül üş meler ▁) ▁p eki ▁, ▁eğer ▁büyük ▁bey in ▁bu ▁gönder ilen ▁sin yal i ▁gör mez den ▁gelir se ▁ne ▁olur ▁ ?\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - \tHypothesis: ▁( ▁gül üş meler ▁) ▁bu ▁, ▁bu ▁, ▁bu ▁, ▁bey in iz in ▁en ▁az ından ▁daha ▁fazla ▁insan ın ▁nasıl ▁ ?\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     3000: bleu:   3.29, loss: 96139.1641, ppl:  46.6989, duration: 46.9973s\n",
            "2021-07-28 17:00:01,804 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     3.237124, Tokens per Sec:    14911, Lr: 0.000300\n",
            "2021-07-28 17:00:17,811 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     3.741183, Tokens per Sec:    15031, Lr: 0.000300\n",
            "2021-07-28 17:00:34,045 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     4.225142, Tokens per Sec:    15262, Lr: 0.000300\n",
            "2021-07-28 17:00:49,975 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     4.210491, Tokens per Sec:    15347, Lr: 0.000300\n",
            "2021-07-28 17:01:06,026 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     4.105839, Tokens per Sec:    15412, Lr: 0.000300\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
            "    main()\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 35, in main\n",
            "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 805, in train\n",
            "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 440, in train_and_validate\n",
            "    self.optimizer.step()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\", line 89, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\", line 119, in step\n",
            "    group['eps'])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\", line 92, in adam\n",
            "    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "MBoDS09JM807"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "! mkdir -p \"$gdrive_path/models/${src}${tgt}_transformer/\"\n",
        "! cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qNinL_DvYhI"
      },
      "source": [
        "# OR... If continuing from previous work, load models from google drive to notebook storage  \n",
        "! mkdir -p joeynmt/models/${src}${tgt}_transformer\n",
        "! cp -r \"$gdrive_path/models/${src}${tgt}_transformer\" joeynmt/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "n94wlrCjVc17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af45ae97-0cc7-4b8c-8474-6a02108ffbba"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 1000\tLoss: 4971087.00000\tPPL: 85.69424\tbleu: 2.36058\tLR: 0.00030000\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "66WhRE9lIhoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24c5e2f-99a4-4ac2-e3aa-c8a0f86679fe"
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 16:28:56,802 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-28 16:28:56,802 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-28 16:28:57,990 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-28 16:28:58,003 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-28 16:28:58,014 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-28 16:28:58,045 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
            "2021-07-28 16:29:01,412 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-28 16:29:01,642 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-28 16:29:01,712 - INFO - joeynmt.prediction - Decoding on dev set (data/entr/dev.bpe.tr)...\n",
            "2021-07-28 16:30:08,794 - INFO - joeynmt.prediction -  dev bleu[13a]:   0.97 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-28 16:30:08,795 - INFO - joeynmt.prediction - Decoding on test set (data/entr/test.bpe.tr)...\n",
            "2021-07-28 16:31:30,255 - INFO - joeynmt.prediction - test bleu[13a]:   0.75 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-RmEmRGD7Sb"
      },
      "source": [
        "# Fine-tuning to domain\n",
        "\n",
        "One important technique in neural machine translation is in-domain adaptation or fine-tuning. This introduces the model a certain domain we're interested to do translations in. \n",
        "\n",
        "One simple way of doing this is having a pre-trained model and continuing training from it on our in-domain training set. \n",
        "\n",
        "In this example we're going to fine-tune our model to news. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXNGfXzZEbS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1853ccfc-bb8d-48ad-c56a-41e52441b9cd"
      },
      "source": [
        "fine_corpus = \"WMT-News\" \n",
        "os.environ[\"fine\"] = fine_corpus\n",
        "\n",
        "# Downloading our corpus \n",
        "! opus_read -d $fine -s $src -t $tgt -wm moses -w $fine.$src $fine.$tgt -q\n",
        "\n",
        "# Extract the corpus file\n",
        "! gunzip ${fine}_latest_xml_$src-$tgt.xml.gz\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/WMT-News/latest/xml/en-tr.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "  92 KB https://object.pouta.csc.fi/OPUS-WMT-News/v2019/xml/en-tr.xml.gz\n",
            "  63 MB https://object.pouta.csc.fi/OPUS-WMT-News/v2019/xml/en.zip\n",
            "   2 MB https://object.pouta.csc.fi/OPUS-WMT-News/v2019/xml/tr.zip\n",
            "\n",
            "  66 MB Total size\n",
            "./WMT-News_latest_xml_en-tr.xml.gz ... 100% of 92 KB\n",
            "./WMT-News_latest_xml_en.zip ... 100% of 63 MB\n",
            "./WMT-News_latest_xml_tr.zip ... 100% of 2 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjVYnALNFcyP"
      },
      "source": [
        "# Read the corpus into python lists\n",
        "source_file = fine_corpus + '.' + source_language\n",
        "target_file = fine_corpus + '.' + target_language\n",
        "\n",
        "fine_src_all_bpe = [encode_bpe(sentence.strip(),'en') for sentence in open(source_file).readlines()]\n",
        "fine_tgt_all_bpe = [encode_bpe(sentence.strip(), 'tr') for sentence in open(target_file).readlines()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maxV-WTBFwbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f475eb09-2054-4c7e-8c63-f3483a4d3e66"
      },
      "source": [
        "# Let's take a peek at the files\n",
        "print(\"Source size:\", len(fine_src_all_bpe))\n",
        "print(\"Target size:\", len(fine_tgt_all_bpe))\n",
        "print(\"--------\")\n",
        "\n",
        "peek_size = 5\n",
        "for i in range(peek_size):\n",
        "  print(\"Sent #\", i)\n",
        "  print(\"SRC:\", decode_bpe(fine_src_all_bpe[i], 'en'))\n",
        "  print(\"TGT:\", decode_bpe(fine_tgt_all_bpe[i],'tr'))\n",
        "  print(\"---------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source size: 20016\n",
            "Target size: 20016\n",
            "--------\n",
            "Sent # 0\n",
            "SRC: two people drowned in floods in trabzon\n",
            "TGT: trabzon ' da sel iki kişiyi yuttu\n",
            "---------\n",
            "Sent # 1\n",
            "SRC: the ikisu creek overflowed on account of heavy rainfall in the district of yomra in trabzon .\n",
            "TGT: trabzon ’ un yomra ilçesinde etkili olan sağanak yağış nedeniyle i̇kisu deresi taştı .\n",
            "---------\n",
            "Sent # 2\n",
            "SRC: two women disappeared in the floodwaters in the village of tasdelen and the road to the village of sayvan was closed .\n",
            "TGT: taşdelen köyünde sele kapılan iki kadın kaybolurken , sayvan köyü yolu ulaşıma kapandı .\n",
            "---------\n",
            "Sent # 3\n",
            "SRC: the body of one of the women drowned in the floods was found .\n",
            "TGT: selde kayıp olan iki kadından birinin cesedine ulaşıldı .\n",
            "---------\n",
            "Sent # 4\n",
            "SRC: there was precipitation in the highlands of yomra at around 15 : 00 .\n",
            "TGT: yağış , yomra ’ nın yüksek kesiminde saat 15.00 sıralarında etkili oldu .\n",
            "---------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HcjsY5UF_Pk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30845b8-1a5c-4aa4-e435-4ee9af689b1c"
      },
      "source": [
        "# Allocate train, dev, test portions\n",
        "all_size = len(fine_src_all_bpe)\n",
        "dev_size = 500\n",
        "test_size = 500\n",
        "train_size = all_size - test_size - dev_size\n",
        "\n",
        "fine_src_train_bpe = fine_src_all_bpe[0:train_size]\n",
        "fine_tgt_train_bpe = fine_tgt_all_bpe[0:train_size]\n",
        "\n",
        "fine_src_dev_bpe = fine_src_all_bpe[train_size:train_size+dev_size]\n",
        "fine_tgt_dev_bpe = fine_tgt_all_bpe[train_size:train_size+dev_size]\n",
        "\n",
        "fine_src_test_bpe = fine_src_all_bpe[train_size+dev_size:all_size]\n",
        "fine_tgt_test_bpe = fine_tgt_all_bpe[train_size+dev_size:all_size]\n",
        "\n",
        "print(\"Set sizes\")\n",
        "print(\"All:\", len(fine_src_all_bpe))\n",
        "print(\"Train:\", len(fine_src_train_bpe))\n",
        "print(\"Dev:\", len(fine_src_dev_bpe))\n",
        "print(\"Test:\", len(fine_src_test_bpe))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Set sizes\n",
            "All: 20016\n",
            "Train: 19016\n",
            "Dev: 500\n",
            "Test: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvweRBGDGeXL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "88e89b04-0777-4639-a85e-68a7f24228ef"
      },
      "source": [
        "# Store sentences as files\n",
        "with open(\"finetrain.bpe.\"+source_language, \"w\") as src_file, open(\"finetrain.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(fine_src_train_bpe, fine_tgt_train_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"finedev.bpe.\"+source_language, \"w\") as src_file, open(\"finedev.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(fine_src_dev_bpe, fine_tgt_dev_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"finetest.bpe.\"+source_language, \"w\") as src_file, open(\"finetest.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(fine_src_test_bpe, fine_tgt_test_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-d15f6d523ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Store sentences as files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finetrain.bpe.\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msource_language\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finetrain.bpe.\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtarget_language\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtgt_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfine_src_train_bpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfine_tgt_train_bpe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msrc_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtgt_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fine_src_train_bpe' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbz1Hf-0G-59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4afdfa-9382-4bcf-f62a-04d524c8ff22"
      },
      "source": [
        "# If creating data for the first time, move all prepared data to the mounted location in google drive\n",
        "! mkdir -p \"$gdrive_path\"/data\n",
        "! cp finetrain.* \"$gdrive_path\"/data\n",
        "! cp finetest.* \"$gdrive_path\"/data\n",
        "! cp finedev.* \"$gdrive_path\"/data\n",
        "! ls \"$gdrive_path\"/data  #See the contents of the drive directory"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/My Drive/mt-workshop/en-tr-baseline/data’: File exists\n",
            "dev.bpe.en  finedev.bpe.en   finetrain.bpe.en  test.en\t     train.en\n",
            "dev.bpe.tr  finedev.bpe.tr   finetrain.bpe.tr  test.tr\t     train.tr\n",
            "dev.en\t    finetest.bpe.en  test.bpe.en       train.bpe.en\n",
            "dev.tr\t    finetest.bpe.tr  test.bpe.tr       train.bpe.tr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZP6K7qVT9n4"
      },
      "source": [
        "# OR... If continuing from previous run, load finetuning data from drive\n",
        "! cp \"$gdrive_path\"/data/finedev.* .\n",
        "! cp \"$gdrive_path\"/data/finetrain.* .\n",
        "! cp \"$gdrive_path\"/data/finetest.* .\n",
        "! cp \"$gdrive_path\"/data/finetest.* ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mjN3UUoHJN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f60f15f-3bb2-40fd-d060-2300f92ad697"
      },
      "source": [
        "# #Move everything important under joeynmt directory\n",
        "os.environ[\"data_path\"] = os.path.join(\"joeynmt\", \"data\", source_language + target_language)\n",
        "\n",
        "# Move fine-tuning data to data directory\n",
        "! mkdir -p $data_path\n",
        "! cp finetrain.* $data_path\n",
        "! cp finetest.* $data_path\n",
        "! cp finedev.* $data_path\n",
        "! ls $data_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finedev.bpe.en\tfinetest.bpe.en  finetrain.bpe.en\n",
            "finedev.bpe.tr\tfinetest.bpe.tr  finetrain.bpe.tr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dlh1pfvr4qw"
      },
      "source": [
        "# Also, load models from google drive to notebook storage  \n",
        "! mkdir -p joeynmt/models/${src}${tgt}_transformer\n",
        "! cp -r \"$gdrive_path/models/${src}${tgt}_transformer\" joeynmt/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mdL95STIzPK"
      },
      "source": [
        "# Let's create a config file for finetuning training\n",
        "# Changes from previous config are dataset names, model name, batch size and learning rate\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/finetrain.bpe\"\n",
        "    dev:   \"data/{name}/finedev.bpe\"\n",
        "    test:  \"data/{name}/finetest.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    load_model: \"{gdrive_path}/models/{name}_transformer/best.ckpt\" # Load base model from its best scoring checkpoint (from gdrive)\n",
        "    #load_model: \"models/{name}_transformer/best.ckpt\" # Load base model from its best scoring checkpoint (from local)\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0001\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 1028\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}_finetune.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU2ehibpJdrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2769748-9456-4919-9fc7-3081d3f97347"
      },
      "source": [
        "# Test our model on our domain before fine-tuning\n",
        "! cd joeynmt; python3 -m joeynmt test \"configs/transformer_${src}${tgt}_finetune.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-29 09:54:48,631 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-29 09:54:48,631 - INFO - joeynmt.data - Building vocabulary...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
            "    main()\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 38, in main\n",
            "    output_path=args.output_path, save_attention=args.save_attention)\n",
            "  File \"/content/joeynmt/joeynmt/prediction.py\", line 293, in test\n",
            "    data_cfg=cfg[\"data\"], datasets=[\"dev\", \"test\"])\n",
            "  File \"/content/joeynmt/joeynmt/data.py\", line 112, in load_data\n",
            "    dataset=train_data, vocab_file=src_vocab_file)\n",
            "  File \"/content/joeynmt/joeynmt/vocabulary.py\", line 161, in build_vocab\n",
            "    vocab = Vocabulary(file=vocab_file)\n",
            "  File \"/content/joeynmt/joeynmt/vocabulary.py\", line 40, in __init__\n",
            "    self._from_file(file)\n",
            "  File \"/content/joeynmt/joeynmt/vocabulary.py\", line 61, in _from_file\n",
            "    with open(file, \"r\", encoding='utf-8') as open_file:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'data/entr/vocab.txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-ir9d-LKez3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cb76d07-b083-4bb8-96fa-7888b8dcb796"
      },
      "source": [
        "# Train to our domain\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "! cd joeynmt; python3 -m joeynmt train \"configs/transformer_${src}${tgt}_finetune.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 17:35:43,552 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-28 17:35:43,585 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-28 17:35:52,185 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-28 17:35:53,413 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-28 17:35:53,459 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-28 17:35:53,469 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-28 17:35:53,469 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-28 17:35:53,695 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-28 17:35:53.930981: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-28 17:35:54,906 - INFO - joeynmt.training - Total params: 13372928\n",
            "2021-07-28 17:35:58,161 - INFO - joeynmt.training - Loading model from /content/drive/My Drive/mt-workshop/en-tr-baseline/models/entr_transformer/best.ckpt\n",
            "2021-07-28 17:35:58,607 - INFO - joeynmt.helpers - cfg.name                           : entr_transformer_finetune\n",
            "2021-07-28 17:35:58,607 - INFO - joeynmt.helpers - cfg.data.src                       : en\n",
            "2021-07-28 17:35:58,607 - INFO - joeynmt.helpers - cfg.data.trg                       : tr\n",
            "2021-07-28 17:35:58,607 - INFO - joeynmt.helpers - cfg.data.train                     : data/entr/finetrain.bpe\n",
            "2021-07-28 17:35:58,607 - INFO - joeynmt.helpers - cfg.data.dev                       : data/entr/finedev.bpe\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.test                      : data/entr/finetest.bpe\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : data/entr/vocab.txt\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : data/entr/vocab.txt\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/drive/My Drive/mt-workshop/en-tr-baseline/models/entr_transformer/best.ckpt\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0001\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.batch_size            : 1028\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1000\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/entr_transformer\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 370216,\n",
            "\tvalid 1000,\n",
            "\ttest 1000\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] ▁than k ▁you ▁so ▁much ▁, ▁ch ris ▁.\n",
            "\t[TRG] ▁çok ▁teş ek kür ▁eder im ▁chris ▁.\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ▁. (5) ▁, (6) ▁the (7) ▁' (8) ▁and (9) ▁\n",
            "2021-07-28 17:35:58,612 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ▁. (5) ▁, (6) ▁the (7) ▁' (8) ▁and (9) ▁\n",
            "2021-07-28 17:35:58,612 - INFO - joeynmt.helpers - Number of Src words (types): 9034\n",
            "2021-07-28 17:35:58,612 - INFO - joeynmt.helpers - Number of Trg words (types): 9034\n",
            "2021-07-28 17:35:58,612 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=9034),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=9034))\n",
            "2021-07-28 17:35:58,619 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 1028\n",
            "\ttotal batch size (w. parallel & accumulation): 1028\n",
            "2021-07-28 17:35:58,620 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-28 17:36:05,528 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     5.201994, Tokens per Sec:     9767, Lr: 0.000300\n",
            "2021-07-28 17:36:11,767 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.912281, Tokens per Sec:    10804, Lr: 0.000300\n",
            "2021-07-28 17:36:18,021 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     4.973733, Tokens per Sec:    11041, Lr: 0.000300\n",
            "2021-07-28 17:36:24,255 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.809206, Tokens per Sec:    10934, Lr: 0.000300\n",
            "2021-07-28 17:36:30,681 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     4.819091, Tokens per Sec:    10656, Lr: 0.000300\n",
            "2021-07-28 17:36:37,252 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     4.839234, Tokens per Sec:    10245, Lr: 0.000300\n",
            "2021-07-28 17:36:43,827 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     3.323047, Tokens per Sec:    10317, Lr: 0.000300\n",
            "2021-07-28 17:36:50,434 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     4.597960, Tokens per Sec:    10237, Lr: 0.000300\n",
            "2021-07-28 17:36:56,883 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     4.473081, Tokens per Sec:    10245, Lr: 0.000300\n",
            "2021-07-28 17:37:03,243 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     4.721494, Tokens per Sec:    10846, Lr: 0.000300\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
            "    main()\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 35, in main\n",
            "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 805, in train\n",
            "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 475, in train_and_validate\n",
            "    valid_duration = self._validate(valid_data, epoch_no)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 563, in _validate\n",
            "    n_gpu=self.n_gpu\n",
            "  File \"/content/joeynmt/joeynmt/prediction.py\", line 125, in validate_on_data\n",
            "    n_best=n_best)\n",
            "  File \"/content/joeynmt/joeynmt/search.py\", line 444, in run_batch\n",
            "    encoder_hidden=encoder_hidden)\n",
            "  File \"/content/joeynmt/joeynmt/search.py\", line 39, in greedy\n",
            "    src_mask, max_output_length, model, encoder_output, encoder_hidden)\n",
            "  File \"/content/joeynmt/joeynmt/search.py\", line 146, in transformer_greedy\n",
            "    trg_mask=trg_mask\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/model.py\", line 104, in forward\n",
            "    outputs, hidden, att_probs, att_vectors = self._decode(**kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/model.py\", line 178, in _decode\n",
            "    **_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/decoders.py\", line 540, in forward\n",
            "    src_mask=src_mask, trg_mask=trg_mask)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/transformer_layers.py\", line 264, in forward\n",
            "    h1 = self.trg_trg_att(x_norm, x_norm, x_norm, mask=trg_mask)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/transformer_layers.py\", line 73, in forward\n",
            "    scores = scores.masked_fill(~mask.unsqueeze(1), float('-inf'))\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFYE2yj1N-di"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "! mkdir -p \"$gdrive_path/models/${src}${tgt}_transformer_finetune/\"\n",
        "! cp -r joeynmt/models/${src}${tgt}_transformer_finetune/* \"$gdrive_path/models/${src}${tgt}_transformer_finetune/\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd1_yOXANu3-"
      },
      "source": [
        "# Test again to see how our model improved\n",
        "#! cd joeynmt; python3 -m joeynmt test \"configs/transformer_${src}${tgt}_finetune.yaml\"\n",
        "\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer_finetune/config.yaml\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}