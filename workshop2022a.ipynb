{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Machine translation workshop",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Machine Translation for Translators Workshop\n",
        "\n",
        "In this notebook, we'll learn:\n",
        "\n",
        "- Running a pre-trained English-Turkish MT model \n",
        "- Byte-pair encoding (BPE)\n",
        "- Translating a document\n",
        "- Creating training data from translation memory (TMX)\n",
        "- Creating a test set and calculating BLEU\n",
        "- Training a model from scratch\n",
        "- Domain adaptation on our pre-trained model\n",
        "\n",
        "NOTE: This coding template is partially based on Masakhane's starter notebook (https://github.com/masakhane-io/masakhane-mt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Machine translation with JoeyNMT and AfroTranslate\n",
        "\n",
        "For this workshop we will use üê® JoeyNMT. \n",
        "\n",
        "JoeyNMT is an open-source, minimalist neural machine translation toolkit for educational purposes. [code](https://github.com/joeynmt/joeynmt), [documentation](https://joeynmt.readthedocs.io/en/latest/).\n",
        "\n",
        "We will also use a Python package called [AfroTranslate](https://github.com/hgilles06/AfroTranslate) to easily interact with JoeyNMT.\n",
        "\n",
        "Let's start by installing AfroTranslate. Since it's dependent on JoeyNMT, it'll automatically install it for us.\n",
        "\n"
      ],
      "metadata": {
        "id": "k-xY0ng0Qx-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install AfroTranslate"
      ],
      "metadata": {
        "id": "tqxIcN9FQj4q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73f9ccc3-3fac-4abc-8349-c7cda5ead7de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting AfroTranslate\n",
            "  Downloading AfroTranslate-0.0.6-py3-none-any.whl (12 kB)\n",
            "Collecting joeynmt==1.3\n",
            "  Downloading joeynmt-1.3-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 84 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting spacy==3.2.1\n",
            "  Downloading spacy-3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.0 MB 19.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader==0.4 in /usr/local/lib/python3.7/dist-packages (from AfroTranslate) (0.4)\n",
            "Requirement already satisfied: wrapt==1.11.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (1.11.1)\n",
            "Collecting six==1.12\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (7.1.2)\n",
            "Collecting torch==1.8.0\n",
            "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 735.5 MB 14 kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (0.11.2)\n",
            "Collecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.1 MB 23.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: subword-nmt in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (0.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (0.16.0)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (2.7.0)\n",
            "Requirement already satisfied: sacrebleu>=1.3.6 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (2.0.0)\n",
            "Collecting numpy==1.20.1\n",
            "  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15.3 MB 192 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pylint in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (2.12.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (6.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3->AfroTranslate) (57.4.0)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 451 kB 33.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (21.3)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10.1 MB 45.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (1.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (2.23.0)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 628 kB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (3.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (3.10.0.2)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (0.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (2.11.3)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181 kB 50.9 MB/s \n",
            "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (4.62.3)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.1->AfroTranslate) (2.0.6)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy==3.2.1->AfroTranslate) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.2.1->AfroTranslate) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy==3.2.1->AfroTranslate) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1->AfroTranslate) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1->AfroTranslate) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1->AfroTranslate) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1->AfroTranslate) (2.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt==1.3->AfroTranslate) (0.4.4)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt==1.3->AfroTranslate) (0.8.9)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt==1.3->AfroTranslate) (2.3.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt==1.3->AfroTranslate) (2019.12.20)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.43.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (4.10.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3->AfroTranslate) (3.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy==3.2.1->AfroTranslate) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.2.1->AfroTranslate) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3->AfroTranslate) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3->AfroTranslate) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3->AfroTranslate) (0.11.0)\n",
            "Requirement already satisfied: mccabe<0.7,>=0.6 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3->AfroTranslate) (0.6.1)\n",
            "Requirement already satisfied: isort<6,>=4.2.5 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3->AfroTranslate) (5.10.1)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3->AfroTranslate) (2.4.1)\n",
            "Requirement already satisfied: toml>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3->AfroTranslate) (0.10.2)\n",
            "Requirement already satisfied: astroid<2.10,>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3->AfroTranslate) (2.9.3)\n",
            "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.10,>=2.9.0->pylint->joeynmt==1.3->AfroTranslate) (1.7.1)\n",
            "Requirement already satisfied: typed-ast<2.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.10,>=2.9.0->pylint->joeynmt==1.3->AfroTranslate) (1.5.2)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3->AfroTranslate) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3->AfroTranslate) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3->AfroTranslate) (2018.9)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.7/dist-packages (from subword-nmt->joeynmt==1.3->AfroTranslate) (4.0.3)\n",
            "Installing collected packages: six, numpy, catalogue, typer, torch, srsly, pydantic, torchtext, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy, joeynmt, AfroTranslate\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Attempting uninstall: joeynmt\n",
            "    Found existing installation: joeynmt 1.5.1\n",
            "    Uninstalling joeynmt-1.5.1:\n",
            "      Successfully uninstalled joeynmt-1.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.20.1 which is incompatible.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.8.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-python-client 1.12.10 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed AfroTranslate-0.0.6 catalogue-2.0.6 joeynmt-1.3 langcodes-3.3.0 numpy-1.20.1 pathy-0.6.1 pydantic-1.8.2 six-1.12.0 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 torch-1.8.0 torchtext-0.9.0 typer-0.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AfroTranslate comes with direct links to many African languages ([list](https://github.com/masakhane-io/masakhane-mt/tree/master/benchmarks)). \n",
        "\n",
        "Let's use it to load its Tigrinya model and use it."
      ],
      "metadata": {
        "id": "_L0NWncARvl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import translator\n",
        "from afrotranslate import MasakhaneTranslate"
      ],
      "metadata": {
        "id": "r83MXvgZ4prs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create translator object\n",
        "translator = MasakhaneTranslate(model_name=\"en-ti\")\n",
        "\n",
        "#translate \n",
        "translator.translate(\"I love you so much!\")"
      ],
      "metadata": {
        "id": "TE2EArLXSWhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also load a model of our own.\n",
        "\n",
        "Let's first download the model stored in the cloud.\n",
        "\n",
        "[Direct link](https://drive.google.com/file/d/1E_bdkdnYW4wTSdujsDiCqBlBYngvjX0m/view?usp=sharing)"
      ],
      "metadata": {
        "id": "G4Typ_EGYGaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute this to download directly to Colab\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id=\"1E_bdkdnYW4wTSdujsDiCqBlBYngvjX0m\",\n",
        "    \t\t                            dest_path=\"models/entr/entr.zip\",\n",
        "    \t\t                            unzip=True)"
      ],
      "metadata": {
        "id": "y_XH3nSmYM1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's load our English-Turkish translator model using AfroTranslate"
      ],
      "metadata": {
        "id": "DfTj0kRsVWvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cTzqhVbKVgRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2cd8TXgSVwS3",
        "outputId": "455178e5-234f-4170-9d45-7f2e1f7550d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'‚ñÅ ut an √ß ‚ñÅ.'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AThMQfxCaRg2",
        "outputId": "2d3e74e3-610c-4768-b913-3a7be0087c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'‚ñÅ ut an √ß ‚ñÅ.'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results seem to be a bit strange. It's because the models were trained with BPE encoded words. "
      ],
      "metadata": {
        "id": "bjn4LYzZcXtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Byte-pair encoding (BPE)\n",
        "\n",
        "- One of the most powerful improvements for neural machine translation is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- BPE tokenization limits the number of vocabulary into a certain size by smartly dividing words into subwords\n",
        "\n",
        "- This is especially useful for agglutinative languages (like Turkish) where vocabulary is effectively endless. \n",
        "\n",
        "- Below you have the scripts for doing BPE tokenization of our data. We use bpemb library that has pre-trained BPE models to convert our text into subwords.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vMdOfnCOaUgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install bpemb"
      ],
      "metadata": {
        "id": "3GFu_jc5b34b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bpemb import BPEmb\n",
        "\n",
        "BPE_VOCAB_SIZE = 5000\n",
        "bpemb_en = BPEmb(lang=\"en\", vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=True)\n",
        "bpemb_tr = BPEmb(lang=\"tr\", vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=True)"
      ],
      "metadata": {
        "id": "r0ZErPmDbuVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use it now and see how it looks like"
      ],
      "metadata": {
        "id": "Ne7uYQ0_cIuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PhBPgoEwcIgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "slwFSJ85cOTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's go back using our model with properly encoded strings.\n",
        "\n",
        "Don't forget that `bpemb.encode` outputs a list but we need to input a string to our translator.\n",
        "\n",
        "You can use `' '.join(list)` to convert a list to string."
      ],
      "metadata": {
        "id": "Gpvp4BmhcVTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QC59yVXFc4nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w9d05981_2SH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zkClamJ9k9Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And to make that output readable, we need to decode it using the right BPE model"
      ],
      "metadata": {
        "id": "BTAGGdVrdt6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hQ0HmSuQdyrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a translator function that'll reduce our work..."
      ],
      "metadata": {
        "id": "rwMtdgVCm7Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_entr(tr_in):\n",
        "  #..."
      ],
      "metadata": {
        "id": "msoiIxUYkagg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_entr(\"Hello! What's up?\")"
      ],
      "metadata": {
        "id": "l3x1V1gdlyR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 (Challenge) - Translating a document\n",
        "\n",
        "For this exercise, we're going to create a script that translates a word document automatically. \n",
        "\n",
        "We're going to reuse some of the code from last session."
      ],
      "metadata": {
        "id": "utP_op_9ekwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "fpWFnsoZf0S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download our document to translate.\n",
        "\n",
        "[Direct link](https://docs.google.com/document/d/1jwNdh4n_m4M-0Z4j23lH4xyeWfTyHPY-/edit?usp=sharing&ouid=114670265863192986077&rtpof=true&sd=true)"
      ],
      "metadata": {
        "id": "Ho5hJMT5gjHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute this to download directly to Colab\n",
        "gdd.download_file_from_google_drive(file_id=\"1jwNdh4n_m4M-0Z4j23lH4xyeWfTyHPY-\", dest_path=\"/content/MT.docx\")"
      ],
      "metadata": {
        "id": "I6_TtPMWgFlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import docx and use it to read our document"
      ],
      "metadata": {
        "id": "h50CETebfHGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3Ui1asSqgFc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bj_F7IIMhLov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what's inside"
      ],
      "metadata": {
        "id": "MtbXSjQifLoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1PaC1Qj9hNU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's translate paragraphs one by one and put the translations into another list\n",
        "\n"
      ],
      "metadata": {
        "id": "13ZqOe07iw22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zipTAjg9ivyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "E2YWXgytjfsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, write the translated paragraphs into a newly created document."
      ],
      "metadata": {
        "id": "AREK4gqmfXzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wCfQVVv2niZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4 - Creating parallel data from TMX\n",
        "\n",
        "Sometimes, we would like to adapt a model into our style or a domain that we translate in. If we have done plenty of translations already, we could use our translation memory to later enhance a model. For that we need to convert our translation memory to a classic parallel data format. \n",
        "\n",
        "The classic parallel data format is two files, each of them containing the sentences at each line in different languages. \n",
        "\n",
        "A translation memory contains already parallel data in this sense. Although, it is not in the format that we want. \n",
        "\n",
        "We can automate this conversion using Python scripting. \n",
        "\n",
        "Since TMX parsing is a bit complicated for our level, we're going to use this code by [Yasmin Moslem](https://github.com/ymoslem/file-converters/blob/main/TMX2MT/TMX2MT-ElementTree.py)."
      ],
      "metadata": {
        "id": "eFh05S37o_Cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml\n",
        "import xml.etree.ElementTree as ET\n",
        "import sys\n",
        "import re\n",
        "import os\n",
        "\n",
        "def xml_to_parallel(file, source, target):\n",
        "  source_file = os.path.splitext(file)[0] + \".\" + source\n",
        "  target_file = os.path.splitext(file)[0] + \".\" + target\n",
        "\n",
        "  tree = ET.parse(file)  \n",
        "  root = tree.getroot()\n",
        "\n",
        "  langs = []\n",
        "\n",
        "  for tu in root.iter('tu'):\n",
        "      for tuv in tu.iter('tuv'):\n",
        "          lang = list(tuv.attrib.values())\n",
        "          langs.append(lang[0].lower())\n",
        "\n",
        "  langs = set(langs)\n",
        "\n",
        "  if source in langs and target in langs:\n",
        "      with open(source_file, \"w+\", encoding='utf-8') as source_file, open(target_file, \"w+\", encoding='utf-8') as target_file:\n",
        "          for tu in root.iter('tu'):\n",
        "              for tuv in tu.iter('tuv'):\n",
        "                  lang = list(tuv.attrib.values())\n",
        "                  #print(lang[0])\n",
        "                  if lang[0].lower() == source.lower():\n",
        "                      for seg in tuv.iter('seg'):\n",
        "                          source_text = ET.tostring(seg, 'utf-8', method=\"xml\")\n",
        "                          source_text = source_text.decode(\"utf-8\")\n",
        "                          source_text = re.sub('<.*?>|&lt;.*?&gt;|&?(amp|nbsp|quot);|{}', ' ', source_text)\n",
        "                          source_text = re.sub(r'[ ]{2,}', ' ', source_text).strip()\n",
        "                          source_file.write(str(source_text) + \"\\n\")\n",
        "                          #print(source_text)\n",
        "                  elif lang[0].lower() == target.lower():\n",
        "                      for seg in tuv.iter('seg'):\n",
        "                          target_text = ET.tostring(seg, 'utf-8', method=\"xml\")\n",
        "                          target_text = target_text.decode(\"utf-8\")\n",
        "                          target_text = re.sub('<.*?>|&lt;.*?&gt;|&quot;|&apos;|{}', ' ', target_text)\n",
        "                          target_text = re.sub(r'[ ]{2,}', ' ', target_text).strip()\n",
        "                          target_file.write(str(target_text) + \"\\n\")\n",
        "                          #print(target_text)"
      ],
      "metadata": {
        "id": "KFjWrzT6pSfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have a translation memory that contains News translations."
      ],
      "metadata": {
        "id": "hFsaWt3PqOFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdd.download_file_from_google_drive(file_id=\"1xh6k91VFfiWpUnjrs-9mU_tkRkdS-INm\", dest_path=\"/content/news.en-tr.tmx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hywRVrX6qf7C",
        "outputId": "bc019595-db7f-4295-eb08-c3f488029858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 1xh6k91VFfiWpUnjrs-9mU_tkRkdS-INm into /content/news.en-tr.tmx... Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We just need to call our function now.\n",
        "\n"
      ],
      "metadata": {
        "id": "9GAitwTSr0B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z1JhQUr_qphF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can download and check the parallel data that we created.\n",
        "\n",
        "(You can see them in the files panel once you hit refresh)"
      ],
      "metadata": {
        "id": "d3FXTYl5rcRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5 - Creating a test set and calculating BLEU\n",
        "\n",
        "In this part, we'll see how our model performs on a test set we create from our in-domain data. \n",
        "\n",
        "Usually, we don't want use all our in-domain data for training. We allocate a portion of it for testing purposes and we make sure that we don't mix this in the training data. Because if we do, it will be sort-of cheating and the results we get won't reflect the generalized quality of the model.\n",
        "\n",
        "Let's see how big is our data first:"
      ],
      "metadata": {
        "id": "o0DE_xMbrhB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute this to see the size of news dataset\n",
        "#NOTE: This is not Python! \n",
        "!wc news.en-tr.en news.en-tr.tr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yELXCDMugfmO",
        "outputId": "ffb51300-60d4-4a6a-d493-5e9efceff2ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  10007  197808 1180320 news.en-tr.tmx.en\n",
            "  10007  149199 1255282 news.en-tr.tmx.tr\n",
            "  20014  347007 2435602 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's take a portion of it, say the final 200 samples as testing data, and the rest as training data."
      ],
      "metadata": {
        "id": "pAUVcrWXhEdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n -200 news.en-tr.en > news.en-tr.train.en \n",
        "!tail -n 200 news.en-tr.en > news.en-tr.test.en \n",
        "\n",
        "!head -n -200 news.en-tr.en > news.en-tr.train.tr \n",
        "!tail -n 200 news.en-tr.en > news.en-tr.test.tr "
      ],
      "metadata": {
        "id": "41N-3DswhdfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc news.en-tr.test.en news.en-tr.test.tr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkr7k2JUiBAm",
        "outputId": "b01c827c-0b0a-4a78-daea-602849885641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   9987  197548 1178714 news.en-tr.train.en\n",
            "   9987  149030 1253863 news.en-tr.train.tr\n",
            "  19974  346578 2432577 total\n",
            "  20  260 1606 news.en-tr.test.en\n",
            "  20  169 1419 news.en-tr.test.tr\n",
            "  40  429 3025 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, now we have a separate training and testing set. Let's see how our generic English-Turkish model performs on it.\n",
        "\n",
        "We'll first translate the English portion of our test set using our model."
      ],
      "metadata": {
        "id": "JY3QPWqMi7pS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#You can use this function to read a file line by line into a list\n",
        "def read_file_lines_to_list(filename):\n",
        "  return [l[:-1] for l in open(filename, 'r').readlines()]\n",
        "\n",
        "#You can use this function to write a list of strings into a file\n",
        "def write_list_to_file(strlist, filename):\n",
        "  with open(filename, 'w') as f:\n",
        "    for s in strlist:\n",
        "      f.write(s+\"\\n\")\n",
        "      \n",
        "\n",
        "#You can use this function to see first n elements in a list\n",
        "def get_first_n(l, n):\n",
        "  for i, elem in enumerate(l):\n",
        "    print(elem)\n",
        "    if i == n:\n",
        "      break"
      ],
      "metadata": {
        "id": "sSLuFu5Yi7Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7P3uPK_Aj4DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the same code we used before to translate the sentences in our list"
      ],
      "metadata": {
        "id": "FWtFz5T2kfEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cz7Tbj_8kksT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the translations look like"
      ],
      "metadata": {
        "id": "fRDTHCTzl9Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_yQXULL1l5cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's write the translations to a textfile."
      ],
      "metadata": {
        "id": "EffswOIdn62d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1UKNZXSDn_h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For doing BLEU evaluations, we can use the SacreBLEU package"
      ],
      "metadata": {
        "id": "yB2yMe2InBa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-88G48FDnOb0",
        "outputId": "455ab2e9-2cbf-4417-c386-4a2d7358bf3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.9)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2019.12.20)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2.3.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.4.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.20.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sacrebleu ______ -i ________"
      ],
      "metadata": {
        "id": "XV1B0T_rnvc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 7 - Fine-tuning our model\n"
      ],
      "metadata": {
        "id": "w95KvrhTqlW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a discrepancy between AfroTranslate and JoeyNMT. \n",
        "\n",
        "We need to restart our runtime at this step and install JoeyNMT and bpemb again. \n",
        "\n",
        "Don't worry, the files we have prepared will stay in their places. "
      ],
      "metadata": {
        "id": "yiwcjrJV5_V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "metadata": {
        "id": "3HsYgpZf5-kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bpemb import BPEmb\n",
        "\n",
        "BPE_VOCAB_SIZE = 5000\n",
        "bpemb_en = BPEmb(lang=\"en\", vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=True)\n",
        "bpemb_tr = BPEmb(lang=\"tr\", vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=True)"
      ],
      "metadata": {
        "id": "MyEFTPox6r99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BPE encode the in-domain training data\n",
        "news_en_bpe = [' '.join(bpemb_en.encode(l[:-1])) for l in open('news.en-tr.en', 'r').readlines()]\n",
        "news_tr_bpe = [' '.join(bpemb_tr.encode(l[:-1])) for l in open('news.en-tr.tr', 'r').readlines()]\n",
        "\n",
        "#Allocate first 1000 samples as development set\n",
        "news_dev_en_bpe = news_en_bpe[0:1000]\n",
        "news_dev_tr_bpe = news_tr_bpe[0:1000]\n",
        "\n",
        "#Allocate last 200 samples as test set\n",
        "news_test_en_bpe = news_en_bpe[-200:]\n",
        "news_test_tr_bpe = news_tr_bpe[-200:]\n",
        "\n",
        "#Allocate rest as training data\n",
        "news_train_en_bpe = news_en_bpe[1000:-200]\n",
        "news_train_tr_bpe = news_tr_bpe[1000:-200]"
      ],
      "metadata": {
        "id": "LXxiFiZLq1zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_list_to_file(news_train_en_bpe, \"news.en-tr.train.BPE.en\")\n",
        "write_list_to_file(news_train_tr_bpe, \"news.en-tr.train.BPE.tr\")\n",
        "write_list_to_file(news_dev_en_bpe, \"news.en-tr.dev.BPE.en\")\n",
        "write_list_to_file(news_dev_tr_bpe, \"news.en-tr.dev.BPE.tr\")\n",
        "write_list_to_file(news_test_en_bpe, \"news.en-tr.test.BPE.en\")\n",
        "write_list_to_file(news_test_tr_bpe, \"news.en-tr.test.BPE.tr\")"
      ],
      "metadata": {
        "id": "i50AAey2sb-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a JoeyNMT config file for finetuning training\n",
        "# Changes from previous config are dataset names, model name, batch size and learning rate\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"entr_finetune\"\n",
        "\n",
        "data:\n",
        "    src: \"en\"\n",
        "    trg: \"tr\"\n",
        "    train: \"/content/news.en-tr.train.BPE\"\n",
        "    dev:   \"/content/news.en-tr.dev.BPE\"\n",
        "    test:  \"/content/news.en-tr.test.BPE\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 150\n",
        "    src_vocab: \"/content/models/entr/src_vocab.txt\"\n",
        "    trg_vocab: \"/content/models/entr/trg_vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    load_model: \"/content/models/entr/best.ckpt\" # Load base model from its best scoring checkpoint (from local)\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0001\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 128\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 64\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 100          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"/content/models/entr_finetune\"\n",
        "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_best_ckpts: 3\n",
        "    save_latest_ckpt: True\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\"\n",
        "with open(\"/content/entr_finetune.yaml\",'w') as f:\n",
        "    f.write(config)"
      ],
      "metadata": {
        "id": "s_0Ukw5sqscX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cd joeynmt; python3 -m joeynmt train \"/content/entr_finetune.yaml\""
      ],
      "metadata": {
        "id": "ElUNLotpwEFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c3794e9-65d6-418a-be62-df478b3b74b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-28 08:46:42,471 - INFO - root - Hello! This is Joey-NMT (version 1.5.1).\n",
            "2022-01-28 08:46:42,497 - INFO - joeynmt.data - Loading training data...\n",
            "2022-01-28 08:46:42,681 - INFO - joeynmt.data - Building vocabulary...\n",
            "2022-01-28 08:46:44,037 - INFO - joeynmt.data - Loading dev data...\n",
            "2022-01-28 08:46:44,055 - INFO - joeynmt.data - Loading test data...\n",
            "2022-01-28 08:46:44,058 - INFO - joeynmt.data - Data loaded.\n",
            "2022-01-28 08:46:44,058 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2022-01-28 08:46:44,344 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2022-01-28 08:46:46,757 - INFO - joeynmt.training - Total params: 13372928\n",
            "2022-01-28 08:46:49,639 - INFO - joeynmt.training - Loading model from /content/models/entr/best.ckpt\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                           cfg.name : entr_transformer\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                       cfg.data.src : en\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                       cfg.data.trg : tr\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                     cfg.data.train : /content/news.en-tr.train.BPE\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                       cfg.data.dev : /content/news.en-tr.dev.BPE\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                      cfg.data.test : /content/news.en-tr.test.BPE\n",
            "2022-01-28 08:46:49,951 - INFO - joeynmt.helpers -                     cfg.data.level : bpe\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -                 cfg.data.lowercase : False\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -           cfg.data.max_sent_length : 100\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -                 cfg.data.src_vocab : /content/models/entr/src_vocab.txt\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -                 cfg.data.trg_vocab : /content/models/entr/trg_vocab.txt\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -            cfg.training.load_model : /content/models/entr/best.ckpt\n",
            "2022-01-28 08:46:49,952 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -              cfg.training.patience : 5\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -  cfg.training.learning_rate_factor : 0.5\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 1000\n",
            "2022-01-28 08:46:49,953 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.1\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -            cfg.training.batch_size : 1028\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -            cfg.training.batch_type : token\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 3600\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token\n",
            "2022-01-28 08:46:49,954 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 1\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -                cfg.training.epochs : 30\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 200\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/entr_finetune\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -             cfg.training.overwrite : True\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -               cfg.training.shuffle : True\n",
            "2022-01-28 08:46:49,955 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 3\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0\n",
            "2022-01-28 08:46:49,956 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 4\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 1024\n",
            "2022-01-28 08:46:49,957 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.3\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 4\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 1024\n",
            "2022-01-28 08:46:49,958 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.3\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 8747,\n",
            "\tvalid 1000,\n",
            "\ttest 200\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] ‚ñÅin ‚ñÅ iz mir ‚ñÅin ‚ñÅoctober ‚ñÅlast ‚ñÅyear , ‚ñÅfat ma ‚ñÅ ≈ü ah in , ‚ñÅminister ‚ñÅof ‚ñÅwomen ‚ñÅand ‚ñÅsocial ‚ñÅw elf are , ‚ñÅple aded ‚ñÅfor ‚ñÅr ig or ous ‚ñÅlife ‚ñÅimp ris onment ‚ñÅat ‚ñÅthe ‚ñÅtrial ‚ñÅof ‚ñÅs . √ß . ‚ñÅand ‚ñÅ0 ‚ñÅrel atives ‚ñÅwho ‚ñÅwere ‚ñÅalle ged ‚ñÅto ‚ñÅhave ‚ñÅst ab bed ‚ñÅhis ‚ñÅwife ‚ñÅto ‚ñÅdeath ‚ñÅbecause ‚ñÅshe ‚ñÅwanted ‚ñÅa ‚ñÅdiv or ce .\n",
            "\t[TRG] ‚ñÅ- ‚ñÅi Ãá z mir ‚Äô de ‚ñÅge√ßen ‚ñÅyƒ±lƒ±n ‚ñÅekim ‚ñÅayƒ±nda , ‚ñÅbo≈ü an mak ‚ñÅ√ºzere ‚ñÅolduƒüu ‚ñÅe≈ü ini ‚ñÅb ƒ± √ßak layarak ‚ñÅ√∂ld√ºr d√ºƒü√º ‚ñÅiddi asƒ±yla , ‚ñÅaƒüƒ±r la≈ütƒ±r ƒ±lmƒ±≈ü ‚ñÅm√º eb bet ‚ñÅhapis ‚ñÅcez asƒ± ‚ñÅistem iyle ‚ñÅhakkƒ±nda ‚ñÅdav a ‚ñÅa√ß ƒ±lan ‚ñÅs . √ß . ‚ñÅve ‚ñÅ0 ‚ñÅyak ƒ±nƒ±n ƒ±n ‚ñÅyarg ƒ±lan dƒ±ƒüƒ± ‚ñÅdav aya ‚ñÅda ‚ñÅaile ‚ñÅve ‚ñÅsosyal ‚ñÅpolitik alar ‚ñÅbakanƒ± ‚ñÅfat ma ‚ñÅ≈üah in , ‚ñÅhukuk ‚ñÅm √º≈ü av iri ‚ñÅbir sel ‚ñÅkurt ‚ñÅaracƒ±lƒ±ƒüƒ± ‚ñÅile ‚ñÅdav aya ‚ñÅm√º da h il ‚ñÅolmak ‚ñÅist edi .\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ‚ñÅ. (5) ‚ñÅ, (6) ‚ñÅthe (7) ‚ñÅ' (8) ‚ñÅand (9) ‚ñÅ\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ‚ñÅ. (5) ‚ñÅ, (6) ‚ñÅthe (7) ‚ñÅ' (8) ‚ñÅand (9) ‚ñÅ\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - Number of Src words (types): 9034\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.helpers - Number of Trg words (types): 9034\n",
            "2022-01-28 08:46:49,959 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=9034),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=9034))\n",
            "2022-01-28 08:46:49,965 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 1028\n",
            "\ttotal batch size (w. parallel & accumulation): 1028\n",
            "2022-01-28 08:46:49,966 - INFO - joeynmt.training - EPOCH 1\n",
            "2022-01-28 08:46:49,995 - INFO - joeynmt.training - Epoch   1: total training loss 0.00\n",
            "2022-01-28 08:46:49,995 - INFO - joeynmt.training - EPOCH 2\n",
            "2022-01-28 08:47:04,509 - INFO - joeynmt.training - Epoch   2, Step:   114100, Batch Loss:     3.014850, Tokens per Sec:     3210, Lr: 0.000300\n",
            "2022-01-28 08:47:18,757 - INFO - joeynmt.training - Epoch   2, Step:   114200, Batch Loss:     2.906295, Tokens per Sec:     3108, Lr: 0.000300\n",
            "2022-01-28 08:48:37,423 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2022-01-28 08:48:37,424 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2022-01-28 08:48:37,424 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2022-01-28 08:48:37,905 - INFO - joeynmt.training - Example #0\n",
            "2022-01-28 08:48:37,905 - INFO - joeynmt.training - \tSource:     ‚ñÅdefense ‚ñÅw ants ‚ñÅcase ‚ñÅdis miss ed ‚ñÅon ‚ñÅground s ‚ñÅthat ‚ñÅman ning ' s ‚ñÅconf in ement ‚ñÅwas ‚ñÅhar sh\n",
            "2022-01-28 08:48:37,905 - INFO - joeynmt.training - \tReference:  ‚ñÅsavunma ‚ñÅman n ing ' in ‚ñÅhap s ed ilm esinin ‚ñÅsert ‚ñÅolduƒüu ‚ñÅgerek √ß esiyle ‚ñÅdav anƒ±n ‚ñÅkap an masƒ±nƒ± ‚ñÅist iyor\n",
            "2022-01-28 08:48:37,905 - INFO - joeynmt.training - \tHypothesis: ‚ñÅsavunma , ‚ñÅinsan ƒ±n ‚ñÅkaf asƒ±nƒ±n ‚ñÅkar ƒ±≈ütƒ±r ƒ±lmasƒ± ‚ñÅzarar lƒ± ‚ñÅolduƒüu ‚ñÅyer lerde ‚ñÅsu√ß lam ayƒ± ‚ñÅist iyor .\n",
            "2022-01-28 08:48:37,905 - INFO - joeynmt.training - Example #1\n",
            "2022-01-28 08:48:37,906 - INFO - joeynmt.training - \tSource:     ‚ñÅthe ‚ñÅarmy ‚ñÅprivate ‚ñÅis ‚ñÅacc used ‚ñÅof ‚ñÅste aling ‚ñÅthous ands ‚ñÅof ‚ñÅclass ified ‚ñÅdoc uments\n",
            "2022-01-28 08:48:37,906 - INFO - joeynmt.training - \tReference:  ‚ñÅordu daki ‚ñÅer ‚ñÅbin lerce ‚ñÅgizli ‚ñÅbel ge ‚ñÅ√ßal mak la ‚ñÅsu√ß lan ƒ±yor\n",
            "2022-01-28 08:48:37,906 - INFO - joeynmt.training - \tHypothesis: ‚ñÅordu ‚ñÅ√∂zel liƒüi ‚ñÅbin lerce ‚ñÅsƒ±nƒ±f landƒ±r ƒ±lmƒ±≈ü ‚ñÅbelg eyi ‚ñÅ√ßal mak la ‚ñÅsu√ß landƒ± .\n",
            "2022-01-28 08:48:37,906 - INFO - joeynmt.training - Example #2\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - \tSource:     ‚ñÅpro sec ut ors ‚ñÅtried ‚ñÅto ‚ñÅestablish ‚ñÅfr iday ‚ñÅthat ‚ñÅarmy ‚ñÅprivate ‚ñÅbr ad ley ‚ñÅman ning ‚ñÅ- - ‚ñÅchar ged ‚ñÅin ‚ñÅthe ‚ñÅlargest ‚ñÅle ak ‚ñÅof ‚ñÅclass ified ‚ñÅmaterial ‚ñÅin ‚ñÅu . s . ‚ñÅhistory ‚ñÅ- - ‚ñÅmiss ed ‚ñÅmultiple ‚ñÅopportun ities ‚ñÅto ‚ñÅcompl ain ‚ñÅabout ‚ñÅthe ‚ñÅm ist reat ment ‚ñÅhe ' s ‚ñÅalle ging ‚ñÅhe ‚ñÅsuffered ‚ñÅin ‚ñÅmilitary ‚ñÅcust ody .\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - \tReference:  ‚ñÅsav cƒ±lar ‚ñÅcum a ‚ñÅg√ºn√º ‚ñÅabd ‚ñÅtarihinde ki ‚ñÅen ‚ñÅb√ºy√ºk ‚ñÅgizli ‚ñÅmat er yal ‚ñÅs ƒ±z ƒ±nt ƒ±sƒ±yla ‚ñÅsu√ß lanan ‚ñÅordu daki ‚ñÅer ‚ñÅbr ad ley ‚ñÅman n ing ' in ‚ñÅaskeri ‚ñÅg√∂z altƒ± ‚ñÅsƒ±rasƒ±nda ‚ñÅya≈üadƒ±ƒüƒ± ‚ñÅk√∂t√º ‚ñÅdavranƒ±≈ü ‚ñÅiddi alarƒ± ‚ñÅhakkƒ±nda ‚ñÅbirden ‚ñÅfazla ‚ñÅ≈ü ik ayet ‚ñÅet me ‚ñÅfƒ±r sat ƒ±nƒ± ‚ñÅka√ß ƒ±r dƒ± ƒüƒ±nƒ± ‚ñÅbelirt meye ‚ñÅ√ßalƒ±≈ütƒ± .\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - \tHypothesis: ‚ñÅsav cƒ±lar , ‚ñÅabd ‚ñÅtarih inin ‚ñÅen ‚ñÅb√ºy√ºk ‚ñÅsƒ±nƒ±f landƒ±r ƒ±lmƒ±≈ü ‚ñÅmat er yal in ‚ñÅen ‚ñÅb√ºy√ºk ‚ñÅle ak esinde , ‚ñÅabd ‚ñÅtarih indeki ‚ñÅaskeri ‚ñÅg√∂z alt ƒ±na ‚ñÅsƒ±k ƒ±ntƒ± ‚ñÅ√ßek tiƒüi ‚ñÅcum a , ‚ñÅasker inin ‚ñÅtutuk landƒ± ƒüƒ±nƒ± ‚ñÅiddia ‚ñÅeden ‚ñÅbir ‚ñÅ√ßok ‚ñÅfƒ±r sat ‚ñÅka√ß ƒ±r dƒ± .\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - Example #3\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - \tSource:     ‚ñÅwhile ‚ñÅcross - ex am ining ‚ñÅman ning ‚ñÅat ‚ñÅa ‚ñÅpre - t rial ‚ñÅhe aring ‚ñÅat ‚ñÅf t . ‚ñÅme ade , ‚ñÅmary land , ‚ñÅpro sec ut or ‚ñÅmaj . ‚ñÅash den ‚ñÅfe in ‚ñÅass er ted ‚ñÅthat ‚ñÅrecords ‚ñÅof ‚ñÅweek ly ‚ñÅvis its ‚ñÅman ning ‚ñÅhad ‚ñÅwith ‚ñÅunit ‚ñÅofficers ‚ñÅduring ‚ñÅnine ‚ñÅmonths ‚ñÅof ‚ñÅdet ention ‚ñÅat ‚ñÅquant ico , ‚ñÅvirginia , ‚ñÅshow ‚ñÅno ‚ñÅcompl ain ts ‚ñÅabout ‚ñÅhis ‚ñÅtreatment .\n",
            "2022-01-28 08:48:37,907 - INFO - joeynmt.training - \tReference:  ‚ñÅf t . ‚ñÅme ade ‚ñÅmar y land ' deki ‚ñÅdur u≈ü ma ‚ñÅ√∂ncesi ‚ñÅc els ede ‚ñÅsav cƒ± ‚ñÅma j . ‚ñÅas h den ‚ñÅfe in ‚ñÅman n ing ' i ‚ñÅ√ßap raz ‚ñÅsor gu lar ken , ‚ñÅqu an ti co , ‚ñÅvir g ini a ' da ‚ñÅdokuz ‚ñÅay ‚ñÅg√∂z alt ƒ±nda ‚ñÅkal dƒ±ƒüƒ± ‚ñÅs√ºr ece ‚ñÅbirim ‚ñÅmem ur larƒ±yla ‚ñÅhaft alƒ±k ‚ñÅziyaret lerinin ‚ñÅkayƒ±t larƒ±nƒ±n ‚ñÅk√∂t√º ‚ñÅdavran ƒ±lmasƒ± yla ‚ñÅilgili ‚ñÅbir ‚ñÅ≈ü ik ayet ‚ñÅg√∂ster med iƒüini ‚ñÅortaya ‚ñÅkoy du .\n",
            "2022-01-28 08:48:37,908 - INFO - joeynmt.training - \tHypothesis: ‚ñÅf t , ‚ñÅb t , ‚ñÅmar y land , ‚ñÅmar y land , ‚ñÅpr os ter th en ‚ñÅf t , ‚ñÅmar y land , ‚ñÅpro ter in , ‚ñÅf t t , ‚ñÅpr os ed √ºr <unk> de , ‚ñÅ00 ‚ñÅay lƒ±k ‚ñÅtespit ‚ñÅedilen , ‚ñÅvir g ini a , ‚ñÅteda v isi ‚ñÅhakkƒ±nda ‚ñÅ≈ü ik √¢ yet ‚ñÅver med iƒüini ‚ñÅbelirt ti .\n",
            "2022-01-28 08:48:37,908 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   114200: bleu:  15.27, loss: 73264.8047, ppl:  10.9133, duration: 79.1501s\n",
            "2022-01-28 08:48:52,204 - INFO - joeynmt.training - Epoch   2, Step:   114300, Batch Loss:     2.934678, Tokens per Sec:     3201, Lr: 0.000300\n",
            "2022-01-28 08:49:06,575 - INFO - joeynmt.training - Epoch   2, Step:   114400, Batch Loss:     3.330844, Tokens per Sec:     3175, Lr: 0.000300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a translator for our new finetuned model"
      ],
      "metadata": {
        "id": "B5qWnAmQxLWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_translator = MasakhaneTranslate(model_path=\"/content/models/entr_finetune\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "4M6VB4QexK0P",
        "outputId": "2cac592d-106d-44da-fa4a-e18bbe00bdc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-8affe51e2add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinetuned_translator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMasakhaneTranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/models/entr_finetune\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/afrotranslate/translator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, model_name, version, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \"\"\"\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/afrotranslate/translator.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_path, model_name, version, device)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mckpt_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_candidates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No checkpoint file under model directory'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_candidates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WARNING: More than one checkpoint under model directory. Taking first:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_candidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: ('No checkpoint file under model directory', '/content/models/entr_finetune')"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's test on the finetuned model to see if it has improved on the test set"
      ],
      "metadata": {
        "id": "5tIKZG18A3KT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7tEJdHnZxskB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aQLZu7d_xsh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VIs2sJBWxsfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l929HimrxS0a"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Part 8 (homework) - Training a model from scratch\n",
        "\n",
        "## Retrieve your data & make a parallel corpus\n",
        "\n",
        "In this part we will use open corpus available from OPUS repository to train a translation model. We will first download the data, create training, development, testing sets from it and then use JoeyNMT to train a baseline model. \n",
        "\n",
        "In the next cell, you need to set the languages you want to work with and specify which corpus you want to use to train. \n",
        "\n",
        "To select a corpus go to https://opus.nlpl.eu/, enter your language pair and select one that you think is more appropriate (size, domain)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Cn3tgQLzUxwn"
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"tr\"\n",
        "\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO5IY_RywQyA",
        "outputId": "06663965-1bc8-4ee6-f622-528cac71dff0"
      },
      "source": [
        "# This will save it to a folder in our gdrive instead!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p \"/content/drive/My Drive/mt-workshop/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/mt-workshop/%s-%s-%s\" % (source_language, target_language, tag)\n",
        "\n",
        "!echo $gdrive_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/mt-workshop/en-tr-baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gA75Fs9ys8Y9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2977aa26-bec6-4e69-b72a-654090a41441"
      },
      "source": [
        "# Install opus-tools (Warning! This is not really python)\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opustools-pkg\n",
            "  Downloading opustools_pkg-0.0.52-py3-none-any.whl (80 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà                            | 10 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 20 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 30 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 40 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 51 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 61 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 71 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80 kB 5.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: opustools-pkg\n",
            "Successfully installed opustools-pkg-0.0.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xq-tDZVks7ZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f74b9853-c632-42cb-9c14-19f0a2583d6d"
      },
      "source": [
        "# TODO: Indicate here the ID of the corpus you want to use from OPUS\n",
        "opus_corpus = \"TED2020\" \n",
        "os.environ[\"corpus\"] = opus_corpus\n",
        "\n",
        "# Downloading our corpus \n",
        "! opus_read -d $corpus -s $src -t $tgt -wm moses -w $corpus.$src $corpus.$tgt -q\n",
        "\n",
        "# Extract the corpus file\n",
        "! gunzip ${corpus}_latest_xml_$src-$tgt.xml.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/TED2020/latest/xml/en-tr.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "   2 MB https://object.pouta.csc.fi/OPUS-TED2020/v1/xml/en-tr.xml.gz\n",
            "  47 MB https://object.pouta.csc.fi/OPUS-TED2020/v1/xml/en.zip\n",
            "  37 MB https://object.pouta.csc.fi/OPUS-TED2020/v1/xml/tr.zip\n",
            "\n",
            "  85 MB Total size\n",
            "./TED2020_latest_xml_en-tr.xml.gz ... 100% of 2 MB\n",
            "./TED2020_latest_xml_en.zip ... 100% of 47 MB\n",
            "./TED2020_latest_xml_tr.zip ... 100% of 37 MB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sazl7hv9xZFg"
      },
      "source": [
        "# Read the corpus into python lists\n",
        "source_file = opus_corpus + '.' + source_language\n",
        "target_file = opus_corpus + '.' + target_language\n",
        "\n",
        "src_all = [sentence.strip() for sentence in open(source_file).readlines()]\n",
        "tgt_all = [sentence.strip() for sentence in open(target_file).readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjcOBlYMxxTC",
        "outputId": "71fffa11-d629-4780-94fb-c1b511acb6b4"
      },
      "source": [
        "# Let's take a peek at the files\n",
        "print(\"Source size:\", len(src_all))\n",
        "print(\"Target size:\", len(tgt_all))\n",
        "print(\"--------\")\n",
        "\n",
        "peek_size = 5\n",
        "for i in range(peek_size):\n",
        "  print(\"Sent #\", i)\n",
        "  print(\"SRC:\", src_all[i])\n",
        "  print(\"TGT:\", tgt_all[i])\n",
        "  print(\"---------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source size: 374378\n",
            "Target size: 374378\n",
            "--------\n",
            "Sent # 0\n",
            "SRC: Thank you so much , Chris .\n",
            "TGT: √áok te≈üekk√ºr ederim Chris .\n",
            "---------\n",
            "Sent # 1\n",
            "SRC: And it 's truly a great honor to have the opportunity to come to this stage twice ; I 'm extremely grateful .\n",
            "TGT: Bu sahnede ikinci kez yer alma fƒ±rsatƒ±na sahip olmak ger√ßekten b√ºy√ºk bir onur . √áok minnettarƒ±m .\n",
            "---------\n",
            "Sent # 2\n",
            "SRC: I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "TGT: Bu konferansta √ßok mutlu oldum , ve anlattƒ±klarƒ±mla ilgili g√ºzel yorumlarƒ±nƒ±z i√ßin sizlere √ßok te≈üekk√ºr ederim .\n",
            "---------\n",
            "Sent # 3\n",
            "SRC: And I say that sincerely , partly because ( Mock sob ) I need that .\n",
            "TGT: Bunu i√ßtenlikle s√∂yl√ºyorum , √ß√ºnk√º ... ( Aƒülama taklidi ) Buna ihtiyacƒ±m var .\n",
            "---------\n",
            "Sent # 4\n",
            "SRC: ( Laughter ) Put yourselves in my position .\n",
            "TGT: ( Kahkahalar ) Kendinizi benim yerime koyun !\n",
            "---------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkuK3B4p2AkN"
      },
      "source": [
        "## Making training, development and testing sets\n",
        "\n",
        "We need to pick training, development and testing sets from our corpus. Training set will contain the sentences that we'll teach our model. Development set will be used to see how our model is progressing during the training. And finally, testing set will be used to evaluate the model.\n",
        "\n",
        "You can optionally load your own testing set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkcE9T75y9za",
        "outputId": "5a45acc5-04ac-4334-f53b-2518fd0c6f4c"
      },
      "source": [
        "# TODO: Determine ratios of each set\n",
        "all_size = len(src_all)\n",
        "dev_size = 1000\n",
        "test_size = 1000\n",
        "train_size = all_size - test_size - dev_size\n",
        "\n",
        "src_train = src_all[0:train_size]\n",
        "tgt_train = tgt_all[0:train_size]\n",
        "\n",
        "src_dev = src_all[train_size:train_size+dev_size]\n",
        "tgt_dev = tgt_all[train_size:train_size+dev_size]\n",
        "\n",
        "src_test = src_all[train_size+dev_size:all_size]\n",
        "tgt_test = tgt_all[train_size+dev_size:all_size]\n",
        "\n",
        "print(\"Set sizes\")\n",
        "print(\"All:\", len(src_all))\n",
        "print(\"Train:\", len(src_train))\n",
        "print(\"Dev:\", len(src_dev))\n",
        "print(\"Test:\", len(src_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Set sizes\n",
            "All: 374378\n",
            "Train: 372378\n",
            "Dev: 1000\n",
            "Test: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# Preprocessing the Data into Subword BPE Tokens\n",
        "\n",
        "- One of the most powerful improvements for neural machine translation is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- BPE tokenization limits the number of vocabulary into a certain size by smartly dividing words into subwords\n",
        "\n",
        "- This is especially useful for agglutinative languages (like Turkish) where vocabulary is effectively endless. \n",
        "\n",
        "- Below you have the scripts for doing BPE tokenization of our data. We use bpemb library that has pre-trained BPE models to convert our data into subwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xYKgReL6A76",
        "outputId": "b61b48f6-873b-4c78-9b45-14e5f81071be"
      },
      "source": [
        "! pip install bpemb\n",
        "from bpemb import BPEmb\n",
        "\n",
        "BPE_VOCAB_SIZE = 5000\n",
        "bpemb_src = BPEmb(lang=source_language, vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=False)\n",
        "bpemb_tgt = BPEmb(lang=target_language, vs=BPE_VOCAB_SIZE, segmentation_only=True, preprocess=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bpemb\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bpemb) (4.41.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from bpemb) (3.6.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.2 MB 10.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bpemb) (2.23.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (5.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Installing collected packages: sentencepiece, bpemb\n",
            "Successfully installed bpemb-0.3.3 sentencepiece-0.1.96\n",
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs5000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 315918/315918 [00:00<00:00, 557183.59B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/tr/tr.wiki.bpe.vs5000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 315775/315775 [00:00<00:00, 713720.23B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_RWTDq169WQ",
        "outputId": "e2c68f65-1b1a-4b3b-de0a-e8383b822473"
      },
      "source": [
        "# Testing BPE encoding\n",
        "encoded_tokens = bpemb_src.encode(\"This is a test sentence to demonstrate how BPE encoding works for our source language.\")\n",
        "print(encoded_tokens)\n",
        "\n",
        "encoded_string = \" \".join(encoded_tokens)\n",
        "print(encoded_string)\n",
        "\n",
        "decoded_string = bpemb_src.decode(encoded_tokens)\n",
        "print(decoded_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['‚ñÅ', 'T', 'h', 'is', '‚ñÅis', '‚ñÅa', '‚ñÅtest', '‚ñÅsent', 'ence', '‚ñÅto', '‚ñÅdemonstr', 'ate', '‚ñÅhow', '‚ñÅ', 'BPE', '‚ñÅenc', 'od', 'ing', '‚ñÅworks', '‚ñÅfor', '‚ñÅour', '‚ñÅsource', '‚ñÅlanguage', '.']\n",
            "‚ñÅ T h is ‚ñÅis ‚ñÅa ‚ñÅtest ‚ñÅsent ence ‚ñÅto ‚ñÅdemonstr ate ‚ñÅhow ‚ñÅ BPE ‚ñÅenc od ing ‚ñÅworks ‚ñÅfor ‚ñÅour ‚ñÅsource ‚ñÅlanguage .\n",
            "This is a test sentence to demonstrate how BPE encoding works for our source language.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMMbGLEX8pct"
      },
      "source": [
        "# Shortcut functions to encode and decode\n",
        "def encode_bpe(string, lang, to_lower=True):\n",
        "  if to_lower:\n",
        "    string = string.lower()\n",
        "  if lang == source_language:\n",
        "    return \" \".join(bpemb_src.encode(string))\n",
        "  elif lang == target_language:\n",
        "    return \" \".join(bpemb_tgt.encode(string))\n",
        "  else:\n",
        "    return \"\"\n",
        "\n",
        "def decode_bpe(string, lang):\n",
        "  tokens = string.strip().split()\n",
        "  if lang == source_language:\n",
        "    return bpemb_src.decode(tokens)\n",
        "  elif lang == target_language:\n",
        "    return bpemb_tgt.decode(tokens)\n",
        "  else:\n",
        "    return \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fMMGYQ27ZUN"
      },
      "source": [
        "# Let's encode all our sets with BPE\n",
        "src_train_bpe = [encode_bpe(sentence, source_language) for sentence in src_train]\n",
        "tgt_train_bpe = [encode_bpe(sentence, target_language) for sentence in tgt_train]\n",
        "\n",
        "src_dev_bpe = [encode_bpe(sentence, source_language) for sentence in src_dev]\n",
        "tgt_dev_bpe = [encode_bpe(sentence, target_language) for sentence in tgt_dev]\n",
        "\n",
        "src_test_bpe = [encode_bpe(sentence, source_language) for sentence in src_test]\n",
        "tgt_test_bpe = [encode_bpe(sentence, target_language) for sentence in tgt_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRQ7hNIS0D0z"
      },
      "source": [
        "# Now let's write all our sets into separate files\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_train, tgt_train):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_dev, tgt_dev):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"test.\"+source_language, \"w\") as src_file, open(\"test.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_test, tgt_test):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"train.bpe.\"+source_language, \"w\") as src_file, open(\"train.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_train_bpe, tgt_train_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"dev.bpe.\"+source_language, \"w\") as src_file, open(\"dev.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_dev_bpe, tgt_dev_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"test.bpe.\"+source_language, \"w\") as src_file, open(\"test.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(src_test_bpe, tgt_test_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "hxxBOCA-xXhy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75594eef-8053-4f99-e1e7-b07c1c54003b"
      },
      "source": [
        "# Doublecheck the files. There should be no extra quotation marks or weird characters.\n",
        "! head -n5 train.*\n",
        "! head -n5 dev.*\n",
        "! head -n5 test.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.bpe.en <==\n",
            "‚ñÅthan k ‚ñÅyou ‚ñÅso ‚ñÅmuch ‚ñÅ, ‚ñÅch ris ‚ñÅ.\n",
            "‚ñÅand ‚ñÅit ‚ñÅ' s ‚ñÅtr u ly ‚ñÅa ‚ñÅgreat ‚ñÅhonor ‚ñÅto ‚ñÅhave ‚ñÅthe ‚ñÅopportun ity ‚ñÅto ‚ñÅcome ‚ñÅto ‚ñÅthis ‚ñÅstage ‚ñÅtwice ‚ñÅ; ‚ñÅi ‚ñÅ' m ‚ñÅextrem ely ‚ñÅgr ate ful ‚ñÅ.\n",
            "‚ñÅi ‚ñÅhave ‚ñÅbeen ‚ñÅbl own ‚ñÅaway ‚ñÅby ‚ñÅthis ‚ñÅconference ‚ñÅ, ‚ñÅand ‚ñÅi ‚ñÅwant ‚ñÅto ‚ñÅthan k ‚ñÅall ‚ñÅof ‚ñÅyou ‚ñÅfor ‚ñÅthe ‚ñÅmany ‚ñÅn ice ‚ñÅcom ments ‚ñÅabout ‚ñÅwhat ‚ñÅi ‚ñÅhad ‚ñÅto ‚ñÅsay ‚ñÅthe ‚ñÅother ‚ñÅnight ‚ñÅ.\n",
            "‚ñÅand ‚ñÅi ‚ñÅsay ‚ñÅthat ‚ñÅs inc er ely ‚ñÅ, ‚ñÅpart ly ‚ñÅbecause ‚ñÅ( ‚ñÅm ock ‚ñÅso b ‚ñÅ ) ‚ñÅi ‚ñÅneed ‚ñÅthat ‚ñÅ.\n",
            "‚ñÅ( ‚ñÅla ugh ter ‚ñÅ ) ‚ñÅput ‚ñÅy ours elves ‚ñÅin ‚ñÅmy ‚ñÅposition ‚ñÅ.\n",
            "\n",
            "==> train.bpe.tr <==\n",
            "‚ñÅ√ßok ‚ñÅte≈ü ek k√ºr ‚ñÅeder im ‚ñÅchris ‚ñÅ.\n",
            "‚ñÅbu ‚ñÅsahne de ‚ñÅikinci ‚ñÅkez ‚ñÅyer ‚ñÅal ma ‚ñÅfƒ±r sat ƒ±na ‚ñÅsahip ‚ñÅolmak ‚ñÅger √ß ekten ‚ñÅb√ºy√ºk ‚ñÅbir ‚ñÅonur ‚ñÅ. ‚ñÅ√ßok ‚ñÅmin net tar ƒ±m ‚ñÅ.\n",
            "‚ñÅbu ‚ñÅkonfer ans ta ‚ñÅ√ßok ‚ñÅmut lu ‚ñÅol d um ‚ñÅ, ‚ñÅve ‚ñÅanlat t ƒ±klarƒ± m la ‚ñÅilgili ‚ñÅg√ºzel ‚ñÅyorum larƒ±n ƒ±z ‚ñÅi√ßin ‚ñÅs iz lere ‚ñÅ√ßok ‚ñÅte≈ü ek k√ºr ‚ñÅeder im ‚ñÅ.\n",
            "‚ñÅbunu ‚ñÅi√ß ten likle ‚ñÅs√∂y l √ºyor um ‚ñÅ, ‚ñÅ√ß√ºnk√º ‚ñÅ... ‚ñÅ( ‚ñÅaƒü lama ‚ñÅtak li di ‚ñÅ) ‚ñÅbuna ‚ñÅihtiy ac ƒ±m ‚ñÅvar ‚ñÅ.\n",
            "‚ñÅ( ‚ñÅkah k ah alar ‚ñÅ) ‚ñÅkend in izi ‚ñÅbenim ‚ñÅyer ime ‚ñÅkoy un ‚ñÅ !\n",
            "\n",
            "==> train.en <==\n",
            "Thank you so much , Chris .\n",
            "And it 's truly a great honor to have the opportunity to come to this stage twice ; I 'm extremely grateful .\n",
            "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "And I say that sincerely , partly because ( Mock sob ) I need that .\n",
            "( Laughter ) Put yourselves in my position .\n",
            "\n",
            "==> train.tr <==\n",
            "√áok te≈üekk√ºr ederim Chris .\n",
            "Bu sahnede ikinci kez yer alma fƒ±rsatƒ±na sahip olmak ger√ßekten b√ºy√ºk bir onur . √áok minnettarƒ±m .\n",
            "Bu konferansta √ßok mutlu oldum , ve anlattƒ±klarƒ±mla ilgili g√ºzel yorumlarƒ±nƒ±z i√ßin sizlere √ßok te≈üekk√ºr ederim .\n",
            "Bunu i√ßtenlikle s√∂yl√ºyorum , √ß√ºnk√º ... ( Aƒülama taklidi ) Buna ihtiyacƒ±m var .\n",
            "( Kahkahalar ) Kendinizi benim yerime koyun !\n",
            "==> dev.bpe.en <==\n",
            "‚ñÅit ‚ñÅ' s ‚ñÅa ‚ñÅvery ‚ñÅbig ‚ñÅsignal ‚ñÅ; ‚ñÅit ‚ñÅ' s ‚ñÅsent ‚ñÅto ‚ñÅthe ‚ñÅbra in ‚ñÅsays ‚ñÅ, ‚ñÅ\" ‚ñÅgo ‚ñÅand ‚ñÅe at ‚ñÅ. ‚ñÅ\"\n",
            "‚ñÅyou ‚ñÅhave ‚ñÅstop ‚ñÅsign als ‚ñÅ- - ‚ñÅwe ‚ñÅhave ‚ñÅup ‚ñÅto ‚ñÅeight ‚ñÅstop ‚ñÅsign als ‚ñÅ.\n",
            "‚ñÅat ‚ñÅleast ‚ñÅin ‚ñÅmy ‚ñÅcase ‚ñÅ, ‚ñÅthey ‚ñÅare ‚ñÅnot ‚ñÅlist ened ‚ñÅto ‚ñÅ.\n",
            "‚ñÅ( ‚ñÅla ugh ter ‚ñÅ ) ‚ñÅso ‚ñÅwhat ‚ñÅhapp ens ‚ñÅif ‚ñÅthe ‚ñÅbig ‚ñÅbra in ‚ñÅin ‚ñÅthe ‚ñÅintegr ation ‚ñÅover r ides ‚ñÅthe ‚ñÅsignal ‚ñÅ ?\n",
            "‚ñÅso ‚ñÅif ‚ñÅyou ‚ñÅover r ide ‚ñÅthe ‚ñÅhun ger ‚ñÅsignal ‚ñÅ, ‚ñÅyou ‚ñÅcan ‚ñÅhave ‚ñÅa ‚ñÅdis order ‚ñÅ, ‚ñÅwhich ‚ñÅis ‚ñÅcalled ‚ñÅan ore x ia ‚ñÅ.\n",
            "\n",
            "==> dev.bpe.tr <==\n",
            "‚ñÅbu ‚ñÅ√ßok ‚ñÅg√º√ßl√º ‚ñÅbir ‚ñÅsin yal dir ‚ñÅ. ‚ñÅbey ine ‚ñÅgider ‚ñÅve ‚ñÅder ‚ñÅki ‚ñÅ, ‚ñÅ\" ‚ñÅgit ‚ñÅve ‚ñÅye ‚ñÅ. ‚ñÅ\"\n",
            "‚ñÅayrƒ±ca ‚ñÅdur ‚ñÅsin yal leri ‚ñÅde ‚ñÅvardƒ±r . ‚ñÅhemen ‚ñÅhemen ‚ñÅsekiz ‚ñÅtane ‚ñÅfarklƒ± ‚ñÅdur ‚ñÅsin yal imiz ‚ñÅvar ‚ñÅ.\n",
            "‚ñÅama ‚ñÅbenim ‚ñÅg ib iler ‚ñÅbu ‚ñÅsin yal leri ‚ñÅpek ‚ñÅde ‚ñÅdin lem iyor lar ‚ñÅ.\n",
            "‚ñÅ( ‚ñÅg√ºl √º≈ü meler ‚ñÅ) ‚ñÅp eki ‚ñÅ, ‚ñÅeƒüer ‚ñÅb√ºy√ºk ‚ñÅbey in ‚ñÅbu ‚ñÅg√∂nder ilen ‚ñÅsin yal i ‚ñÅg√∂r mez den ‚ñÅgelir se ‚ñÅne ‚ñÅolur ‚ñÅ ?\n",
            "‚ñÅeƒüer ‚ñÅa√ß lƒ±k ‚ñÅsin yal ini ‚ñÅg√∂r mez den ‚ñÅgelir sen iz ‚ñÅan or ek si ‚ñÅden en ‚ñÅhast alƒ± ƒüa ‚ñÅtut ulur sun uz ‚ñÅ.\n",
            "\n",
            "==> dev.en <==\n",
            "It 's a very big signal ; it 's sent to the brain says , \" Go and eat . \"\n",
            "You have stop signals -- we have up to eight stop signals .\n",
            "At least in my case , they are not listened to .\n",
            "( Laughter ) So what happens if the big brain in the integration overrides the signal ?\n",
            "So if you override the hunger signal , you can have a disorder , which is called anorexia .\n",
            "\n",
            "==> dev.tr <==\n",
            "Bu √ßok g√º√ßl√º bir sinyaldir . Beyine gider ve der ki , \" Git ve ye . \"\n",
            "Ayrƒ±ca dur sinyalleri de vardƒ±r. hemen hemen sekiz tane farklƒ± dur sinyalimiz var .\n",
            "Ama benim gibiler bu sinyalleri pek de dinlemiyorlar .\n",
            "( G√ºl√º≈ümeler ) Peki , eƒüer b√ºy√ºk beyin bu g√∂nderilen sinyali g√∂rmezden gelirse ne olur ?\n",
            "Eƒüer a√ßlƒ±k sinyalini g√∂rmezden gelirseniz anoreksi denen hastalƒ±ƒüa tutulursunuz .\n",
            "==> test.bpe.en <==\n",
            "‚ñÅit ‚ñÅ' s ‚ñÅsomething ‚ñÅcalled ‚ñÅthe ‚ñÅre ward ‚ñÅsched ule ‚ñÅ.\n",
            "‚ñÅand ‚ñÅby ‚ñÅthis ‚ñÅ, ‚ñÅi ‚ñÅmean ‚ñÅlook ing ‚ñÅat ‚ñÅwhat ‚ñÅmill ions ‚ñÅupon ‚ñÅmill ions ‚ñÅof ‚ñÅpeople ‚ñÅhave ‚ñÅdone ‚ñÅand ‚ñÅcare ful ly ‚ñÅcal ib r ating ‚ñÅthe ‚ñÅrate ‚ñÅ, ‚ñÅthe ‚ñÅnature ‚ñÅ, ‚ñÅthe ‚ñÅtype ‚ñÅ, ‚ñÅthe ‚ñÅint ensity ‚ñÅof ‚ñÅre wards ‚ñÅin ‚ñÅgames ‚ñÅto ‚ñÅkeep ‚ñÅthem ‚ñÅeng aged ‚ñÅover ‚ñÅst ag ger ing ‚ñÅamount s ‚ñÅof ‚ñÅtime ‚ñÅand ‚ñÅeffort ‚ñÅ.\n",
            "‚ñÅnow ‚ñÅ, ‚ñÅto ‚ñÅt ry ‚ñÅand ‚ñÅexpl ain ‚ñÅthis ‚ñÅin ‚ñÅs ort ‚ñÅof ‚ñÅreal ‚ñÅterms ‚ñÅ, ‚ñÅi ‚ñÅwant ‚ñÅto ‚ñÅtalk ‚ñÅabout ‚ñÅa ‚ñÅkind ‚ñÅof ‚ñÅt ask ‚ñÅthat ‚ñÅmight ‚ñÅfall ‚ñÅto ‚ñÅyou ‚ñÅin ‚ñÅso ‚ñÅmany ‚ñÅgames ‚ñÅ.\n",
            "‚ñÅgo ‚ñÅand ‚ñÅget ‚ñÅa ‚ñÅcertain ‚ñÅamount ‚ñÅof ‚ñÅa ‚ñÅcertain ‚ñÅlittle ‚ñÅgame - y ‚ñÅit em ‚ñÅ.\n",
            "‚ñÅlet ‚ñÅ' s ‚ñÅsay ‚ñÅ, ‚ñÅfor ‚ñÅthe ‚ñÅs ake ‚ñÅof ‚ñÅarg ument ‚ñÅ, ‚ñÅmy ‚ñÅmission ‚ñÅis ‚ñÅto ‚ñÅget ‚ñÅ 15 ‚ñÅp ies ‚ñÅand ‚ñÅi ‚ñÅcan ‚ñÅget ‚ñÅ 15 ‚ñÅp ies ‚ñÅby ‚ñÅkill ing ‚ñÅthese ‚ñÅc ute ‚ñÅ, ‚ñÅlittle ‚ñÅmon st ers ‚ñÅ.\n",
            "\n",
            "==> test.bpe.tr <==\n",
            "‚ñÅ√∂d√ºl ‚ñÅtak v imi ‚ñÅden iyor ‚ñÅ.\n",
            "‚ñÅbununla ‚ñÅ, ‚ñÅmilyon lar ca ‚ñÅinsan ƒ±n ‚ñÅne ‚ñÅyaptƒ± ƒüƒ±na ‚ñÅve ‚ñÅs√ºr es iz ‚ñÅzaman ‚ñÅve ‚ñÅ√ß aba ‚ñÅhar c ay arak ‚ñÅoyun lara ‚ñÅbaƒülƒ± ‚ñÅkal malar ƒ±nƒ± ‚ñÅsaƒülayan ‚ñÅ√∂d√ºl lerin ‚ñÅt√ºr√º ‚ñÅ, ‚ñÅ√ße≈ü idi ‚ñÅve ‚ñÅ√∂l√ß √ºs √ºn√ºn ‚ñÅay ar lan masƒ±na ‚ñÅdikkat le ‚ñÅbak mayƒ± ‚ñÅk ast ed iyor um ‚ñÅ.\n",
            "‚ñÅ≈üim di ‚ñÅ, ‚ñÅbunu ‚ñÅger√ßek ‚ñÅanlam da ‚ñÅden em ek ‚ñÅve ‚ñÅa√ßƒ±klam ak ‚ñÅi√ßin ‚ñÅbir√ßok ‚ñÅoy unda ‚ñÅkar≈üƒ±la≈ü abilece ƒüin iz ‚ñÅbir ‚ñÅg√∂rev ‚ñÅhakkƒ±nda ‚ñÅkonu≈ü mak ‚ñÅist iyor um ‚ñÅ.\n",
            "‚ñÅgi di p ‚ñÅk√º√ß√ºk ‚ñÅbir ‚ñÅoyun ‚ñÅnes n esinden ‚ñÅbelirli ‚ñÅmiktar da ‚ñÅgetir in ‚ñÅ.\n",
            "‚ñÅ√∂rnek ‚ñÅolmasƒ± ‚ñÅi√ßin ‚ñÅfar z ed el im ‚ñÅki ‚ñÅbenim ‚ñÅg√∂rev im ‚ñÅ 15 ‚ñÅtane ‚ñÅ√ß √∂r ek ‚ñÅgetir mek ‚ñÅve ‚ñÅben ‚ñÅ 15 ‚ñÅtane ‚ñÅ√ß √∂r eƒüi ‚ñÅ≈üir in ‚ñÅ, ‚ñÅk√º√ß√ºk ‚ñÅcan av ar larƒ± ‚ñÅ√∂ld√ºr erek ‚ñÅgetir ebilir im ‚ñÅ.\n",
            "\n",
            "==> test.en <==\n",
            "It 's something called the reward schedule .\n",
            "And by this , I mean looking at what millions upon millions of people have done and carefully calibrating the rate , the nature , the type , the intensity of rewards in games to keep them engaged over staggering amounts of time and effort .\n",
            "Now , to try and explain this in sort of real terms , I want to talk about a kind of task that might fall to you in so many games .\n",
            "Go and get a certain amount of a certain little game-y item .\n",
            "Let 's say , for the sake of argument , my mission is to get 15 pies and I can get 15 pies by killing these cute , little monsters .\n",
            "\n",
            "==> test.tr <==\n",
            "√ñd√ºl Takvimi deniyor .\n",
            "Bununla , milyonlarca insanƒ±n ne yaptƒ±ƒüƒ±na ve s√ºresiz zaman ve √ßaba harcayarak oyunlara baƒülƒ± kalmalarƒ±nƒ± saƒülayan √∂d√ºllerin t√ºr√º , √ße≈üidi ve √∂l√ß√ºs√ºn√ºn ayarlanmasƒ±na dikkatle bakmayƒ± kastediyorum .\n",
            "≈ûimdi , bunu ger√ßek anlamda denemek ve a√ßƒ±klamak i√ßin bir√ßok oyunda kar≈üƒ±la≈üabileceƒüiniz bir g√∂rev hakkƒ±nda konu≈ümak istiyorum .\n",
            "Gidip k√º√ß√ºk bir oyun nesnesinden belirli miktarda getirin .\n",
            "√ñrnek olmasƒ± i√ßin farzedelim ki benim g√∂revim 15 tane √ß√∂rek getirmek ve ben 15 tane √ß√∂reƒüi ≈üirin , k√º√ß√ºk canavarlarƒ± √∂ld√ºrerek getirebilirim .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "IlMitUHR8Qy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "593e67cf-6fd0-437b-c3f3-f5991f564715"
      },
      "source": [
        "# If creating data for the first time, move all prepared data to the mounted location in google drive\n",
        "! mkdir \"$gdrive_path\"/data\n",
        "! cp train.* \"$gdrive_path\"/data\n",
        "! cp test.* \"$gdrive_path\"/data\n",
        "! cp dev.* \"$gdrive_path\"/data\n",
        "! ls \"$gdrive_path\"/data  #See the contents of the drive directory"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev.bpe.en  dev.en  test.bpe.en  test.en  train.bpe.en\ttrain.en\n",
            "dev.bpe.tr  dev.tr  test.bpe.tr  test.tr  train.bpe.tr\ttrain.tr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwNhkXPXtFtx"
      },
      "source": [
        "# OR... If continuing from previous run, load files from drive\n",
        "! cp \"$gdrive_path\"/data/dev.* .\n",
        "! cp \"$gdrive_path\"/data/train.* .\n",
        "! cp \"$gdrive_path\"/data/test.* ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iBRMm4kMxZ8L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe0350bc-e88f-482c-ac33-d905b748b4a7"
      },
      "source": [
        "#IMPORTANT: Restart runtime if you have installed AfroTranslate\n",
        "\n",
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'joeynmt' already exists and is not an empty directory.\n",
            "Processing /content/joeynmt\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (1.20.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (57.4.0)\n",
            "Collecting torch>=1.9.0\n",
            "  Using cached torch-1.10.2-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (2.7.0)\n",
            "Collecting torchtext>=0.10.0\n",
            "  Using cached torchtext-0.11.2-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n",
            "Requirement already satisfied: sacrebleu>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (2.0.0)\n",
            "Requirement already satisfied: subword-nmt in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (0.3.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (0.11.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (6.0)\n",
            "Requirement already satisfied: pylint>=2.9.6 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (2.12.2)\n",
            "Requirement already satisfied: six>=1.12 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (1.12.0)\n",
            "Requirement already satisfied: wrapt==1.11.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.5.1) (1.11.1)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (2.4.1)\n",
            "Requirement already satisfied: toml>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (0.10.2)\n",
            "Requirement already satisfied: astroid<2.10,>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (2.9.3)\n",
            "Requirement already satisfied: isort<6,>=4.2.5 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (5.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (3.10.0.2)\n",
            "Requirement already satisfied: mccabe<0.7,>=0.6 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.5.1) (0.6.1)\n",
            "Requirement already satisfied: typed-ast<2.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.10,>=2.9.0->pylint>=2.9.6->joeynmt==1.5.1) (1.5.2)\n",
            "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.10,>=2.9.0->pylint>=2.9.6->joeynmt==1.5.1) (1.7.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.5.1) (2019.12.20)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.5.1) (0.4.4)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.5.1) (2.3.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.5.1) (0.8.9)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (1.43.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (3.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (1.35.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.5.1) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.5.1) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.5.1) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.5.1) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.5.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.5.1) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.5.1) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.5.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.5.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.5.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.5.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.5.1) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.5.1) (3.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext>=0.10.0->joeynmt==1.5.1) (4.62.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.5.1) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.5.1) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.5.1) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.5.1) (0.11.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.5.1) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.5.1) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.5.1) (2018.9)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.7/dist-packages (from subword-nmt->joeynmt==1.5.1) (4.0.3)\n",
            "Building wheels for collected packages: joeynmt\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-1.5.1-py3-none-any.whl size=86003 sha256=f0fd9a53198708427af3f8e21bba478d2a83c8bf1f6b08083466061fbae5bf19\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c7_9zzy7/wheels/0a/f4/bf/6c9d3b8efbfece6cd209f865be37382b02e7c3584df2e28ca4\n",
            "Successfully built joeynmt\n",
            "Installing collected packages: torch, torchtext, joeynmt\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.8.0+cu101\n",
            "    Uninstalling torch-1.8.0+cu101:\n",
            "      Successfully uninstalled torch-1.8.0+cu101\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.9.0\n",
            "    Uninstalling torchtext-0.9.0:\n",
            "      Successfully uninstalled torchtext-0.9.0\n",
            "  Attempting uninstall: joeynmt\n",
            "    Found existing installation: joeynmt 1.3\n",
            "    Uninstalling joeynmt-1.3:\n",
            "      Successfully uninstalled joeynmt-1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.10.2 which is incompatible.\n",
            "afrotranslate 0.0.6 requires joeynmt==1.3, but you have joeynmt 1.5.1 which is incompatible.\u001b[0m\n",
            "Successfully installed joeynmt-1.5.1 torch-1.10.2 torchtext-0.11.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu101\n",
            "  Using cached https://download.pytorch.org/whl/cu101/torch-1.8.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.5 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.20.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.2\n",
            "    Uninstalling torch-1.10.2:\n",
            "      Successfully uninstalled torch-1.10.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "torchtext 0.11.2 requires torch==1.10.2, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "joeynmt 1.5.1 requires torch>=1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "afrotranslate 0.0.6 requires joeynmt==1.3, but you have joeynmt 1.5.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0+cu101\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "H-TyjtmXB1mL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c45c71b7-9fcd-42e6-ea03-2c29c0bd3115"
      },
      "source": [
        "#Move everything important under joeynmt directory\n",
        "os.environ[\"data_path\"] = os.path.join(\"joeynmt\", \"data\", source_language + target_language)\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! head -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman\n",
        "\n",
        "# Backup vocab to drive\n",
        "! cp joeynmt/data/$src$tgt/vocab.txt \"$gdrive_path\"/data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev.bpe.en  dev.en  test.bpe.en  test.en  train.bpe.en\ttrain.en\n",
            "dev.bpe.tr  dev.tr  test.bpe.tr  test.tr  train.bpe.tr\ttrain.tr\n",
            "Combined BPE Vocab\n",
            "774\n",
            "531\n",
            "883\n",
            "6397\n",
            "794\n",
            "431\n",
            "381\n",
            "1414\n",
            "761\n",
            "548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PIs1lY2hxMsl"
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"models/entr_transformer/1000.ckpt\"\n",
        "    load_model: \"{gdrive_path}/models/{name}_transformer/1000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "6ZBPFwT94WpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8be06a9-c20c-41e6-f44a-7bffdf87dd8a"
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 16:52:38,026 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-28 16:52:38,058 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-28 16:52:46,845 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-28 16:52:48,172 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-28 16:52:48,225 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-28 16:52:48,238 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-28 16:52:48,238 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-28 16:52:48,484 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-28 16:52:48.642085: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-28 16:52:49,639 - INFO - joeynmt.training - Total params: 13372928\n",
            "2021-07-28 16:52:52,915 - INFO - joeynmt.training - Loading model from /content/drive/My Drive/mt-workshop/en-tr-baseline/models/entr_transformer/1000.ckpt\n",
            "2021-07-28 16:52:53,369 - INFO - joeynmt.helpers - cfg.name                           : entr_transformer\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.src                       : en\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.trg                       : tr\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.train                     : data/entr/train.bpe\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.dev                       : data/entr/dev.bpe\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.test                      : data/entr/test.bpe\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : data/entr/vocab.txt\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : data/entr/vocab.txt\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/drive/My Drive/mt-workshop/en-tr-baseline/models/entr_transformer/1000.ckpt\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-28 16:52:53,370 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-28 16:52:53,371 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1000\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/entr_transformer\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-28 16:52:53,372 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-28 16:52:53,373 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 370216,\n",
            "\tvalid 1000,\n",
            "\ttest 1000\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] ‚ñÅthan k ‚ñÅyou ‚ñÅso ‚ñÅmuch ‚ñÅ, ‚ñÅch ris ‚ñÅ.\n",
            "\t[TRG] ‚ñÅ√ßok ‚ñÅte≈ü ek k√ºr ‚ñÅeder im ‚ñÅchris ‚ñÅ.\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ‚ñÅ. (5) ‚ñÅ, (6) ‚ñÅthe (7) ‚ñÅ' (8) ‚ñÅand (9) ‚ñÅ\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ‚ñÅ. (5) ‚ñÅ, (6) ‚ñÅthe (7) ‚ñÅ' (8) ‚ñÅand (9) ‚ñÅ\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - Number of Src words (types): 9034\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.helpers - Number of Trg words (types): 9034\n",
            "2021-07-28 16:52:53,374 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=9034),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=9034))\n",
            "2021-07-28 16:52:53,383 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-28 16:52:53,383 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-28 16:53:08,820 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     5.040347, Tokens per Sec:    15405, Lr: 0.000300\n",
            "2021-07-28 16:53:23,770 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.679378, Tokens per Sec:    16521, Lr: 0.000300\n",
            "2021-07-28 16:53:39,046 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     4.687664, Tokens per Sec:    16342, Lr: 0.000300\n",
            "2021-07-28 16:53:54,321 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.884373, Tokens per Sec:    15794, Lr: 0.000300\n",
            "2021-07-28 16:54:09,629 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     4.775616, Tokens per Sec:    15877, Lr: 0.000300\n",
            "2021-07-28 16:54:25,084 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     3.958221, Tokens per Sec:    15485, Lr: 0.000300\n",
            "2021-07-28 16:54:41,036 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     4.736360, Tokens per Sec:    15393, Lr: 0.000300\n",
            "2021-07-28 16:54:57,260 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.919059, Tokens per Sec:    14830, Lr: 0.000300\n",
            "2021-07-28 16:55:13,384 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     4.417824, Tokens per Sec:    14910, Lr: 0.000300\n",
            "2021-07-28 16:55:29,522 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.567585, Tokens per Sec:    15001, Lr: 0.000300\n",
            "2021-07-28 16:56:17,824 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-28 16:56:17,825 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-28 16:56:18,299 - INFO - joeynmt.training - Example #0\n",
            "2021-07-28 16:56:18,299 - INFO - joeynmt.training - \tSource:     ‚ñÅit ‚ñÅ' s ‚ñÅa ‚ñÅvery ‚ñÅbig ‚ñÅsignal ‚ñÅ; ‚ñÅit ‚ñÅ' s ‚ñÅsent ‚ñÅto ‚ñÅthe ‚ñÅbra in ‚ñÅsays ‚ñÅ, ‚ñÅ\" ‚ñÅgo ‚ñÅand ‚ñÅe at ‚ñÅ. ‚ñÅ\"\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tReference:  ‚ñÅbu ‚ñÅ√ßok ‚ñÅg√º√ßl√º ‚ñÅbir ‚ñÅsin yal dir ‚ñÅ. ‚ñÅbey ine ‚ñÅgider ‚ñÅve ‚ñÅder ‚ñÅki ‚ñÅ, ‚ñÅ\" ‚ñÅgit ‚ñÅve ‚ñÅye ‚ñÅ. ‚ñÅ\"\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tHypothesis: ‚ñÅbu ‚ñÅy√ºzden ‚ñÅ, ‚ñÅ\" ‚ñÅbu ‚ñÅy√ºzden ‚ñÅ, ‚ñÅ\" ‚ñÅd ed iƒü in iz ‚ñÅ, ‚ñÅ\" ‚ñÅd ed iƒü im ‚ñÅ, ‚ñÅ\"\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - Example #1\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tSource:     ‚ñÅyou ‚ñÅhave ‚ñÅstop ‚ñÅsign als ‚ñÅ- - ‚ñÅwe ‚ñÅhave ‚ñÅup ‚ñÅto ‚ñÅeight ‚ñÅstop ‚ñÅsign als ‚ñÅ.\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tReference:  ‚ñÅayrƒ±ca ‚ñÅdur ‚ñÅsin yal leri ‚ñÅde ‚ñÅvardƒ±r . ‚ñÅhemen ‚ñÅhemen ‚ñÅsekiz ‚ñÅtane ‚ñÅfarklƒ± ‚ñÅdur ‚ñÅsin yal imiz ‚ñÅvar ‚ñÅ.\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tHypothesis: ‚ñÅbu ‚ñÅy√ºzden ‚ñÅ, ‚ñÅbu ‚ñÅy√ºzden ‚ñÅ, ‚ñÅbu ‚ñÅy√ºzden ‚ñÅ, ‚ñÅbu ‚ñÅy√ºzden ‚ñÅ, ‚ñÅbu ‚ñÅy√ºzden ‚ñÅ, ‚ñÅbu ‚ñÅy√ºzden ‚ñÅ, ‚ñÅbu ‚ñÅy√ºzden ‚ñÅbir ‚ñÅ≈üey ‚ñÅ.\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - Example #2\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tSource:     ‚ñÅat ‚ñÅleast ‚ñÅin ‚ñÅmy ‚ñÅcase ‚ñÅ, ‚ñÅthey ‚ñÅare ‚ñÅnot ‚ñÅlist ened ‚ñÅto ‚ñÅ.\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tReference:  ‚ñÅama ‚ñÅbenim ‚ñÅg ib iler ‚ñÅbu ‚ñÅsin yal leri ‚ñÅpek ‚ñÅde ‚ñÅdin lem iyor lar ‚ñÅ.\n",
            "2021-07-28 16:56:18,300 - INFO - joeynmt.training - \tHypothesis: ‚ñÅbu ‚ñÅy√ºzden ‚ñÅ, ‚ñÅbu ‚ñÅy√ºzden ‚ñÅ, ‚ñÅbu ‚ñÅy√ºzden ‚ñÅ, ‚ñÅbu ‚ñÅy√ºzden ‚ñÅ, ‚ñÅbu ‚ñÅy√ºzden ‚ñÅ,\n",
            "2021-07-28 16:56:18,301 - INFO - joeynmt.training - Example #3\n",
            "2021-07-28 16:56:18,301 - INFO - joeynmt.training - \tSource:     ‚ñÅ( ‚ñÅla ugh ter ‚ñÅ ) ‚ñÅso ‚ñÅwhat ‚ñÅhapp ens ‚ñÅif ‚ñÅthe ‚ñÅbig ‚ñÅbra in ‚ñÅin ‚ñÅthe ‚ñÅintegr ation ‚ñÅover r ides ‚ñÅthe ‚ñÅsignal ‚ñÅ ?\n",
            "2021-07-28 16:56:18,301 - INFO - joeynmt.training - \tReference:  ‚ñÅ( ‚ñÅg√ºl √º≈ü meler ‚ñÅ) ‚ñÅp eki ‚ñÅ, ‚ñÅeƒüer ‚ñÅb√ºy√ºk ‚ñÅbey in ‚ñÅbu ‚ñÅg√∂nder ilen ‚ñÅsin yal i ‚ñÅg√∂r mez den ‚ñÅgelir se ‚ñÅne ‚ñÅolur ‚ñÅ ?\n",
            "2021-07-28 16:56:18,301 - INFO - joeynmt.training - \tHypothesis: ‚ñÅ( ‚ñÅg√ºl √º≈ü meler ‚ñÅ) ‚ñÅbu ‚ñÅy√ºzden ‚ñÅ, ‚ñÅbu ‚ñÅy√ºzden ‚ñÅbu ‚ñÅy√ºzden ‚ñÅbu ‚ñÅy√ºzden ‚ñÅbu ‚ñÅ≈üekilde ‚ñÅnasƒ±l ‚ñÅnasƒ±l ‚ñÅnasƒ±l ‚ñÅyap abilir ‚ñÅmi ‚ñÅ ?\n",
            "2021-07-28 16:56:18,301 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     2000: bleu:   2.31, loss: 104788.8047, ppl:  65.9925, duration: 48.7782s\n",
            "2021-07-28 16:56:34,324 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     4.015805, Tokens per Sec:    15108, Lr: 0.000300\n",
            "2021-07-28 16:56:50,301 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     4.364112, Tokens per Sec:    14957, Lr: 0.000300\n",
            "2021-07-28 16:57:06,377 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     3.913055, Tokens per Sec:    15505, Lr: 0.000300\n",
            "2021-07-28 16:57:22,392 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     4.053391, Tokens per Sec:    15228, Lr: 0.000300\n",
            "2021-07-28 16:57:38,524 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     4.355020, Tokens per Sec:    15342, Lr: 0.000300\n",
            "2021-07-28 16:57:54,767 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.925006, Tokens per Sec:    15665, Lr: 0.000300\n",
            "2021-07-28 16:58:10,709 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     3.960259, Tokens per Sec:    15288, Lr: 0.000300\n",
            "2021-07-28 16:58:26,716 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     3.953107, Tokens per Sec:    15453, Lr: 0.000300\n",
            "2021-07-28 16:58:42,729 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     3.973259, Tokens per Sec:    15140, Lr: 0.000300\n",
            "2021-07-28 16:58:58,866 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     4.405763, Tokens per Sec:    15363, Lr: 0.000300\n",
            "2021-07-28 16:59:45,427 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-28 16:59:45,428 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-28 16:59:45,862 - INFO - joeynmt.training - Example #0\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tSource:     ‚ñÅit ‚ñÅ' s ‚ñÅa ‚ñÅvery ‚ñÅbig ‚ñÅsignal ‚ñÅ; ‚ñÅit ‚ñÅ' s ‚ñÅsent ‚ñÅto ‚ñÅthe ‚ñÅbra in ‚ñÅsays ‚ñÅ, ‚ñÅ\" ‚ñÅgo ‚ñÅand ‚ñÅe at ‚ñÅ. ‚ñÅ\"\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tReference:  ‚ñÅbu ‚ñÅ√ßok ‚ñÅg√º√ßl√º ‚ñÅbir ‚ñÅsin yal dir ‚ñÅ. ‚ñÅbey ine ‚ñÅgider ‚ñÅve ‚ñÅder ‚ñÅki ‚ñÅ, ‚ñÅ\" ‚ñÅgit ‚ñÅve ‚ñÅye ‚ñÅ. ‚ñÅ\"\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tHypothesis: ‚ñÅbu ‚ñÅ, ‚ñÅbir ‚ñÅ≈üey ‚ñÅ, ‚ñÅ\" ‚ñÅbu ‚ñÅ, ‚ñÅ\" ‚ñÅbir ‚ñÅ≈üey ‚ñÅ, ‚ñÅ\" ‚ñÅd ed iƒü im ‚ñÅgibi ‚ñÅ, ‚ñÅ\"\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - Example #1\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tSource:     ‚ñÅyou ‚ñÅhave ‚ñÅstop ‚ñÅsign als ‚ñÅ- - ‚ñÅwe ‚ñÅhave ‚ñÅup ‚ñÅto ‚ñÅeight ‚ñÅstop ‚ñÅsign als ‚ñÅ.\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tReference:  ‚ñÅayrƒ±ca ‚ñÅdur ‚ñÅsin yal leri ‚ñÅde ‚ñÅvardƒ±r . ‚ñÅhemen ‚ñÅhemen ‚ñÅsekiz ‚ñÅtane ‚ñÅfarklƒ± ‚ñÅdur ‚ñÅsin yal imiz ‚ñÅvar ‚ñÅ.\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tHypothesis: ‚ñÅ 2 0 ‚ñÅ' de ‚ñÅ, ‚ñÅ 2 0 ‚ñÅ' de ‚ñÅ, ‚ñÅ 2 0 ‚ñÅ' de ‚ñÅ, ‚ñÅ 2 0 ‚ñÅ' de ‚ñÅ.\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - Example #2\n",
            "2021-07-28 16:59:45,863 - INFO - joeynmt.training - \tSource:     ‚ñÅat ‚ñÅleast ‚ñÅin ‚ñÅmy ‚ñÅcase ‚ñÅ, ‚ñÅthey ‚ñÅare ‚ñÅnot ‚ñÅlist ened ‚ñÅto ‚ñÅ.\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - \tReference:  ‚ñÅama ‚ñÅbenim ‚ñÅg ib iler ‚ñÅbu ‚ñÅsin yal leri ‚ñÅpek ‚ñÅde ‚ñÅdin lem iyor lar ‚ñÅ.\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - \tHypothesis: ‚ñÅi Ãá n san lar ‚ñÅ, ‚ñÅbenim ‚ñÅi√ßin ‚ñÅ, ‚ñÅbenim ‚ñÅi√ßin ‚ñÅ, ‚ñÅbenim ‚ñÅi√ßin ‚ñÅ√ßok ‚ñÅiyi ‚ñÅdeƒüil ‚ñÅ.\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - Example #3\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - \tSource:     ‚ñÅ( ‚ñÅla ugh ter ‚ñÅ ) ‚ñÅso ‚ñÅwhat ‚ñÅhapp ens ‚ñÅif ‚ñÅthe ‚ñÅbig ‚ñÅbra in ‚ñÅin ‚ñÅthe ‚ñÅintegr ation ‚ñÅover r ides ‚ñÅthe ‚ñÅsignal ‚ñÅ ?\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - \tReference:  ‚ñÅ( ‚ñÅg√ºl √º≈ü meler ‚ñÅ) ‚ñÅp eki ‚ñÅ, ‚ñÅeƒüer ‚ñÅb√ºy√ºk ‚ñÅbey in ‚ñÅbu ‚ñÅg√∂nder ilen ‚ñÅsin yal i ‚ñÅg√∂r mez den ‚ñÅgelir se ‚ñÅne ‚ñÅolur ‚ñÅ ?\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - \tHypothesis: ‚ñÅ( ‚ñÅg√ºl √º≈ü meler ‚ñÅ) ‚ñÅbu ‚ñÅ, ‚ñÅbu ‚ñÅ, ‚ñÅbu ‚ñÅ, ‚ñÅbey in iz in ‚ñÅen ‚ñÅaz ƒ±ndan ‚ñÅdaha ‚ñÅfazla ‚ñÅinsan ƒ±n ‚ñÅnasƒ±l ‚ñÅ ?\n",
            "2021-07-28 16:59:45,864 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     3000: bleu:   3.29, loss: 96139.1641, ppl:  46.6989, duration: 46.9973s\n",
            "2021-07-28 17:00:01,804 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     3.237124, Tokens per Sec:    14911, Lr: 0.000300\n",
            "2021-07-28 17:00:17,811 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     3.741183, Tokens per Sec:    15031, Lr: 0.000300\n",
            "2021-07-28 17:00:34,045 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     4.225142, Tokens per Sec:    15262, Lr: 0.000300\n",
            "2021-07-28 17:00:49,975 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     4.210491, Tokens per Sec:    15347, Lr: 0.000300\n",
            "2021-07-28 17:01:06,026 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     4.105839, Tokens per Sec:    15412, Lr: 0.000300\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
            "    main()\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 35, in main\n",
            "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 805, in train\n",
            "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 440, in train_and_validate\n",
            "    self.optimizer.step()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\", line 89, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\", line 119, in step\n",
            "    group['eps'])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\", line 92, in adam\n",
            "    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "MBoDS09JM807"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "! mkdir -p \"$gdrive_path/models/${src}${tgt}_transformer/\"\n",
        "! cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qNinL_DvYhI"
      },
      "source": [
        "# OR... If continuing from previous work, load models from google drive to notebook storage  \n",
        "! mkdir -p joeynmt/models/${src}${tgt}_transformer\n",
        "! cp -r \"$gdrive_path/models/${src}${tgt}_transformer\" joeynmt/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "n94wlrCjVc17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af45ae97-0cc7-4b8c-8474-6a02108ffbba"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 1000\tLoss: 4971087.00000\tPPL: 85.69424\tbleu: 2.36058\tLR: 0.00030000\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "66WhRE9lIhoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24c5e2f-99a4-4ac2-e3aa-c8a0f86679fe"
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 16:28:56,802 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-28 16:28:56,802 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-28 16:28:57,990 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-28 16:28:58,003 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-28 16:28:58,014 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-28 16:28:58,045 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
            "2021-07-28 16:29:01,412 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-28 16:29:01,642 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-28 16:29:01,712 - INFO - joeynmt.prediction - Decoding on dev set (data/entr/dev.bpe.tr)...\n",
            "2021-07-28 16:30:08,794 - INFO - joeynmt.prediction -  dev bleu[13a]:   0.97 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-28 16:30:08,795 - INFO - joeynmt.prediction - Decoding on test set (data/entr/test.bpe.tr)...\n",
            "2021-07-28 16:31:30,255 - INFO - joeynmt.prediction - test bleu[13a]:   0.75 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-RmEmRGD7Sb"
      },
      "source": [
        "# Fine-tuning to domain\n",
        "\n",
        "One important technique in neural machine translation is in-domain adaptation or fine-tuning. This introduces the model a certain domain we're interested to do translations in. \n",
        "\n",
        "One simple way of doing this is having a pre-trained model and continuing training from it on our in-domain training set. \n",
        "\n",
        "In this example we're going to fine-tune our model to news. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXNGfXzZEbS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1853ccfc-bb8d-48ad-c56a-41e52441b9cd"
      },
      "source": [
        "fine_corpus = \"WMT-News\" \n",
        "os.environ[\"fine\"] = fine_corpus\n",
        "\n",
        "# Downloading our corpus \n",
        "! opus_read -d $fine -s $src -t $tgt -wm moses -w $fine.$src $fine.$tgt -q\n",
        "\n",
        "# Extract the corpus file\n",
        "! gunzip ${fine}_latest_xml_$src-$tgt.xml.gz\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/WMT-News/latest/xml/en-tr.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "  92 KB https://object.pouta.csc.fi/OPUS-WMT-News/v2019/xml/en-tr.xml.gz\n",
            "  63 MB https://object.pouta.csc.fi/OPUS-WMT-News/v2019/xml/en.zip\n",
            "   2 MB https://object.pouta.csc.fi/OPUS-WMT-News/v2019/xml/tr.zip\n",
            "\n",
            "  66 MB Total size\n",
            "./WMT-News_latest_xml_en-tr.xml.gz ... 100% of 92 KB\n",
            "./WMT-News_latest_xml_en.zip ... 100% of 63 MB\n",
            "./WMT-News_latest_xml_tr.zip ... 100% of 2 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjVYnALNFcyP"
      },
      "source": [
        "# Read the corpus into python lists\n",
        "source_file = fine_corpus + '.' + source_language\n",
        "target_file = fine_corpus + '.' + target_language\n",
        "\n",
        "fine_src_all_bpe = [encode_bpe(sentence.strip(),'en') for sentence in open(source_file).readlines()]\n",
        "fine_tgt_all_bpe = [encode_bpe(sentence.strip(), 'tr') for sentence in open(target_file).readlines()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maxV-WTBFwbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f475eb09-2054-4c7e-8c63-f3483a4d3e66"
      },
      "source": [
        "# Let's take a peek at the files\n",
        "print(\"Source size:\", len(fine_src_all_bpe))\n",
        "print(\"Target size:\", len(fine_tgt_all_bpe))\n",
        "print(\"--------\")\n",
        "\n",
        "peek_size = 5\n",
        "for i in range(peek_size):\n",
        "  print(\"Sent #\", i)\n",
        "  print(\"SRC:\", decode_bpe(fine_src_all_bpe[i], 'en'))\n",
        "  print(\"TGT:\", decode_bpe(fine_tgt_all_bpe[i],'tr'))\n",
        "  print(\"---------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source size: 20016\n",
            "Target size: 20016\n",
            "--------\n",
            "Sent # 0\n",
            "SRC: two people drowned in floods in trabzon\n",
            "TGT: trabzon ' da sel iki ki≈üiyi yuttu\n",
            "---------\n",
            "Sent # 1\n",
            "SRC: the ikisu creek overflowed on account of heavy rainfall in the district of yomra in trabzon .\n",
            "TGT: trabzon ‚Äô un yomra il√ßesinde etkili olan saƒüanak yaƒüƒ±≈ü nedeniyle iÃákisu deresi ta≈ütƒ± .\n",
            "---------\n",
            "Sent # 2\n",
            "SRC: two women disappeared in the floodwaters in the village of tasdelen and the road to the village of sayvan was closed .\n",
            "TGT: ta≈üdelen k√∂y√ºnde sele kapƒ±lan iki kadƒ±n kaybolurken , sayvan k√∂y√º yolu ula≈üƒ±ma kapandƒ± .\n",
            "---------\n",
            "Sent # 3\n",
            "SRC: the body of one of the women drowned in the floods was found .\n",
            "TGT: selde kayƒ±p olan iki kadƒ±ndan birinin cesedine ula≈üƒ±ldƒ± .\n",
            "---------\n",
            "Sent # 4\n",
            "SRC: there was precipitation in the highlands of yomra at around 15 : 00 .\n",
            "TGT: yaƒüƒ±≈ü , yomra ‚Äô nƒ±n y√ºksek kesiminde saat 15.00 sƒ±ralarƒ±nda etkili oldu .\n",
            "---------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HcjsY5UF_Pk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30845b8-1a5c-4aa4-e435-4ee9af689b1c"
      },
      "source": [
        "# Allocate train, dev, test portions\n",
        "all_size = len(fine_src_all_bpe)\n",
        "dev_size = 500\n",
        "test_size = 500\n",
        "train_size = all_size - test_size - dev_size\n",
        "\n",
        "fine_src_train_bpe = fine_src_all_bpe[0:train_size]\n",
        "fine_tgt_train_bpe = fine_tgt_all_bpe[0:train_size]\n",
        "\n",
        "fine_src_dev_bpe = fine_src_all_bpe[train_size:train_size+dev_size]\n",
        "fine_tgt_dev_bpe = fine_tgt_all_bpe[train_size:train_size+dev_size]\n",
        "\n",
        "fine_src_test_bpe = fine_src_all_bpe[train_size+dev_size:all_size]\n",
        "fine_tgt_test_bpe = fine_tgt_all_bpe[train_size+dev_size:all_size]\n",
        "\n",
        "print(\"Set sizes\")\n",
        "print(\"All:\", len(fine_src_all_bpe))\n",
        "print(\"Train:\", len(fine_src_train_bpe))\n",
        "print(\"Dev:\", len(fine_src_dev_bpe))\n",
        "print(\"Test:\", len(fine_src_test_bpe))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Set sizes\n",
            "All: 20016\n",
            "Train: 19016\n",
            "Dev: 500\n",
            "Test: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvweRBGDGeXL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "88e89b04-0777-4639-a85e-68a7f24228ef"
      },
      "source": [
        "# Store sentences as files\n",
        "with open(\"finetrain.bpe.\"+source_language, \"w\") as src_file, open(\"finetrain.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(fine_src_train_bpe, fine_tgt_train_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"finedev.bpe.\"+source_language, \"w\") as src_file, open(\"finedev.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(fine_src_dev_bpe, fine_tgt_dev_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n",
        "\n",
        "with open(\"finetest.bpe.\"+source_language, \"w\") as src_file, open(\"finetest.bpe.\"+target_language, \"w\") as tgt_file:\n",
        "  for s, t in zip(fine_src_test_bpe, fine_tgt_test_bpe):\n",
        "    src_file.write(s+\"\\n\")\n",
        "    tgt_file.write(t+\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-d15f6d523ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Store sentences as files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finetrain.bpe.\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msource_language\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finetrain.bpe.\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtarget_language\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtgt_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfine_src_train_bpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfine_tgt_train_bpe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msrc_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtgt_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fine_src_train_bpe' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbz1Hf-0G-59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4afdfa-9382-4bcf-f62a-04d524c8ff22"
      },
      "source": [
        "# If creating data for the first time, move all prepared data to the mounted location in google drive\n",
        "! mkdir -p \"$gdrive_path\"/data\n",
        "! cp finetrain.* \"$gdrive_path\"/data\n",
        "! cp finetest.* \"$gdrive_path\"/data\n",
        "! cp finedev.* \"$gdrive_path\"/data\n",
        "! ls \"$gdrive_path\"/data  #See the contents of the drive directory"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‚Äò/content/drive/My Drive/mt-workshop/en-tr-baseline/data‚Äô: File exists\n",
            "dev.bpe.en  finedev.bpe.en   finetrain.bpe.en  test.en\t     train.en\n",
            "dev.bpe.tr  finedev.bpe.tr   finetrain.bpe.tr  test.tr\t     train.tr\n",
            "dev.en\t    finetest.bpe.en  test.bpe.en       train.bpe.en\n",
            "dev.tr\t    finetest.bpe.tr  test.bpe.tr       train.bpe.tr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZP6K7qVT9n4"
      },
      "source": [
        "# OR... If continuing from previous run, load finetuning data from drive\n",
        "! cp \"$gdrive_path\"/data/finedev.* .\n",
        "! cp \"$gdrive_path\"/data/finetrain.* .\n",
        "! cp \"$gdrive_path\"/data/finetest.* .\n",
        "! cp \"$gdrive_path\"/data/finetest.* ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mjN3UUoHJN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f60f15f-3bb2-40fd-d060-2300f92ad697"
      },
      "source": [
        "# #Move everything important under joeynmt directory\n",
        "os.environ[\"data_path\"] = os.path.join(\"joeynmt\", \"data\", source_language + target_language)\n",
        "\n",
        "# Move fine-tuning data to data directory\n",
        "! mkdir -p $data_path\n",
        "! cp finetrain.* $data_path\n",
        "! cp finetest.* $data_path\n",
        "! cp finedev.* $data_path\n",
        "! ls $data_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finedev.bpe.en\tfinetest.bpe.en  finetrain.bpe.en\n",
            "finedev.bpe.tr\tfinetest.bpe.tr  finetrain.bpe.tr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dlh1pfvr4qw"
      },
      "source": [
        "# Also, load models from google drive to notebook storage  \n",
        "! mkdir -p joeynmt/models/${src}${tgt}_transformer\n",
        "! cp -r \"$gdrive_path/models/${src}${tgt}_transformer\" joeynmt/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mdL95STIzPK"
      },
      "source": [
        "# Let's create a config file for finetuning training\n",
        "# Changes from previous config are dataset names, model name, batch size and learning rate\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/finetrain.bpe\"\n",
        "    dev:   \"data/{name}/finedev.bpe\"\n",
        "    test:  \"data/{name}/finetest.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    load_model: \"{gdrive_path}/models/{name}_transformer/best.ckpt\" # Load base model from its best scoring checkpoint (from gdrive)\n",
        "    #load_model: \"models/{name}_transformer/best.ckpt\" # Load base model from its best scoring checkpoint (from local)\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0001\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 1028\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}_finetune.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU2ehibpJdrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2769748-9456-4919-9fc7-3081d3f97347"
      },
      "source": [
        "# Test our model on our domain before fine-tuning\n",
        "! cd joeynmt; python3 -m joeynmt test \"configs/transformer_${src}${tgt}_finetune.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-29 09:54:48,631 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-29 09:54:48,631 - INFO - joeynmt.data - Building vocabulary...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
            "    main()\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 38, in main\n",
            "    output_path=args.output_path, save_attention=args.save_attention)\n",
            "  File \"/content/joeynmt/joeynmt/prediction.py\", line 293, in test\n",
            "    data_cfg=cfg[\"data\"], datasets=[\"dev\", \"test\"])\n",
            "  File \"/content/joeynmt/joeynmt/data.py\", line 112, in load_data\n",
            "    dataset=train_data, vocab_file=src_vocab_file)\n",
            "  File \"/content/joeynmt/joeynmt/vocabulary.py\", line 161, in build_vocab\n",
            "    vocab = Vocabulary(file=vocab_file)\n",
            "  File \"/content/joeynmt/joeynmt/vocabulary.py\", line 40, in __init__\n",
            "    self._from_file(file)\n",
            "  File \"/content/joeynmt/joeynmt/vocabulary.py\", line 61, in _from_file\n",
            "    with open(file, \"r\", encoding='utf-8') as open_file:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'data/entr/vocab.txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-ir9d-LKez3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cb76d07-b083-4bb8-96fa-7888b8dcb796"
      },
      "source": [
        "# Train to our domain\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "! cd joeynmt; python3 -m joeynmt train \"configs/transformer_${src}${tgt}_finetune.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 17:35:43,552 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-28 17:35:43,585 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-28 17:35:52,185 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-28 17:35:53,413 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-28 17:35:53,459 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-28 17:35:53,469 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-28 17:35:53,469 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-28 17:35:53,695 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-28 17:35:53.930981: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-28 17:35:54,906 - INFO - joeynmt.training - Total params: 13372928\n",
            "2021-07-28 17:35:58,161 - INFO - joeynmt.training - Loading model from /content/drive/My Drive/mt-workshop/en-tr-baseline/models/entr_transformer/best.ckpt\n",
            "2021-07-28 17:35:58,607 - INFO - joeynmt.helpers - cfg.name                           : entr_transformer_finetune\n",
            "2021-07-28 17:35:58,607 - INFO - joeynmt.helpers - cfg.data.src                       : en\n",
            "2021-07-28 17:35:58,607 - INFO - joeynmt.helpers - cfg.data.trg                       : tr\n",
            "2021-07-28 17:35:58,607 - INFO - joeynmt.helpers - cfg.data.train                     : data/entr/finetrain.bpe\n",
            "2021-07-28 17:35:58,607 - INFO - joeynmt.helpers - cfg.data.dev                       : data/entr/finedev.bpe\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.test                      : data/entr/finetest.bpe\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : data/entr/vocab.txt\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : data/entr/vocab.txt\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/drive/My Drive/mt-workshop/en-tr-baseline/models/entr_transformer/best.ckpt\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-28 17:35:58,608 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0001\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.batch_size            : 1028\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1000\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-28 17:35:58,609 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/entr_transformer\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-28 17:35:58,610 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 370216,\n",
            "\tvalid 1000,\n",
            "\ttest 1000\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] ‚ñÅthan k ‚ñÅyou ‚ñÅso ‚ñÅmuch ‚ñÅ, ‚ñÅch ris ‚ñÅ.\n",
            "\t[TRG] ‚ñÅ√ßok ‚ñÅte≈ü ek k√ºr ‚ñÅeder im ‚ñÅchris ‚ñÅ.\n",
            "2021-07-28 17:35:58,611 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ‚ñÅ. (5) ‚ñÅ, (6) ‚ñÅthe (7) ‚ñÅ' (8) ‚ñÅand (9) ‚ñÅ\n",
            "2021-07-28 17:35:58,612 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ‚ñÅ. (5) ‚ñÅ, (6) ‚ñÅthe (7) ‚ñÅ' (8) ‚ñÅand (9) ‚ñÅ\n",
            "2021-07-28 17:35:58,612 - INFO - joeynmt.helpers - Number of Src words (types): 9034\n",
            "2021-07-28 17:35:58,612 - INFO - joeynmt.helpers - Number of Trg words (types): 9034\n",
            "2021-07-28 17:35:58,612 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=9034),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=9034))\n",
            "2021-07-28 17:35:58,619 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 1028\n",
            "\ttotal batch size (w. parallel & accumulation): 1028\n",
            "2021-07-28 17:35:58,620 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-28 17:36:05,528 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     5.201994, Tokens per Sec:     9767, Lr: 0.000300\n",
            "2021-07-28 17:36:11,767 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.912281, Tokens per Sec:    10804, Lr: 0.000300\n",
            "2021-07-28 17:36:18,021 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     4.973733, Tokens per Sec:    11041, Lr: 0.000300\n",
            "2021-07-28 17:36:24,255 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.809206, Tokens per Sec:    10934, Lr: 0.000300\n",
            "2021-07-28 17:36:30,681 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     4.819091, Tokens per Sec:    10656, Lr: 0.000300\n",
            "2021-07-28 17:36:37,252 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     4.839234, Tokens per Sec:    10245, Lr: 0.000300\n",
            "2021-07-28 17:36:43,827 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     3.323047, Tokens per Sec:    10317, Lr: 0.000300\n",
            "2021-07-28 17:36:50,434 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     4.597960, Tokens per Sec:    10237, Lr: 0.000300\n",
            "2021-07-28 17:36:56,883 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     4.473081, Tokens per Sec:    10245, Lr: 0.000300\n",
            "2021-07-28 17:37:03,243 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     4.721494, Tokens per Sec:    10846, Lr: 0.000300\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
            "    main()\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 35, in main\n",
            "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 805, in train\n",
            "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 475, in train_and_validate\n",
            "    valid_duration = self._validate(valid_data, epoch_no)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 563, in _validate\n",
            "    n_gpu=self.n_gpu\n",
            "  File \"/content/joeynmt/joeynmt/prediction.py\", line 125, in validate_on_data\n",
            "    n_best=n_best)\n",
            "  File \"/content/joeynmt/joeynmt/search.py\", line 444, in run_batch\n",
            "    encoder_hidden=encoder_hidden)\n",
            "  File \"/content/joeynmt/joeynmt/search.py\", line 39, in greedy\n",
            "    src_mask, max_output_length, model, encoder_output, encoder_hidden)\n",
            "  File \"/content/joeynmt/joeynmt/search.py\", line 146, in transformer_greedy\n",
            "    trg_mask=trg_mask\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/model.py\", line 104, in forward\n",
            "    outputs, hidden, att_probs, att_vectors = self._decode(**kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/model.py\", line 178, in _decode\n",
            "    **_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/decoders.py\", line 540, in forward\n",
            "    src_mask=src_mask, trg_mask=trg_mask)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/transformer_layers.py\", line 264, in forward\n",
            "    h1 = self.trg_trg_att(x_norm, x_norm, x_norm, mask=trg_mask)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/transformer_layers.py\", line 73, in forward\n",
            "    scores = scores.masked_fill(~mask.unsqueeze(1), float('-inf'))\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFYE2yj1N-di"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "! mkdir -p \"$gdrive_path/models/${src}${tgt}_transformer_finetune/\"\n",
        "! cp -r joeynmt/models/${src}${tgt}_transformer_finetune/* \"$gdrive_path/models/${src}${tgt}_transformer_finetune/\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd1_yOXANu3-"
      },
      "source": [
        "# Test again to see how our model improved\n",
        "#! cd joeynmt; python3 -m joeynmt test \"configs/transformer_${src}${tgt}_finetune.yaml\"\n",
        "\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer_finetune/config.yaml\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}